{"posts":[{"title":"2023.9.19 DailyNote","text":"2023-09-19 星期二2023.09.19 Tue 🌞 今年已过了 262 天（第 39 周/共 53 周）,距离 2024 年还有 103 天。距离 2023-11-04 高级-架构师考试还有 46 天。🗓️Weather：🌞🌥☁️⛈🌧🌦🌈🌪🌀⚡❄️🔥🥶🌊🌫🏠Location： 中国·广东省🛌Sleep：1：00 → 7：45 希望早日调整好作息时间 ⏰Must-To-Do 第一次发表… 没有早起 🚀️进展 自律：没有进展，依旧六块腹肌，单手五十俯卧撑 英法：菜够，法语真的难学 遇见：无，又是平凡的一天 其他：参加斗地主比赛 🐣Mini-Habits 月计划与反思 每日Golang 数据结构和算法 回顾昨天 每天早起 🧠今日反省思源功能太强大… 已经玩不明白了… 日历配置这个DailyNote template 配置原理又忘记了… 算了… 毕竟只是工具而已 争取每日后续更新一篇博客 日记推送到timeline​tag下 大概一周一次 看心情决定是否发布 随机复习 距离 2024-07-01​ 还剩 285​ 天，加油，你一定行！ 随机复习是一个好习惯，可惜现在笔记内容提炼不够精简，就不放到博客上展示了….","link":"/post/20230919-tuesday-uuzu4.html"},{"title":"命运齿轮","text":"命运齿轮2023.10.14 Sat 🌞 今年已过了 287 天（第 42 周/共 53 周）,距离 2024 年还有 78 天。距离 2023-11-04 高级-架构师考试还有 21 天。🗓️Weather：🌞🌥☁️⛈🌧🌦🌈🌪🌀⚡❄️🔥🥶🌊🌫🏠Location： 中国·广东省🛌Sleep：5.30 → 12 熬了个大夜-深夜思考人生《痛苦面具》 ⏰Must-To-Do ‍ 🚀️进展 自律：周五晚拉满了，游泳0.5h、力量训练1h、骑车10KM、跑步5kM 25min、核心训练0.5h 英法：IOS 播客很好用 遇见：清华经管-混血修勾？？ 其他：努力永远不会错 🐣Mini-Habits 月计划与反思 每日Golang 数据结构和算法 回顾昨天 每天早起 🧠今日反省 我发现很多技术手段，或者思想在某种程度上都是类似的，到一定阶段必然是融汇贯通的。 很多知识会就是会，不会就是不会，如果一些问题有过自己的思考量，即使知识本身的内容记不住，但是这个逻辑关系，逻辑追溯还是很重要的 最近玩且看狼人杀，记忆力好真的很关键，逻辑梳理更是一种能力，需要培养 ✨随缘记随便写点什么吧…. 缓存的分片有用到哈希算法与一致性哈希算法，同等联想到负载均衡的一种策略，也是一致性哈希算法，瞬间有点悟，好多其实都是相通的… ‍","link":"/post/20231014-saturday-zh8hf1.html"},{"title":"2023-12-07 星期四","text":"2023-12-07 星期四2023.12.07 Thu 🌞 今年已过了 341 天（第 50 周/共 53 周）,距离 2024 年还有 24 天。距离 2024-02-10 春节还有 65 天。🗓️Weather：🌞🌥☁️⛈🌧🌦🌈🌪🌀⚡❄️🔥🥶🌊🌫🏠Location： 中国·广东省🛌Sleep：? → ? ✨随缘记这次更新没有使用前面固化的模板，每天做到日更博客是真的难，更别提是原创博客了，现在越来越能理解那些公众号总是互相转载文章了，除了引流之外，技术文章原创是真的难，而他们那些知名博主纯原创文章周更一篇已是很难得的了。 早在三个月前我就曾记录过“早在之前就有过建立自己的博客，但没有坚持下来。现重操旧业，坚持每天输出一篇Blog”。但这一次绝不会放弃了，即使做不到日更，也要坚持周更、两周更，宁可牺牲一些其他时间，还是秉承原则，宁可粗浅，也要保持清晰、易懂的内容输出。 ‍ ‍","link":"/post/20231207-thursday-1igq3d.html"},{"title":"","text":"2023-10-21 星期六2023.10.21 Sat 🌞 今年已过了 294 天（第 43 周/共 53 周）,距离 2024 年还有 71 天。距离 2023-11-04 高级-架构师考试还有 14 天。🗓️Weather：🌞🌥☁️⛈🌧🌦🌈🌪🌀⚡❄️🔥🥶🌊🌫🏠Location： 中国·广东省🛌Sleep：2 → 7.30 ；9.30 → 12 ⏰Must-To-Do 周末学习打卡 🚀️进展 自律：马拉松备赛 ； 俄挺和前水平有点小帅 英法：关注了几个英语博主 遇见：三只羊在沈阳开分公司了？？ 其他： 🐣Mini-Habits 月计划与反思 每日Golang 数据结构和算法 回顾昨天 每天早起 🧠今日反省 越来越信因果效应了… 信息差也是一种能力… 早睡早起肯定算一种优点，但真的好难 快速建立知识体系 ✨随缘记随便写点什么吧…. 如果不知道布隆过滤器，那几乎不可能知道布谷鸟过滤器… 如果知道缓存穿透，那几乎必知道布隆过滤器… 百因必有果… 三缓存问题很常见，不必多说： ​​​​ 今天看过一个系统架构分析的实例： 系统使用过程中，由于同样的数据分别存在于数据库和缓存系统中，必然会造成数据同步或数据不一致性的问题。 法一：应用程序读数据时，首先读缓存，当该数据不在缓存时,再读取数据库；应用程序写数据吋，先写缓存，成功后再写数据库；或者先写数据库，再写缓存。 法二： 读数据操作的基本步骤：1.根据key读缓存；2.读取成功则直接返回；3.若key不在缓存中时，根据key (从数据库中读取数据) ；4.读取成功后， (更新缓存中key值) ；5.成功返回。写数据操作的基本步骤：1.根据key值写 (数据库) ；2.成功后 (更新缓存(key值)) ；3.成功返回。 ‍ ‍","link":"/post/20231021-saturday-z1vmbxc.html"},{"title":"2023.8.30","text":"2023.8.30从今天起，要狠狠滴加练，潶櫹潶櫹！！！ ‍","link":"/post/2023830-zy8cbl.html"},{"title":"几个后端的问题","text":"6.14 以下几个简单但并不简单的问题验证下自己的后端能力水平 业务Rpc服务如何理解 什么场景下应用RPC 是远程调用协议，服务之间只需要维持一个通信协议即可，不需要对编程语言有任何限制，也不用关系底层网络协议。多数是用在微服务、分布式场景下。 分布式下rpc如何调用可以通过注册中心+服务发现的方式实现调用，市面上很多组件比如consul、etcd等等都可以实现。 负载均衡策略有哪些 你们公司用的是哪种包括轮询、随机、加权、一致性哈希及最小连接数等。我们用的是轮询方式。 这个最好是有自己实现过的代码。 手写负载均衡、手写缓存中间件…后续再补充 Rpc协议和HTTP协议如何理解，有什么区别 RPC协议是远程调用协议，HTTP是超文本传输协议； RPC多用在微服务分布式和内部调用，HTTP多用在外部api服务； RPC的调用速度快于HTTP，但HTTP的实现要比RPC简单很多。 RPC有什么优势 为什么有这种优势RPC 不要考虑编程语言，服务只需要维持一个通信协议即可；调用速度非常的快；封装了很多方法，比如负载均衡等；适用于分布式微服务场景，内部扩展型较好。 为什么有这种优势不会答，我估计是要深入到socket层面 月百万级数据以上，总共一亿数据如何管理 优化思路如果是这种场景可以根据时间来划分，按月水平分表，这种思路，可以及时清理掉冷数据，如果在业务评估阶段就估计月表的数据量是百万级，且又可能在业务高峰期上升到千万或者亿界别的数据，那么可以考虑多一个月的数据再次分片，按照大小拆分，建议500w数据量拆分一个，虽然单表的数据量在2000w的时候，MySQL也能维持较好的层数，但是考虑到性能等问题，参考阿里巴巴数据库的设计思路，他们在海量数据的业务场景下经验丰富，比较推荐500w拆分一张 另外一种思路是根据用户id来进行hash分片拆分，多个MySQL部署，举个例子通过userid来计算哈希函数，然后和哈希掩码取模，可以快速定位到用户的数据存储在哪一个实例中，比较推荐使用MyCat这种业内比较成熟的数据库分片组件。 按月分表情况下(几十张表），这种情况下如何查某个用户的所有的订单可以加一层设计，比如用Redis的Hash结构，用户的userid作为key，list作为field，value是所有的订单信息的月份、id等，可以快速定位表位置、索引的信息。 12345678910111213141516{ &quot;user_id_1&quot;: { &quot;order_info&quot;: { &quot;2022-01&quot;: [&quot;order_1&quot;, &quot;order_2&quot;], &quot;2022-02&quot;: [&quot;order_3&quot;], &quot;2022-03&quot;: [&quot;order_4&quot;, &quot;order_5&quot;] } }, &quot;user_id_2&quot;: { &quot;order_info&quot;: { &quot;2022-01&quot;: [&quot;order_6&quot;, &quot;order_7&quot;], &quot;2022-02&quot;: [&quot;order_8&quot;], &quot;2022-03&quot;: [&quot;order_9&quot;, &quot;order_10&quot;] } }} 在这个示例中，每个 userid 都是 Hash 表的 key，而对应的 value 是一个包含了 order_info​ 这个 field。order_info​ 是一个内部的 Hash 结构，它使用年月（例如 “2022-01”）作为 field，而对应的值是一个列表，包含该用户在该月份下的所有订单编号。 通过这种设计，你可以快速定位到指定用户的订单信息，并按照不同月份进行索引。例如： 获取某个用户的所有订单：使用 HGETALL user_id_x 命令获取该用户的完整信息，然后再读取其中的 order_info​ 字段即可得到该用户在不同月份下的订单列表。 获取某个用户指定月份的订单信息：使用 HGET user_id_x order_info:YYYY-MM 命令获取对应月份的订单列表。 需要注意的是，为了保证不同月份的订单信息不会相互冲突，每个月份的 field 最好采用类似 “YYYY-MM” 的格式进行命名。 这种结构可以帮助你更好地组织和索引订单信息，同时也提供了快速定位、查询和统计的能力。 可以实现以下快速定位和索引的优势： 快速访问指定用户的订单信息：通过用户的 userid 作为 key，直接从 Redis 中获取对应的 Hash 结构，从而快速获取该用户的订单信息。 快速定位到指定年月的订单列表：在用户的订单信息中，使用年月作为 field，可以直接访问指定年月的订单列表，而无需遍历整个订单信息。 高效添加和删除订单信息：使用 Redis 的 List 结构存储订单列表，可通过头尾插入和删除操作，快速添加和删除订单，而不会影响其他年月的订单列表。 订单状态怎么查（已支付，待支付，待发货这种）设定字段订单状态status，通过上述的redis方案快速在数据库中查询，这里正好使用了Redis，可以设定缓存，但这就涉及到数据库和缓存更新的一致性问题了，写操作先更新数据库再删除缓存，读如果命中则直接返回，没命中则读取后写入缓存的方案可以避免。 如果用热点数据如何定义热点数据可以用 Redis 的 Zset 来给数据进行评分，这里的热点可以以时间、点击量、购买数等等因素综合考量。 订单状态转换如何更新？如何保持同步这个在上面已经说明了，写是先更新数据库再删除缓存，读是命中则返回，没命中则读取后写入缓存保持同步。这里同样需要保证了两步操作同时执行成功，可以用两种方案来保证，一种是重试机制，将操作发送到消息队列中，执行成功则在消息队列中删除该信息，如果失败则重新读取这条消息，超过一定次数则向上游返回报错，第二种方案是订阅MySQL的binlog日志，由于MySQL执行完之后会将操作记录在binlog里，所以可以使用binlog来找到具体的操作，这里推荐使用阿里巴巴的canal组件，他的思想就是模拟了mysql主从的交互协议实现这个功能。 服务器线上问题有排查过吗，怎么定位问题所在，整体链路讲讲，比如504这种问题。结合实际场景和原因进行分析有排查过，首先思路是并不是问题出现了再去排查问题，我们先要设计一套预防方案，如果是常规的报错问题，可以通过日志、链路追踪查询定位到，如果是服务崩溃等信息，可以通过监控报警，比如grafana，包括在CPU、内存负载压力过高的时候可以提前预警人工介入。 真的是线上出现了bug、崩溃、超时等问题，那么整体流程是，先通过链路追踪定位返回错误信息或者崩溃的服务是哪个，查看服务崩溃的日志的具体原因，精确到代码行，如果无法从代码的逻辑角度排查问题，我们要看CPU、内存的使用情况，通过系统监控确定是哪一个环节出问题，然后查看是否是内存泄漏、SQL慢查询、GC异常等问题。 504 是属于 http请求返回超时的问题，举一个实际场景的例子，比如我做过一个音视频合成的例子，之前我调用合成api接口的超时，然后返回给上游的调度服务错误信息，定位到是我服务返回的超时，模拟了当时的请求信息，通过链路追踪，在jager上查看了收到合成api请求的时间耗时过长，解决方案是，最开始合成方是用http的方式返回合成url，后来我们采用grpc 流式传输的方案可以实时返回，在没接收到结束信号时，不断开链接。 502 和 504 区别 502 是网络无法请求，接收到错误的响应； 504 是网络请求，但是接受响应超时； MySQL慢查询有涉及过吗？如果用了索引还是慢查询该怎么办 ？慢查询可以通过 MySQL 的慢查询日志查到，不过默认 MySQL 的慢查询日志是关闭的。 如果用了索引还是慢查询，可以使用 explain 命令来查看 sql 语句的执行计划，可以查看是否正常使用了索引，排查是否存在索引失效的可能。另外排查是否是数据库的参数，比如缓存、连接等待等问题。 数据量过大该如何优化数据量过大可以考虑分库分表了。分库就是根据实际场景将数据分散到多个库，分表是将数据拆分成多个表，防止单表过大。 如何分库分表 垂直拆分：由于前期表的设计没有抽象，所以这时候要根据关系性较强的几个字段对表进行拆分。一种思路是将长度比较大、不常用的信息，移到扩展表。 水平拆分：将一张大表拆分成数据结构相同的几个表，防止单表过大。 这里举个商城的例子来说明，我们可以拆分成订单信息库、用户信息库、商品详情库，每个库中的表的数据量过于庞大，比如超过500万行就可以考虑水平拆分，如果有几个关系性较强的字段，可以垂直拆分建立一张新表，具体根据自己的业务实际场景来进行扩展。 你常用的索引有哪些 结合业务说明 按照类型分类：B+ 数索引，哈希索引，fulltext索引； 按照存储方式分类：聚簇索引和非聚簇索引； 按照使用的列分类：联合索引和单一索引； 按照使用的字段分类：主键索引、唯一索引、前缀索引和普通索引。 常用的是联合索引和主键索引，比如通过id直接查询一条记录的完整信息，我们可以使用主键索引快速定位，再比如联合性比较强的两个字段，可以建立联合索引，比如要查询年龄为x、成绩是y的成员name，可以用（x，y，name）建立联合索引，利用覆盖索引可以减少回表。 什么是聚簇索引，什么是非聚簇索引，它们有什么区别聚簇索引具有唯一性，比如主键就是聚簇索引，如果没有主键会选择唯一且不为NULL的列作为主键索引，如果没有则会生成一个自增id作为主键索引。聚簇索引存放的信息是完整的数据记录，而非聚簇索引只存放聚簇索引信息。如果查询语句使用的是非聚簇索引，且查询的数据不是主键值，会在叶子节点找到主键值后进行回表，如果是主键值就会进行索引覆盖。 主键索引的底层存储结构 大致实现过程B+ 树。B+ 树是由 B 树改进而来的，所有的索引都存放在叶子节点，并构成有序的链表，其实是双向链表，叶子节点的值存放的是数据页，数据页里包含完整的记录，而非叶子节点只存放索引，非叶子节点会作为叶子节点中索引的最大或者最小节点，比如举个例子，根节点存放的索引是 (1、10、19)，那么第二层就可以是 (1、4、7)，(10、13、16)，(19、22、25)，而第三层如果是叶子节点，就会存放完整的索引和数据。并且支持范围查询，由于 MySQL 的 B+ 树底层节点是双向链表，所以范围查询效率很高；B+ 树的非叶子节点只存放索引，可以存放更多的记录，所以相同数据量下，B+ 树更矮胖，减少了 磁盘 IO；B+ 树有大量的冗余节点，在插入和删除时不会发生过多的树结构变化。 非主键索引、联合索引等的存储结构B+ 树。 这些索引在存储数据时有什么区别主键索引的值是完整的数据记录，其他索引存储的值是主键索引，联合索引的key是多个字段，查询的时候按照最左匹配原则。 主键索引的索引信息存在哪里叶子节点 MySQL的数据存在哪里存在磁盘中，如果是 InnoDB 引擎，则会按照三个文件存放，db.opt 存放的是数据库设置的默认字符集和字符校验规则，.frm 文件是存放表的结构信息，比如列、数据类型、索引等，.ibd 是存放数据的内容。 范围查询时主键索引是如何去做的首先通过二分查找定位到边界，然后通过双向链表，开始遍历即可。 什么是索引覆盖索引覆盖是在联合索引是，查询的内容在联合索引的key上就可以查到，避免了回表。比如联合索引（x，y），现在想通过x条件查询y的内容，就可以使用（x，y）避免再次回表。 MySQL用过什么存储引擎常见的是 Innodb、MyISAM、Memory等。现在默认是 InnoDB。 InnoDB有什么特性 存储：存放在 .frm、.ibd文件； 索引：支持聚簇索引和非聚簇索引； 事务：有undo log 和 redo log，支持事务； 故障恢复：有 redo log，支持故障恢复； 锁：支持表级锁和行级锁； 就针对和 MyISAM 的区别聊 事务有什么特性ACID，原子性、隔离性、一致性、持久性。 可重复读是如何做的，如何实现的通过 MVCC 实现的。 MySQL在事务启动后会为事务生成一个 Read View 快照，Read View 会记录当前事务的id、活跃的事务id列表、活跃事务id的最小值、下一个创建的事务id值，MySQL的行数据也会记录最新修改过这一行的事务id，同样会记录该行的上一版本记录的指针，像一个链表一样。当事务A启动后，事务B启动，事务B的活跃事务id列表就是A和B的事务id，现在事务要对一个行数据进行操作，如果活跃事务id列表的最小值比当前行数据记录的最新修改过这一行的事务id值大，说明最新操作该行数据记录的事务已经提交完成，所以可以对该行进行操作，如果大于下一个创建的事务id值，说明这个最新操作该行数据记录的事务是在当前事务A和B启动之后再启动的事务，那么就不可以对这行数据进行操作。之后要分两种情况讨论，如果在活跃的事务id列表之间，说明有其他事务在操作该行，那么不可以对该行操作，会去找该行记录的上一版本指针，如果不在则说明最新操作该行数据记录的事务已经提交，那么可以对该行进行操作，操作了之后，该行记录会更新最新修改这一样的事务id，同样以链表形式将上一版本记录连接起来。 MVCC 是如何做的上面那一段 如果commit 时，数据版本和快照版本不一致该怎么办回滚 如果加锁的话加的什么锁没懂要问啥，如果是问的可重复读里面的幻读问题，那就是间隙锁，如果带有记录的话是 nextkey lock（间隙锁+记录锁），举个例子： 事务A，执行了 for update 语句 当前读，查询大于等于 5 的记录，这时候事务 B，插入了一条 10 的记录，这样事务 A 如果再次查询的话，前后两次查询就会幻读，所以这时候会引入一个 next key lock，锁住 [5, +∞]的记录，（如果是大于 5 那就是间隙锁）。 Redis常用数据类型String、List、Hash、Set、Sort Set。 可以加上 Hyperloglog、Stream、Bitmap、Geospatial index 。 大 key 问题如何解决如果是 Set 结构则可以修改为 Hash，不要一次性全查。可以将大 key 拆分成多个小 key，读取的时候批量读取拼接即可。而且尽量给大 key 设置较长的过期时间，不让其在缓存中淘汰。删除可以用分批次删除或者异步删除的做法，避免阻塞主线程。 用的 Redis 单机还是集群集群 如何存数据不清楚要问的是底层数据结构还是持久化存储，底层数据结构就将五种基本类型的底层数据结构，持久化就是说 RDB 和 AOF 举个例子，String，如果存储 int 整形，可以用 long 表示，则用 int 来存储，如果是字符串，则小于等于 44 字节用 embstr 存储，其他情况是 raw。 RDB 和 AOF， 就是一个快照，一个追加日志，可能要问一些详细的过程了，还有刷盘策略等等。看具体想问什么。 讲讲主从复制由于 Redis 具有持久化能力（RDB 和 AOF），为了避免单点故障，可以引入主从模式，主机可以进行读写操作，每次的写操作都会同步数据给从机。主从模式主要是为了减轻主机压力以及容灾恢复。接下来大致介绍一下主从复制的流程： 建立好集群及主从关系后，从机会连接主机，发送 SYNC 命令，主机接收到 SYNC 命令后，开始执行 BGSAVE 命令生成 RDB 文件并使用缓冲区记录此后执行的所有写命令，之后会向所有从机发送 RDB ，并在发送期间继续记录被执行的写命令，从机接收到后会载入执行，之后的每一个主机的写命令都会发送到从机执行，如果有断线重连会才用增量复制，补全缓冲区中的命令。 主服务器挂了怎么办哨兵模式会进行监控、选主、通知。首先是如何判断主服务器真的挂了，这里分为主观下线和客观下线，如果哨兵节点接受不到主服务器的信号就会认定为其主观下线，但是哨兵也是集群大的方式部署的，如果超过一半节点认为是主观下线就认定其客观下线。之后哨兵leader会从从机中选取一个新的主节点，进行主从故障转移，让原主下的所有从机都作为新的主机的从机，并且通过发布订阅通知客户端，另外会监控原主机的情况，如果原主机重新上线，那么会作为新主的从。 这里可能还会涉及到哨兵leader、选主策略、主从数据不一致的问题。 主服务器选举是怎么做的首先会找优先级最高的从节点，其次找复制进度最靠前的节点，最后通过id排序。 联想分布式算法原理…… ‍ ‍","link":"/post/614-1z6cwp.html"},{"title":"训练历程","text":"我的数据结构和算法知识体系我的算法体系由hello+代码随想录培养…. 相互补充 K神：力扣（LeetCode）全网阅读量最高博主，发表的《图解算法数据结构》已被订阅 27 万本。 本部分目的是巩固并加强对数据结构的全面理解，大学期间也有多的这门课程，但是内容有限，很难成为体系，算是一个基础的巩固，比起直接手撕LeetCode1000题，我感觉基础巩固更重要，重点记录新的理解。 待办： labuladong Cookbook LeetCode HOT 100 ‍","link":"/post/algorithm-system-1wyqpm.html"},{"title":"Docker一文通览","text":"Docker通览 通览篇主要是构建基本的认知，本篇从基础概念、应用、核心底层组成，甚至包括一点点个人理解。 Docker基础与应用 官方文档地址:https://www.docker.com/get-started 中文参考手册:https://docker_practice.gitee.io/zh-cn/ 什么是 Docker官方定义 最新官网首页 1234# 1.官方介绍- We have a complete container solution for you - no matter who you are and where you are on your containerization journey.- 翻译: 我们为你提供了一个完整的容器解决方案,不管你是谁,不管你在哪,你都可以开始容器的的旅程。- 官方定义: docker是一个容器技术。 Docker的起源12345Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护。Docker 项目后来还加入了 Linux 基金会，并成立推动 开放容器联盟（OCI）。Docker 自开源后受到广泛的关注和讨论，至今其 GitHub 项目 已经超过 5 万 7 千个星标和一万多个 fork。甚至由于 Docker 项目的火爆，在 2013 年底，dotCloud 公司决定改名为 Docker。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。 为什么是Docker 在开发的时候，在本机测试环境可以跑，生产环境跑不起来 这里我们拿java Web应用程序举例，我们一个java Web应用程序涉及很多东西，比如jdk、tomcat、mysql等软件环境。当这些其中某一项版本不一致的时候，可能就会导致应用程序跑不起来这种情况。Docker 则将程序以及使用软件环境直接打包在一起，无论在那个机器上保证了环境一致。 优势1: 一致的运行环境,更轻松的迁移 服务器自己的程序挂了，结果发现是别人程序出了问题把内存吃完了，自己程序因为内存不够就挂了 这种也是一种比较常见的情况，如果你的程序重要性不是特别高的话，公司基本上不可能让你的程序独享一台服务器的，这时候你的服务器就会跟公司其他人的程序共享一台服务器，所以不可避免地就会受到其他程序的干扰，导致自己的程序出现问题。Docker就很好解决了环境隔离的问题，别人程序不会影响到自己的程序。 优势2：对进程进行封装隔离,容器与容器之间互不影响,更高效的利用系统资源 公司要弄一个活动，可能会有大量的流量进来，公司需要再多部署几十台服务器 在没有Docker的情况下，要在几天内部署几十台服务器，这对运维来说是一件非常折磨人的事，而且每台服务器的环境还不一定一样，就会出现各种问题，最后部署地头皮发麻。用Docker的话，我只需要将程序打包到镜像，你要多少台服务，我就给力跑多少容器，极大地提高了部署效率。 优势3: 通过镜像复制N多个环境一致容器 Docker和虚拟机区别 关于Docker与虚拟机的区别，我在网上找到的一张图，非常直观形象地展示出来，话不多说，直接上图。 比较上面两张图，我们发现虚拟机是携带操作系统，本身很小的应用程序却因为携带了操作系统而变得非常大，很笨重。Docker是不携带操作系统的，所以Docker的应用就非常的轻巧。另外在调用宿主机的CPU、磁盘等等这些资源的时候，拿内存举例，虚拟机是利用Hypervisor去虚拟化内存，整个调用过程是虚拟内存-&gt;虚拟物理内存-&gt;真正物理内存，但是Docker是利用Docker Engine去调用宿主的的资源，这时候过程是虚拟内存-&gt;真正物理内存。 传统虚拟机 Docker容器 磁盘占用 几个GB到几十个GB左右 几十MB到几百MB左右 CPU内存占用 虚拟操作系统非常占用CPU和内存 Docker引擎占用极低 启动速度 （从开机到运行项目）几分钟 （从开启容器到运行项目）几秒 安装管理 需要专门的运维技术 安装、管理方便 应用部署 每次部署都费时费力 从第二次部署开始轻松简捷 耦合性 多个应用服务安装到一起，容易互相影响 每个应用服务一个容器，达成隔离 系统依赖 无 需求相同或相似的内核，目前推荐是Linux Docker的安装安装docker(centos7.x) 卸载原始docker 12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 安装docker依赖 123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 设置docker的yum源 123$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 安装最新版的docker 1$ sudo yum install docker-ce docker-ce-cli containerd.io 指定版本安装docker 123$ yum list docker-ce --showduplicates | sort -r$ sudo yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io$ sudo yum install docker-ce-18.09.5-3.el7 docker-ce-cli-18.09.5-3.el7 containerd.io 启动docker 12$ sudo systemctl enable docker$ sudo systemctl start docker 关闭docker 1$ sudo systemctl stop docker 测试docker安装 1$ sudo docker run hello-world bash安装(通用所有平台) 在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，CentOS 系统上可以使用这套脚本安装，另外可以通过 --mirror 选项使用国内源进行安装：执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker 的稳定(stable)版本安装在系统中。 12$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 启动docker 12$ sudo systemctl enable docker$ sudo systemctl start docker 创建docker用户组 1$ sudo groupadd docker 将当前用户加入docker组 1$ sudo usermod -aG docker $USER 测试docker安装是否正确 1$ docker run hello-world Docker 的核心架构​​ ​镜像:​ 一个镜像代表一个应用环境,他是一个只读的文件,如 mysql镜像,tomcat镜像,nginx镜像等 ​容器:​ 镜像每次运行之后就是产生一个容器,就是正在运行的镜像,特点就是可读可写 ​仓库:​用来存放镜像的位置,类似于maven仓库,也是镜像下载和上传的位置 ​dockerFile:​docker生成镜像配置文件,用来书写自定义镜像的一些配置 ​tar:​一个对镜像打包的文件,日后可以还原成镜像 ​​ Docker 配置阿里镜像加速服务docker 运行流程 docker配置阿里云镜像加速 访问阿里云登录自己账号查看docker镜像加速服务 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ &quot;registry-mirrors&quot;: [&quot;https://lz2nib3q.mirror.aliyuncs.com&quot;]}EOFsudo systemctl daemon-reloadsudo systemctl restart docker 验证docker的镜像加速是否生效 1234567[root@localhost ~]# docker info .......... 127.0.0.0/8 Registry Mirrors: 'https://lz2nib3q.mirror.aliyuncs.com/' Live Restore Enabled: false Product License: Community Engine Docker的入门应用docker 的第一个程序 docker run hello-world 12345678910111213141516171819202122[root@localhost ~]# docker run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 常用命令启动命令启动docker 1systemctl start docker 关闭docker 1systemctl stop docker 重启docker 1systemctl restart docker docker设置随服务启动而自启动 1systemctl enable docker 辅助命令1234# 1.安装完成辅助命令 docker version -------------------------- 查看docker的信息 docker info -------------------------- 查看更详细的信息 docker --help -------------------------- 帮助命令 Images 镜像命令12345678910111213141516171819202122232425262728293031# 1.查看本机中所有镜像 docker images -------------------------- 列出本地所有镜像 -a 列出所有镜像（包含中间映像层） -q 只显示镜像id# 2.搜索镜像 docker search [options] 镜像名 ------------------- 去dockerhub上查询当前镜像 -s 指定值 列出收藏数不少于指定值的镜像 --no-trunc 显示完整的镜像信息# 3.从仓库下载镜像/拉取仓库镜像到本地 docker pull 镜像名[:TAG|@DIGEST] ----------------- 下载镜像# 4.删除镜像 docker rmi 镜像名 -------------------------- 删除镜像 -f 强制删除 如果删除多个镜像用空格隔开 删除全部镜像 -a 意思为显示全部, -q 意思为只显示ID docker rmi -f $(docker images -aq)# 5.加载本地件tar，恢复为镜像 docker load -i 本地文件 与docker save对应 docker load -i hello.tar# 6.保存镜像为本地文件（打包镜像）** docker save -o 镜像保存位置与名字 镜像名/镜像ID 与docker load对应 docker save -o hello.tar hello-world# 7.上传镜像到docker hub docker push hello:V1 search: ​​ ​docker 安装镜像时如果不指定安装的版本默认最新版本​ Contrainer 容器命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 1.运行容器 docker run 镜像名 -------------------------- 镜像名新建并启动容器 --name 别名为容器起一个名字 -d, --detach 启动守护式容器（在后台启动容器） -p 映射端口号：原始端口号 指定端口号启动 -v, --volume list 给容器挂载数据卷 --name string 指定容器名称 -e, --env list 设置容器环境变量 -i, --interactive 以交互模式运行容器，通常与 -t 同时使用 -m, --memory bytes 容器内存上限 -t, --tty 为容器重新分配一个伪输入终端，通常与 -i 同时使用 -w, --workdir string 指定工作目录 例：docker run -it --name myTomcat -p 8888:8080 tomcat docker run -d --name myTomcat -P tomcat 常用换行的方式： docker run -dit \\ -v $PWD/ql/config:/ql/config \\ -p 5600:5600 \\ --name qinglong \\ --hostname qinglong \\ --restart unless-stopped \\ whyour/qinglong:2.11.3# 2.查看运行的容器 docker ps -------------------------- 列出所有正在运行的容器 -a 正在运行的和历史运行过的容器（已经停止）# 4.删除容器 docker rm -f 容器id和容器名 docker rm -f $(docker ps -aq) -------------------------- 删除所有容器# 5.查看容器内进程 docker top 容器id或者容器名 ------------------ 查看容器内的进程# 6.查看查看容器内部细节 docker inspect 容器id ------------------ 查看容器内部细节# 7.查看容器的运行日志 docker logs [OPTIONS] 容器id或容器名 ------------------ 查看容器日志 -t 加入时间戳 -f 跟随最新的日志打印 --tail 数字 显示最后多少条# 8.进入容器内部 docker exec [options] 容器id 容器内命令 ------------------ 进入容器执行命令 -i 以交互模式运行容器，通常与-t一起使用 -t 分配一个伪终端 shell窗口 bash 常用：docker exec -it redis sh 或者使用docker attach 容器名/容器ID【 不常用 】---没有使用过....# 9.容器和宿主机之间复制文件 docker cp 文件|目录 容器id:容器路径 ----------------- 将宿主机复制到容器内部 docker cp 容器id:容器内资源路径 宿主机目录路径 ----------------- 将容器内资源拷贝到主机上# 10.数据卷(volum)实现与宿主机共享目录 docker run -v 宿主机的路径|任意别名:/容器内的路径 镜像名 注意: 1.如果是宿主机路径必须是绝对路径,宿主机目录会覆盖容器内目录内容 2.如果是别名则会在docker运行容器时自动在宿主机中创建一个目录,并将容器目录文件复制到宿主机中# 11.打包镜像 docker save 镜像名 -o 名称.tar# 12.载入镜像 docker load -i 名称.tar# 13.容器打包成新的镜像 docker commit -m &quot;描述信息&quot; -a &quot;作者信息&quot; （容器id或者名称）打包的镜像名称:标签 补充： 123456789101112# 1.开机自启 docker update --restart=always 容器Id/容器名# 2.更换容器名字 docker rename 容器ID/容器名字 新容器名字# 3.查看docker磁盘 docker system df# 4.查看容器占用内存 docker stats 容器名/容器ID docker stats redis 查看docker磁盘占用 ​​ ​SIZE​（大小）：表示Docker系统中各个部分（镜像、容器、卷、构建缓存）所占用的总磁盘空间大小，以字节为单位。 ​RECLAIMABLE​（可回收大小）：表示可以通过清理或删除不再使用的资源来释放的磁盘空间大小，同样以字节为单位。 docker save 与docker export区别： docker export需要指定容器(container)，不能像docker save那样指定镜像(image)或容器(container)都可以。 docker save保存的是镜像（image），docker export保存的是容器（container）； docker load用来载入镜像包，docker import用来载入容器包，但两者都会恢复为镜像； docker load不能对载入的镜像重命名，而docker import可以为镜像指定新名称。 docker save的应用场景是，如果你的应用是使用docker-compose.yml编排的多个镜像组合，但你要部署的客户服务器并不能连外网。这时，你可以使用docker save将用到的镜像打个包，然后拷贝到客户服务器上使用docker load载入 。—这一点​***深有体会*** docker export的应用场景主要用来制作基础镜像，比如你从一个ubuntu镜像启动一个容器，然后安装一些软件和进行一些设置后，使用docker export保存为一个基础镜像。然后，把这个镜像分发给其他人使用，比如作为基础的开发环境。—这个确实没有应用过 docker的镜像原理镜像是什么？ 镜像是一种轻量级的，可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时所需的库、环境变量和配置文件。 为什么一个镜像会那么大？ 镜像就是花卷 UnionFS（联合文件系统）: Union文件系统是一种分层，轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。Union文件系统是Docker镜像的基础。这种文件系统特性:就是一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 。 Docker镜像原理 docker的镜像实际是由一层一层的文件系统组成。 bootfs（boot file system）主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。在docker镜像的最底层就是bootfs。这一层与Linux/Unix 系统是一样的，包含boot加载器（bootloader）和内核（kernel）。当boot加载完,后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时会卸载bootfs。 rootfs（root file system），在bootfs之上，包含的就是典型的linux系统中的/dev，/proc，/bin，/etc等标准的目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu/CentOS等等。 我们平时安装进虚拟机的centos都有1到几个GB，为什么docker这里才200MB？对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令，工具，和程序库就可以了，因为底层直接使用Host的Kernal，自己只需要提供rootfs就行了。由此可见不同的linux发行版，他们的bootfs是一致的，rootfs会有差别。因此不同的发行版可以共用bootfs。 为什么docker镜像要采用这种分层结构呢? 最大的一个好处就是资源共享 比如：有多个镜像都是从相同的base镜像构建而来的，那么宿主机只需在磁盘中保存一份base镜像。同时内存中也只需要加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。Docker镜像都是只读的。当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称为容器层，容器层之下都叫镜像层。 Docker安装常用服务安装mysql12345678910111213141516171819202122232425262728# 1.拉取mysql镜像到本地 docker pull mysql:tag (tag不加默认最新版本) # 2.运行mysql服务 docker run --name mysql -e MYSQL_ROOT_PASSWORD=root -d mysql:tag --没有暴露外部端口外部不能连接 docker run --name mysql -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -d mysql:tag --没有暴露外部端口# 3.进入mysql容器 docker exec -it 容器名称|容器id bash# 4.外部查看mysql日志 docker logs 容器名称|容器id# 5.使用自定义配置参数 docker run --name mysql -v /root/mysql/conf.d:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=root -d mysql:tag# 6.将容器数据位置与宿主机位置挂载保证数据安全 docker run --name mysql -v /root/mysql/data:/var/lib/mysql -v /root/mysql/conf.d:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -d mysql:tag# 7.通过其他客户端访问 如在window系统|macos系统使用客户端工具访问 # 8.将mysql数据库备份为sql文件 docker exec mysql|容器id sh -c 'exec mysqldump --all-databases -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;' &gt; /root/all-databases.sql --导出全部数据 docker exec mysql sh -c 'exec mysqldump --databases 库表 -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;' &gt; /root/all-databases.sql --导出指定库数据 docker exec mysql sh -c 'exec mysqldump --no-data --databases 库表 -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;' &gt; /root/all-databases.sql --导出指定库数据不要数据# 9.执行sql文件到mysql中 docker exec -i mysql sh -c 'exec mysql -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;' &lt; /root/xxx.sql 安装Redis服务12345678910111213141516171819202122232425262728# 1.在docker hub搜索redis镜像 docker search redis# 2.拉取redis镜像到本地 docker pull redis# 3.启动redis服务运行容器 docker run --name redis -d redis:tag (没有暴露外部端口) docker run --name redis -p 6379:6379 -d redis:tag (暴露外部宿主机端口为6379进行连接) # 4.查看启动日志 docker logs -t -f 容器id|容器名称# 5.进入容器内部查看 docker exec -it 容器id|名称 bash # 6.加载外部自定义配置启动redis容器 默认情况下redis官方镜像中没有redis.conf配置文件 需要去官网下载指定版本的配置文件 1. wget http://download.redis.io/releases/redis-5.0.8.tar.gz 下载官方安装包 2. 将官方安装包中配置文件进行复制到宿主机指定目录中如 /root/redis/redis.conf文件 3. 修改需要自定义的配置 bind 0.0.0.0 开启远程权限 appenonly yes 开启aof持久化 4. 加载配置启动 docker run --name redis -v /root/redis:/usr/local/etc/redis -p 6379:6379 -d redis redis-server /usr/local/etc/redis/redis.conf # 7.将数据目录挂在到本地保证数据安全 docker run --name redis -v /root/redis/data:/data -v /root/redis/redis.conf:/usr/local/etc/redis/redis.conf -p 6379:6379 -d redis redis-server /usr/local/etc/redis/redis.conf 安装Nginx123456789101112131415161718192021222324252627# 1.在docker hub搜索nginx docker search nginx# 2.拉取nginx镜像到本地 [root@localhost ~]# docker pull nginx Using default tag: latest latest: Pulling from library/nginx afb6ec6fdc1c: Pull complete b90c53a0b692: Pull complete 11fa52a0fdc0: Pull complete Digest: sha256:30dfa439718a17baafefadf16c5e7c9d0a1cde97b4fd84f63b69e13513be7097 Status: Downloaded newer image for nginx:latest docker.io/library/nginx:latest# 3.启动nginx容器 docker run -p 80:80 --name nginx01 -d nginx# 4.进入容器 docker exec -it nginx01 /bin/bash 查找目录: whereis nginx 配置文件: /etc/nginx/nginx.conf# 5.复制配置文件到宿主机 docker cp nginx01(容器id|容器名称):/etc/nginx/nginx.conf 宿主机名录# 6.挂在nginx配置以及html到宿主机外部 docker run --name nginx02 -v /root/nginx/nginx.conf:/etc/nginx/nginx.conf -v /root/nginx/html:/usr/share/nginx/html -p 80:80 -d nginx 安装Tomcat123456789101112131415# 1.在docker hub搜索tomcat docker search tomcat# 2.下载tomcat镜像 docker pull tomcat# 3.运行tomcat镜像 docker run -p 8080:8080 -d --name mytomcat tomcat# 4.进入tomcat容器 docker exec -it mytomcat /bin/bash# 5.将webapps目录挂载在外部 docker run -p 8080:8080 -v /root/webapps:/usr/local/tomcat/webapps -d --name mytomcat tomcat 安装MongoDB数据库12345678910111213141516171819# 1.运行mongDB docker run -d -p 27017:27017 --name mymongo mongo ---无须权限 docker logs -f mymongo --查看mongo运行日志# 2.进入mongodb容器 docker exec -it mymongo /bin/bash 直接执行mongo命令进行操作# 3.常见具有权限的容器 docker run --name mymongo -p 27017:27017 -d mongo --auth# 4.进入容器配置用户名密码 mongo use admin 选择admin库 db.createUser({user:&quot;root&quot;,pwd:&quot;root&quot;,roles:[{role:'root',db:'admin'}]}) //创建用户,此用户创建成功,则后续操作都需要用户认证 exit# 5.将mongoDB中数据目录映射到宿主机中 docker run -d -p 27017:27017 -v /root/mongo/data:/data/db --name mymongo mongo 安装ElasticSearch 注意:​调高JVM线程数限制数量 拉取镜像运行elasticsearch123456# 1.dockerhub 拉取镜像 docker pull elasticsearch:6.4.2# 2.查看docker镜像 docker images# 3.运行docker镜像 docker run -p 9200:9200 -p 9300:9300 elasticsearch:6.4.2 启动出现如下错误 预先配置123456789# 1.在centos虚拟机中，修改配置sysctl.conf vim /etc/sysctl.conf# 2.加入如下配置 vm.max_map_count=262144 # 3.启用配置 sysctl -p 注：这一步是为了防止启动容器时，报出如下错误： bootstrap checks failed max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] 启动EleasticSearch容器1234# 0.复制容器中data目录到宿主机中 docker cp 容器id:/usr/share/share/elasticsearch/data /root/es# 1.运行ES容器 指定jvm内存大小并指定ik分词器位置 docker run -d --name es -p 9200:9200 -p 9300:9300 -e ES_JAVA_OPTS=&quot;-Xms128m -Xmx128m&quot; -v /root/es/plugins:/usr/share/elasticsearch/plugins -v /root/es/data:/usr/share/elasticsearch/data elasticsearch:6.4.2 安装IK分词器123456789101112131415161718192021222324252627# 1.下载对应版本的IK分词器 wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.4.2/elasticsearch-analysis-ik-6.4.2.zip# 2.解压到plugins文件夹中 yum install -y unzip unzip -d ik elasticsearch-analysis-ik-6.4.2.zip# 3.添加自定义扩展词和停用词 cd plugins/elasticsearch/config vim IKAnalyzer.cfg.xml &lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;ext_dict.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;ext_stopwords.dic&lt;/entry&gt; &lt;/properties&gt;# 4.在ik分词器目录下config目录中创建ext_dict.dic文件 编码一定要为UTF-8才能生效 vim ext_dict.dic 加入扩展词即可# 5. 在ik分词器目录下config目录中创建ext_stopword.dic文件 vim ext_stopwords.dic 加入停用词即可# 6.重启容器生效 docker restart 容器id# 7.将此容器提交成为一个新的镜像 docker commit -a=&quot;xiaochen&quot; -m=&quot;es with IKAnalyzer&quot; 容器id xiaochen/elasticsearch:6.4.2 安装Kibana12345# 1.下载kibana镜像到本地 docker pull kibana:6.4.2# 2.启动kibana容器 docker run -d --name kibana -e ELASTICSEARCH_URL=http://10.15.0.3:9200 -p 5601:5601 kibana:6.4.2 Docker中出现如下错误解决方案12[root@localhost ~]# docker search mysql 或者 docker pull 这些命令无法使用Error response from daemon: Get https://index.docker.io/v1/search?q=mysql&amp;n=25: x509: certificate has expired or is not yet valid 注意:这个错误的原因在于是系统的时间和docker hub时间不一致,需要做系统时间与网络时间同步 1234567# 1.安装时间同步 sudo yum -y install ntp ntpdate# 2.同步时间 sudo ntpdate cn.pool.ntp.org# 3.查看本机时间 date# 4.从新测试 Dockerfile什么是DockerfileDockerfile可以认为是Docker镜像的描述文件，是由一系列命令和参数构成的脚本。主要作用是用来构建docker镜像的构建文件。 ​​ 通过架构图可以看出通过DockerFile可以直接构建镜像 Dockerfile解析过程 Dockerfile的保留命令官方说明:https://docs.docker.com/engine/reference/builder/ 保留字 作用 FROM 当前镜像是基于哪个镜像的 第一个指令必须是FROM MAINTAINER 镜像维护者的姓名和邮箱地址 RUN 构建镜像时需要运行的指令 EXPOSE 当前容器对外暴露出的端口号 WORKDIR 指定在创建容器后，终端默认登录进来的工作目录，一个落脚点 ENV 用来在构建镜像过程中设置环境变量 ADD 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar包 COPY 类似于ADD，拷贝文件和目录到镜像中​将从构建上下文目录中&lt;原路径&gt;的文件/目录复制到新的一层的镜像内的&lt;目标路径&gt;位置 VOLUME 容器数据卷，用于数据保存和持久化工作 CMD 指定一个容器启动时要运行的命令​Dockerfile中可以有多个CMD指令，但只有最后一个生效，CMD会被docker run之后的参数替换 ENTRYPOINT 指定一个容器启动时要运行的命令​ENTRYPOINT的目的和CMD一样，都是在指定容器启动程序及其参数 FROM 命令 基于那个镜像进行构建新的镜像,在构建时会自动从docker hub拉取base镜像 必须作为Dockerfile的第一个指令出现 语法: 123FROM &lt;image&gt;FROM &lt;image&gt;[:&lt;tag&gt;] 使用版本不写为latestFROM &lt;image&gt;[@&lt;digest&gt;] 使用摘要 MAINTAINER 命令 镜像维护者的姓名和邮箱地址[废弃] 语法: 1MAINTAINER &lt;name&gt; RUN 命令 RUN指令将在当前映像之上的新层中执行任何命令并提交结果。生成的提交映像将用于Dockerfile中的下一步 语法: 12345RUN &lt;command&gt; (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows)RUN echo helloRUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec form)RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;] EXPOSE 命令 用来指定构建的镜像在运行为容器时对外暴露的端口 语法: 12EXPOSE 80/tcp 如果没有显示指定则默认暴露都是tcpEXPOSE 80/udp CMD 命令 用来为启动的容器指定执行的命令,在Dockerfile中只能有一条CMD指令。如果列出多个命令，则只有最后一个命令才会生效。 注意: Dockerfile中只能有一条CMD指令。如果列出多个命令，则只有最后一个命令才会生效。 语法: 123CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec form, this is the preferred form)CMD [&quot;param1&quot;,&quot;param2&quot;] (as default parameters to ENTRYPOINT)CMD command param1 param2 (shell form) WORKDIR 命令 用来为Dockerfile中的任何RUN、CMD、ENTRYPOINT、COPY和ADD指令设置工作目录。如果WORKDIR不存在，即使它没有在任何后续Dockerfile指令中使用，它也将被创建。 语法: 123456WORKDIR /path/to/workdirWORKDIR /aWORKDIR bWORKDIR c`注意:WORKDIR指令可以在Dockerfile中多次使用。如果提供了相对路径，则该路径将与先前WORKDIR指令的路径相对` ENV 命令 用来为构建镜像设置环境变量。这个值将出现在构建阶段中所有后续指令的环境中。 语法： 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key&gt;=&lt;value&gt; ... ADD 命令 用来从context上下文复制新文件、目录或远程文件url，并将它们添加到位于指定路径的映像文件系统中。 语法: 12345ADD hom* /mydir/ 通配符添加多个文件ADD hom?.txt /mydir/ 通配符添加ADD test.txt relativeDir/ 可以指定相对路径ADD test.txt /absoluteDir/ 也可以指定绝对路径ADD url COPY 命令 用来将context目录中指定文件复制到镜像的指定目录中 语法: 12COPY src destCOPY [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] VOLUME 命令 用来定义容器运行时可以挂在到宿主机的目录 语法: 1VOLUME [&quot;/data&quot;] ENTRYPOINT命令 用来指定容器启动时执行命令和CMD类似 语法: 12 [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]ENTRYPOINT command param1 param2 ENTRYPOINT指令，往往用于设置容器启动后的第一个命令，这对一个容器来说往往是固定的。CMD指令，往往用于设置容器启动的第一个命令的默认参数，这对一个容器来说可以是变化的。 ENTRYPOINT命令Dockerfile构建springboot项目部署准备springboot可运行项目 将可运行项目放入linux虚拟机中 编写Dockerfile123456FROM openjdk:8WORKDIR /emsADD ems.jar /emsEXPOSE 8989ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;]CMD [&quot;ems.jar&quot;] 构建镜像1[root@localhost ems]# docker build -t ems . 运行镜像1[root@localhost ems]# docker run -p 8989:8989 ems 访问项目1http://10.15.0.8:8989/ems/login.html 高级网络配置说明当 Docker 启动时，会自动在主机上创建一个 docker0 虚拟网桥，实际上是 Linux 的一个 bridge，可以理解为一个软件交换机。它会在挂载到它的网口之间进行转发。 同时，Docker 随机分配一个本地未占用的私有网段（在 RFC1918 中定义）中的一个地址给 docker0 接口。比如典型的 172.17.42.1，掩码为 255.255.0.0。此后启动的容器内的网口也会自动分配一个同一网段（172.17.0.0/16）的地址。 当创建一个 Docker 容器的时候，同时会创建了一对 veth pair 接口（当数据包发送到一个接口时，另外一个接口也可以收到相同的数据包）。这对接口一端在容器内，即 eth0；另一端在本地并被挂载到 docker0 网桥，名称以 veth 开头（例如 vethAQI2QT）。通过这种方式，主机可以跟容器通信，容器之间也可以相互通信。Docker 就创建了在主机和所有容器之间一个虚拟共享网络。 查看网络信息1# docker network ls 创建一个网桥1# docker network create -d bridge 网桥名称 删除一个网桥1# docker network rm 网桥名称 容器之前使用网络通信12# 1.查询当前网络配置- docker network ls 1234NETWORK ID NAME DRIVER SCOPE8e424e5936b7 bridge bridge local17d974db02da docker_gwbridge bridge locald6c326e433f7 host host local 12# 2.创建桥接网络- docker network create -d bridge info 12345678[root@centos ~]# docker network create -d bridge info6e4aaebff79b1df43a064e0e8fdab08f52d64ce34db78dd5184ce7aaaf550a2f[root@centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPE8e424e5936b7 bridge bridge local17d974db02da docker_gwbridge bridge locald6c326e433f7 host host local6e4aaebff79b info bridge local 1234# 3.启动容器指定使用网桥- docker run -d -p 8890:80 --name nginx001 --network info nginx - docker run -d -p 8891:80 --name nginx002 --network info nginx `注意:一旦指定网桥后--name指定名字就是主机名,多个容器指定在同一个网桥时,可以在任意一个容器中使用主机名与容器进行互通` 12345678910111213141516[root@centos ~]# docker run -d -p 8890:80 --name nginx001 --network info nginx c315bcc94e9ddaa36eb6c6f16ca51592b1ac8bf1ecfe9d8f01d892f3f10825fe[root@centos ~]# docker run -d -p 8891:80 --name nginx002 --network info nginxf8682db35dd7fb4395f90edb38df7cad71bbfaba71b6a4c6e2a3a525cb73c2a5[root@centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf8682db35dd7 nginx &quot;/docker-entrypoint.…&quot; 3 seconds ago Up 2 seconds 0.0.0.0:8891-&gt;80/tcp nginx002c315bcc94e9d nginx &quot;/docker-entrypoint.…&quot; 7 minutes ago Up 7 minutes 0.0.0.0:8890-&gt;80/tcp nginx001b63169d43792 mysql:5.7.19 &quot;docker-entrypoint.s…&quot; 7 minutes ago Up 7 minutes 3306/tcp mysql_mysql.1.s75qe5kkpwwttyf0wrjvd2cda[root@centos ~]# docker exec -it f8682db35dd7 /bin/bashroot@f8682db35dd7:/# curl http://nginx001&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;..... 高级数据卷配置说明数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性： 数据卷 可以在容器之间共享和重用 对 数据卷 的修改会立马生效 对 数据卷 的更新，不会影响镜像 数据卷 默认会一直存在，即使容器被删除 注意：数据卷 的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会复制到数据卷中（仅数据卷为空时会复制）。 创建数据卷12[root@centos ~]# docker volume create my-volmy-vol 查看数据卷123456789101112[root@centos ~]# docker volume inspect my-vol [ { &quot;CreatedAt&quot;: &quot;2020-11-25T11:43:56+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;, &quot;Name&quot;: &quot;my-vol&quot;, &quot;Options&quot;: {}, &quot;Scope&quot;: &quot;local&quot; }] 挂载数据卷1234567891011121314[root@centos ~]# docker run -d -P --name web -v my-vol:/usr/share/nginx/html nginx[root@centos ~]# docker inspect web &quot;Mounts&quot;: [ { &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;my-vol&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;, &quot;Destination&quot;: &quot;/usr/share/nginx/html&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;z&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; } ], 删除数据卷1docker volume rm my-vol Docker Compose简介Compose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。从功能上看，跟 OpenStack 中的 Heat 十分类似。 其代码目前在 https://github.com/docker/compose 上开源。 Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig。 通过第一部分中的介绍，我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。 Compose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 安装与卸载1.linux 在 Linux 上的也安装十分简单，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。例如，在 Linux 64 位系统上直接下载对应的二进制包。 12$ sudo curl -L https://github.com/docker/compose/releases/download/1.25.5/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 2.macos、window Compose 可以通过 Python 的包管理工具 pip 进行安装，也可以直接下载编译好的二进制文件使用，甚至能够直接在 Docker 容器中运行。Docker Desktop for Mac/Windows 自带 docker-compose 二进制文件，安装 Docker 之后可以直接使用。 3.bash命令补全1$ curl -L https://raw.githubusercontent.com/docker/compose/1.25.5/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose 4.卸载 如果是二进制包方式安装的，删除二进制文件即可。 1$ sudo rm /usr/local/bin/docker-compose 5.测试安装成功12$ docker-compose --version docker-compose version 1.25.5, build 4667896b docker compose使用1# 1.相关概念 首先介绍几个术语。 服务 (service)：一个应用容器，实际上可以运行多个相同镜像的实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元。∂一个项目可以由多个服务（容器）关联而成，Compose 面向项目进行管理。 1# 2.场景 最常见的项目是 web 网站，该项目应该包含 web 应用和缓存。 springboot应用 mysql服务 redis服务 elasticsearch服务 ……. 12# 3.docker-compose模板- 参考文档:https://docker_practice.gitee.io/zh-cn/compose/compose_file.html 12345678910111213141516171819202122232425262728293031version: &quot;3.0&quot;services: mysqldb: image: mysql:5.7.19 container_name: mysql ports: - &quot;3306:3306&quot; volumes: - /root/mysql/conf:/etc/mysql/conf.d - /root/mysql/logs:/logs - /root/mysql/data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: root networks: - ems depends_on: - redis redis: image: redis:4.0.14 container_name: redis ports: - &quot;6379:6379&quot; networks: - ems volumes: - /root/redis/data:/data command: redis-server networks: ems: 12# 4.通过docker-compose运行一组容器- 参考文档:https://docker_practice.gitee.io/zh-cn/compose/commands.html 12[root@centos ~]# docker-compose up //前台启动一组服务[root@centos ~]# docker-compose up -d //后台启动一组服务 docker-compose 模板文件模板文件是使用 Compose 的核心，涉及到的指令关键字也比较多。但大家不用担心，这里面大部分指令跟 docker run 相关参数的含义都是类似的。 默认的模板文件名称为 docker-compose.yml，格式为 YAML 格式。 123456789version: &quot;3&quot;services: webapp: image: examples/web ports: - &quot;80:80&quot; volumes: - &quot;/data&quot; 注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。 如果使用 build 指令，在 Dockerfile 中设置的选项(例如：CMD, EXPOSE, VOLUME, ENV 等) 将会自动被获取，无需在 docker-compose.yml 中重复设置。 下面分别介绍各个指令的用法。 build指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。 12345version: '3'services: webapp: build: ./dir 你也可以使用 context 指令指定 Dockerfile 所在文件夹的路径。 使用 dockerfile 指令指定 Dockerfile 文件名。 使用 arg 指令指定构建镜像时的变量。 123456789version: '3'services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 command覆盖容器启动后默认执行的命令。 1command: echo &quot;hello world&quot; container_name指定容器名称。默认将会使用 项目名称_服务名称_序号 这样的格式。 1container_name: docker-web-container 注意: 指定容器名称后，该服务将无法进行扩展（scale），因为 Docker 不允许多个容器具有相同的名称。 depends_on解决容器的依赖、启动先后的问题。以下例子中会先启动 redis db 再启动 web 1234567891011121314version: '3'services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 注意：web 服务不会等待 redis db 「完全启动」之后才启动。 env_file从文件中获取环境变量，可以为单独的文件路径或列表。 如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file 中变量的路径会基于模板文件路径。 如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。 123456env_file: .envenv_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境变量文件中每一行必须符合格式，支持 # 开头的注释行。 12# common.env: Set development environmentPROG_ENV=development environment设置环境变量。你可以使用数组或字典两种格式。 只给定名称的变量会自动获取运行 Compose 主机上对应变量的值，可以用来防止泄露不必要的数据。 1234567environment: RACK_ENV: development SESSION_SECRET:environment: - RACK_ENV=development - SESSION_SECRET 如果变量名称或者值中用到 true|false，yes|no 等表达 布尔 含义的词汇，最好放到引号里，避免 YAML 自动解析某些内容为对应的布尔语义。这些特定词汇，包括 1y|Y|yes|Yes|YES|n|N|no|No|NO|true|True|TRUE|false|False|FALSE|on|On|ON|off|Off|OFF healthcheck通过命令检查容器是否健康运行。 12345healthcheck: test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost&quot;] interval: 1m30s timeout: 10s retries: 3 image指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉取这个镜像。 123image: ubuntuimage: orchardup/postgresqlimage: a4bc65fd networks配置容器连接的网络。 1234567891011version: &quot;3&quot;services: some-service: networks: - some-network - other-networknetworks: some-network: other-network: ports暴露端口信息。 使用宿主端口：容器端口 (HOST:CONTAINER) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。 12345ports: - &quot;3000&quot; - &quot;8000:8000&quot; - &quot;49100:22&quot; - &quot;127.0.0.1:8001:8001&quot; *注意：当使用 ​HOST:CONTAINER*​ * 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 ​YAML*​ * 会自动解析 ​xx:yy​ * 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。 sysctls配置容器内核参数。 1234567sysctls: net.core.somaxconn: 1024 net.ipv4.tcp_syncookies: 0sysctls: - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 ulimits指定容器的 ulimits 限制值。 例如，指定最大进程数为 65535，指定文件句柄数为 20000（软限制，应用可以随时修改，不能超过硬限制） 和 40000（系统硬限制，只能 root 用户提高）。 12345ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 volumes数据卷所挂载路径设置。可以设置为宿主机路径(HOST:CONTAINER)或者数据卷名称(VOLUME:CONTAINER)，并且可以设置访问模式 （HOST:CONTAINER:ro）。 该指令中路径支持相对路径。 1234volumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 如果路径为数据卷名称，必须在文件中配置数据卷。 12345678910version: &quot;3&quot;services: my_src: image: mysql:8.0 volumes: - mysql_data:/var/lib/mysqlvolumes: mysql_data: docker-compose 常用命令1. 命令对象与格式对于 Compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到命令影响。 执行 docker-compose [COMMAND] --help 或者 docker-compose help [COMMAND] 可以查看具体某个命令的使用格式。 docker-compose 命令的基本的使用格式是 1docker-compose [-f=&lt;arg&gt;...] [options] [COMMAND] [ARGS...] 2. 命令选项 -f, --file FILE 指定使用的 Compose 模板文件，默认为 docker-compose.yml，可以多次指定。 -p, --project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名。 --x-networking 使用 Docker 的可拔插网络后端特性 --x-network-driver DRIVER 指定网络后端的驱动，默认为 bridge --verbose 输出更多调试信息。 -v, --version 打印版本并退出。 3.命令使用说明up格式为 docker-compose up [options] [SERVICE...]。 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。 链接的服务都将会被自动启动，除非已经处于运行状态。 可以说，大部分时候都可以直接通过该命令来启动一个项目。 默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。 当通过 Ctrl-C 停止命令时，所有容器将会停止。 如果使用 docker-compose up -d，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。 默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容 down 此命令将会停止 up 命令所启动的容器，并移除网络 exec 进入指定的容器。 ps格式为 docker-compose ps [options] [SERVICE...]。 列出项目中目前的所有容器。 选项： -q 只打印容器的 ID 信息。 restart格式为 docker-compose restart [options] [SERVICE...]。 重启项目中的服务。 选项： -t, --timeout TIMEOUT 指定重启前停止容器的超时（默认为 10 秒）。 rm格式为 docker-compose rm [options] [SERVICE...]。 删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。 选项： -f, --force 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。 -v 删除容器所挂载的数据卷。 start格式为 docker-compose start [SERVICE...]。 启动已经存在的服务容器。 stop格式为 docker-compose stop [options] [SERVICE...]。 停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器。 选项： -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。 top查看各个服务容器内运行的进程。 unpause格式为 docker-compose unpause [SERVICE...]。 恢复处于暂停状态中的服务。 可视化管理工具安装Portainer官方安装说明：https://www.portainer.io/installation/ 123456789[root@ubuntu1804 ~]#docker pull portainer/portainer[root@ubuntu1804 ~]#docker volume create portainer_dataportainer_data[root@ubuntu1804 ~]#docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer20db26b67b791648c2ef6aee444a5226a9c897ebcf0160050e722dbf4a4906e3[root@ubuntu1804 ~]#docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES20db26b67b79 portainer/portainer &quot;/portainer&quot; 5 seconds ago Up 4 seconds 0.0.0.0:8000-&gt;8000/tcp, 0.0.0.0:9000-&gt;9000/tcp portainer 登录和使用Portainer 用浏览器访问：http://localhost:9000 ​ 使用过程… Docker 核心原理从表层来看docker的组成部分是由：仓库、镜像、容器三个部分组成，这是基础层面的体现 犹如MySQL事务的特性：ACID（原子性、一致性、隔离性、持久性）但是维持ACID背后的机制/原理是什么？ 答： 持久性是通过 redo log （重做日志）来保证的； 原子性是通过 undo log（回滚日志） 来保证的； 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的； 一致性则是通过持久性+原子性+隔离性来保证； 那么类推docker的三个组成部分或者三个核心概念背后的三个原理是什么支持的？ 答：Namespace、Cgroups、rootfs。 （联想：既然docker中的技术是参考Linux内核实现的，那么这其中究竟有多少联系？） Namespace，做隔离，让进程只能看到Namespace中的世界； Cgroups，做限制，让这个“世界”围着一个看不见的墙。 rootfs，做文件系统，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。 注意此处 Linux Namespace 不要和 k8s Namespace 概念混淆： Linux Namespace 机制：用于资源和视图隔离，使宿主机看不到容器内的资源，容器也看不到其他容器内的资源，实现不同应用的视图隔离，避免干扰 k8s Namespace 机制：就是用户资源的隔离，为了便于管理 k8s 自身的资源 Namespace本质就是一个障眼法，进入容器后ps看到不同的pid，其实就是namespace的幻化，本质上容器就是一个运行的进程，其它的进程实际是pid为1的子进程，namespace提供了许多种的障眼法。 ​ namespace本质是怎么建立的？ 每一次创建容器的时候本质是Linux系统的fork的调用，在fork调用时会传入一些参数，这个会对Linux内核进行控制生成新的namespace rootfs本质是根文件系统，挂载在容器根目录上，为容器提供隔离后执行环境的文件系统，即容器镜像。 容器的rootfs由三部分组成，1：只读层、2：可读写层、3：init层 只读层:都以增量的方式分别包含了操作系统的一部分。 可读写：就是专门用来存放你修改 rootfs 后产生的增量，无论是增、删、改，都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用 docker commit 和 push 指令，保存这个被修改过的可读写层，并上传到 Docker Hub 上，供其他人使用；而与此同时，原先的只读层里的内容则不会有任何变化。这，就是增量 rootfs 的好处。 Init 层：是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。 Cgroups 虽然容器内的第 1 号进程在“障眼法”的干扰下只能看到容器里的情况，但是宿主机上，它作为第 100 号进程与其他所有进程之间依然是平等的竞争关系。这就意味着，虽然第 100 号进程表面上被隔离了起来，但是它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器）占用的。当然，这个 100 号进程自己也可能把所有资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。 而 Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。 Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。 参考： 容器基础之三大基石","link":"/post/docker-z1rrgi5.html"},{"title":"领域算法知识体系","text":"领域算法 最近一直有一个概念在脑子中循环：程序设计 = 数据结构 + 算法 基于目前的认知我想把算法分为常规算法思想和领域算法：常规算法思想如：分治、动态规划、贪心、二分、搜索等；本篇则是领域算法。 不论哪种技术栈或者组件都和算法离不开，比如gin的路由基数树、负载均衡各种算法、分布式一致性算法、和加密相关的安全算法、分布式ID雪花算法，因此准备抽一些时间好好整理并学习下这部分的内容，这部分我称之为领域算法，形成一个完整的知识体系，不求多深入、多底层，但求个人程度上的理解,快速形成知识体系。 ‍ ​​ ‍ 重点关注：负载均衡、雪花、一致性以及一点点大数据相关的，希望形成自己的理解","link":"/post/field-algorithm-1sbsuf.html"},{"title":"多个协程打印相关问题","text":"多个协程打印相关问题​创建三个goroutine，分别输出1 4 7, 2 5 8, 3 6 9, ...... 100， 保证顺序输出1到100​ 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @Author 560463 * @Description //TODO $ 并发：创建三个goroutine，分别输出1 4 7, 2 5 8, 3 6 9, ...... 100， 保证顺序输出1到100, 2种方法 * @Date 2023/10/12 14:36 **/// 实现三个协程并发交替打印数字// 方法一：package mainimport &quot;fmt&quot;var chanDone chan intvar end intvar countA, countB, countC intfunc write(name string, count int, ch, next chan int) { for a := range ch { //fmt.Println(a) count++ fmt.Printf(&quot;协程%s 第%d次打印了%d\\n&quot;, name, count, a) if a &lt; end { next &lt;- a + 1 } else { chanDone &lt;- 0 } }}func main() { ch1, ch2, ch3 := make(chan int, 1), make(chan int, 1), make(chan int, 1) countA, countB, countC = 0, 0, 0 chanDone, end = make(chan int), 100 go write(&quot;A&quot;, countA, ch1, ch2) go write(&quot;B&quot;, countB, ch2, ch3) go write(&quot;C&quot;, countC, ch3, ch1) ch1 &lt;- 1 &lt;-chanDone} ‍ 123456789101112131415161718192021222324252627282930313233343536//方法二：package mainimport &quot;fmt&quot;var wg sync.WaitGroupfunc r(name string, count int, start int, ch1, ch2 chan int) { defer wg.Done() for i := start; i &lt;= 100; { num, ok := &lt;-ch1 if !ok { close(ch2) break } count++ fmt.Printf(&quot;协程%s 第%d次打印了: %d\\n&quot;, name, count, num) i++ ch2 &lt;- i i = i + 2 //如果这样写有个坑：这样写第一次A是打印了1 但是给B的值也是1，虽然后面进行了+3 但是本轮打印时错误的 //ch2 &lt;- i //i = i + 3 }}func main() { chA, chB, chC := make(chan int, 1), make(chan int, 1), make(chan int, 1) countA, countB, countC := 0, 0, 0 wg.Add(3) chA &lt;- 1 go r(&quot;A&quot;, countA, 1, chA, chB) go r(&quot;B&quot;, countB, 2, chB, chC) go r(&quot;C&quot;, countC, 3, chC, chA) wg.Wait()} ‍ 结果： ​​ ​​ ‍ ‍ ​三个协程分别打印100次 cat dog fish​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package mainimport ( &quot;fmt&quot; &quot;sync&quot;)// 三个协程交替打印 cat dog fishvar repeatCount = 100func main() { // wg 用来防止主协程提前先退出 wg := &amp;sync.WaitGroup{} wg.Add(3) chCat := make(chan struct{}, 1) defer close(chCat) chDog := make(chan struct{}, 1) defer close(chDog) chFish := make(chan struct{}, 1) defer close(chFish) go printAnimal(chCat, chDog, &quot;cat&quot;, wg) go printAnimal(chDog, chFish, &quot;dog&quot;, wg) go printAnimal(chFish, chCat, &quot;fish&quot;, wg) chCat &lt;- struct{}{} wg.Wait()}// wg 需要传指针func printAnimal(in, out chan struct{}, s string, wg *sync.WaitGroup) { count := 0 for { &lt;-in count++ fmt.Printf(&quot;第%d次打印%s\\n&quot;, count, s) out &lt;- struct{}{} if count &gt;= repeatCount { wg.Done() return } }} ‍ ​​","link":"/post/concorded-programming-multiple-corporate-printing-zpbnes.html"},{"title":"全局理解：微服务架构演进","text":"全局理解：微服务架构演进 本文参考并引用楼仔的文章作为基础并结合自己的一些理解，做一下微服务演进的链路，后续还会结合补充其它一些好的见解，希望更好地从全局角度理解微服务，但不做技术细节深究。 思考点：单体、单体到微服务的转变、微服务的瓶颈； 微服务涉及到的技术手段、工具组件以及为什么用这些工具 提升点：培养微服务的宏观意识，不想当架构师的开发者不是一个好的自律人。 以电商为例的演进前言本文将介绍微服务架构和相关的组件，介绍他们是什么，以及为什么要使用微服务架构和这些组件。 从单体应用到微服务并不是一蹴而就的，这是一个逐渐演变的过程。本文将以一个网上超市应用为例来说明这一过程。 创业思维-&gt;产品卖点-&gt;产生功能需求和业务-&gt;技术驱动-&gt;技术迭代-&gt;产品迭代升级 ​​ 最初的需求几年前，小明和小皮一起创业做网上超市。小明负责程序开发，小皮负责其他事宜。当时互联网还不发达，网上超市还是蓝海。只要功能实现了就能随便赚钱。所以他们的需求很简单，只需要一个网站挂在公网，用户能够在这个网站上浏览商品、购买商品；另外还需一个管理后台，可以管理商品、用户、以及订单数据 。 我们整理一下功能清单： 1）网站 用户注册、登录功能 商品展示 下单 2）管理后台 ​用户管理 商品管理 订单管理 由于需求简单，小明左手右手一个慢动作，网站就做好了。管理后台出于安全考虑，不和网站做在一起，小明右手左手慢动作重播，管理网站也做好了。总体架构图如下： ​​ 小明挥一挥手，找了家云服务部署上去，网站就上线了。上线后好评如潮，深受各类肥宅喜爱。小明小皮美滋滋地开始躺着收钱。 业务发展：单体瓶颈出现好景不长，没过几天，各类网上超市紧跟着拔地而起，对小明小皮造成了强烈的冲击。 在竞争的压力下，小明小皮决定开展一些营销手段： ​​开展促销活动。比如元旦全场打折，春节买二送一，情人节狗粮优惠券等等。 拓展渠道，新增移动端营销。除了网站外，还需要开发移动端APP，微信小程序等。 精准营销。利用历史数据对用户进行分析，提供个性化服务。 …… 这些​​活动都需要程序开发的支持。小明拉了同学小红加入团队。小红负责数据分析以及移动端相关开发。小明负责促销活动相关功能的开发。 因为开发任务比较紧迫，小明小红没有好好规划整个系统的架构，随便拍了拍脑袋，决定把促销管理和数据分析放在管理后台里，微信和移动端APP另外搭建。通宵了几天后，新功能和新应用基本完工。这时的架构图如下： ​​ 这一阶段存在很多不合理的地方： 网站和移动端应用有很多相同***业务逻辑的重复代码***。 数据有时候通过数据库共享，有时候通过接口调用传输。接口调用关系杂乱。 单个应用为了给其他应用提供接口，渐渐地越改越大，包含了很多本来就不属于它的逻辑。应用边界模糊，功能归属混乱。 管理后台在一开始的设计中保障级别较低。加入数据分析和促销管理相关功能后出现性能瓶颈，影响了其他应用。 数据库**表结构被多个应用依赖**，无法重构和优化。 单库瓶颈： 所有应用都在一个数据库上操作，数据库出现性能瓶颈。特别是数据分析跑起来的时候，数据库性能急剧下降。 开发、测试、部署、维护愈发困难。即使只改动一个小功能，也需要整个应用一起发布。有时候发布会不小心带上了一些未经测试的代码，或者修改了一个功能后，另一个意想不到的地方出错了。为了减轻发布可能产生的问题的影响和线上业务停顿的影响，所有应用都要在凌晨三四点执行发布。发布后为了验证应用正常运行，还得盯到第二天白天的用户高峰期……**需要CICD拯救** 团队出现推诿扯皮现象。关于一些公用的功能应该建设在哪个应用上的问题常常要争论很久，最后要么干脆各做各的，或者随便放个地方但是都不维护。需要职责单一 尽管有着诸多问题，但也不能否认这一阶段的成果：快速地根据业务变化建设了系统。不过紧迫且繁重的任务容易使人陷入局部、短浅的思维方式，从而做出妥协式的决策。在这种架构中，每个人都只关注在自己的一亩三分地，缺乏全局的、长远的设计。长此以往，系统建设将会越来越困难，甚至陷入不断推翻、重建的循环。 拆分幸好小明和小红是有追求有理想的好青年。意识到问题后，小明和小红从琐碎的业务需求中腾出了一部分精力，开始梳理整体架构，针对问题准备着手改造。 要做改造，首先你需要有足够的精力和资源。如果你的需求方（业务人员、项目经理、上司等）很强势地一心追求需求进度，以至于你无法挪出额外的精力和资源的话，那么你可能无法做任何事…… 在编程的世界中，最重要的便是抽象能力。微服务改造的过程实际上也是个抽象的过程。小明和小红整理了网上超市的业务逻辑，抽象出公用的业务能力，做成几个公共服务： 用户服务 商品服务 促销服务 订单服务 数据分析服务 各个应用后台只需从这些服务获取所需的数据，从而删去了大量冗余的代码，就剩个轻薄的控制层和前端。这一阶段的架构如下： ​​ 这个阶段只是将服务分开了，数据库依然是共用的，所以一些烟囱式系统的缺点仍然存在： 数据库成为性能瓶颈，并且有单点故障的风险。 数据管理趋向混乱。即使一开始有良好的模块化设计，随着时间推移，总会有一个服务直接从数据库取另一个服务的数据的现象。 数据库表结构可能被多个服务依赖，牵一发而动全身，很难调整。 如果一直保持共用数据库的模式，则整个架构会越来越僵化，失去了微服务架构的意义。因此小明和小红一鼓作气，把数据库也拆分了。所有持久化层相互隔离，由各个服务自己负责。另外，为了提高系统的实时性，加入了消息队列机制。架构如下： ​​ 完全拆分后各个服务可以采用异构的技术。比如数据分析服务可以使用数据仓库作为持久化层，以便于高效地做一些统计计算；商品服务和促销服务访问频率比较大，因此加入了缓存机制等。 还有一种抽象出公共逻辑的方法是把这些公共逻辑做成公共的框架库。这种方法可以减少服务调用的性能损耗。但是这种方法的管理成本非常高昂，很难保证所有应用版本的一致性。 数据库拆分也有一些问题和挑战：比如说跨库级联的需求，通过服务查询数据颗粒度的粗细问题等。但是这些问题可以通过合理的设计来解决。总体来说，数据库拆分是一个利大于弊的。 微服务架构还有一个技术外的好处，它使整个系统的分工更加明确，责任更加清晰，每个人专心负责为其他人提供更好的服务。在单体应用的时代，公共的业务功能经常没有明确的归属。最后要么各做各的，每个人都重新实现了一遍；要么是随机一个人（一般是能力比较强或者比较热心的人）做到他负责的应用里面。 在后者的情况下，这个人在负责自己应用之外，还要额外负责给别人提供这些公共的功能——而这个功能本来是无人负责的，仅仅因为他能力较强/比较热心，就莫名地背锅（这种情况还被美其名曰能者多劳）。结果最后大家都不愿意提供公共的功能。长此以往，团队里的人渐渐变得各自为政，不再关心全局的架构设计。 微服务架构本质从管理上也是一种手段，而不止步于技术层面 改造完成后，小明和小红分清楚各自的锅。两人十分满意，一切就像是麦克斯韦方程组一样漂亮完美。 然而…… 没有银弹“没有银弹”是一个常用的成语，意思是指没有一种简单而完美的解决方法或办法。这个成语通常用于形容解决复杂问题时的困难和复杂性。我这里更想说高可用没有银弹，强如阿里，双十一语雀崩溃、阿里全系崩溃、BillBill技术团队刚发布高可用落地实践就崩了还泄漏源码······ 春天来了，万物复苏，又到了一年一度的购物狂欢节。眼看着日订单数量蹭蹭地上涨，小皮小明小红喜笑颜开。可惜好景不长，乐极生悲，突然嘣的一下，系统挂了。 以往单体应用，排查问题通常是看一下日志，研究错误信息和调用堆栈。而微服务架构整个应用分散成多个服务，***定位故障点非常困难**。小明一台机器一台机器地查看日志*，一个服务一个服务的手工调用。经过十几分钟的查找，小明终于定位到故障点：促销服务由于接收的请求量太大而停止响应了。其他服务都直接或间接地会调用促销服务，于是也跟着宕机了。 在微服务架构中，一个服务故障可能会产生雪崩效用，导致整个系统故障。 其实在节前，小明和小红是有做过请求量评估的。按照预计，服务器资源是足以支持节日的请求量的，所以肯定是哪里出了问题。不过形势紧急，随着每一分每一秒流逝的都是白花花的银子，因此小明也没时间排查问题，当机立断在云上新建了几台虚拟机，然后一台一台地部署新的促销服务节点。几分钟的操作后，系统总算是勉强恢复正常了。整个故障时间内估计损失了几十万的销售额，三人的心在滴血…… 引入日志系统，才能方便排查故障 事后，小明简单写了个日志分析工具（量太大了，文本编辑器几乎打不开，打开了肉眼也看不过来），统计了促销服务的访问日志，发现在故障期间，商品服务由于代码问题，在某些场景下会对促销服务发起大量请求。这个问题并不复杂，小明手指抖一抖，修复了这个价值几十万的Bug。 问题是解决了，但谁也无法保证不会再发生类似的其他问题。微服务架构虽然逻辑设计上看是完美的，但就像积木搭建的华丽宫殿一样，经不起风吹草动。微服务架构虽然解决了旧问题，也引入了新的问题： 微服务架构整个应用分散成多个服务，定位故障点非常困难。 稳定性下降。服务数量变多导致其中一个服务出现故障的概率增大，并且一个服务故障可能导致整个系统挂掉。事实上，在大访问量的生产场景下，故障总是会出现的。 服务数量非常多，部署、管理的工作量很大。 开发方面：如何保证各个服务在持续开发的情况下仍然保持协同合作。 测试方面：服务拆分后，几乎所有功能都会涉及多个服务。原本单个程序的测试变为服务间调用的测试。测试变得更加复杂。 小明小红痛定思痛，决心好好解决这些问题。对故障的处理一般从两方面入手，一方面尽量减少故障发生的概率，另一方面降低故障造成的影响。 ​​ 故障预测 - 监控系统我个人更喜欢将建立监控系统的核心目的是为了提前做故障的预测。公司内部的实际案例：也是电商的部门，后台服务（具体服务不清楚，电商同事没说）的磁盘满了，第二天人就不在了，大致过程是磁盘监控中占用80%的时候没有扩建或者类似转移的操作，下一周突然就崩溃了，造成了一些损失（电商部门真金白银） 在高并发分布式的场景下，故障经常是突然间就雪崩式爆发。所以必须建立完善的监控体系，尽可能发现故障的征兆。 微服务架构中组件繁多，各个组件所需要监控的指标不同。比如Redis缓存一般监控占用内存值、网络流量，数据库监控连接数、磁盘空间，业务服务监控并发数、响应延迟、错误率等。因此如果做一个大而全的监控系统来监控各个组件是不大现实的，而且扩展性会很差。一般的做法是让各个组件提供报告自己当前状态的接口（metrics接口），这个接口输出的数据格式应该是一致的。然后部署一个指标采集器组件，定时从这些接口获取并保持组件状态，同时提供查询服务。最后还需要一个UI，从指标采集器查询各项指标，绘制监控界面或者根据阈值发出告警。 大部分组件都不需要自己动手开发，网络上有开源组件。小明下载了RedisExporter和MySQLExporter，这两个组件分别提供了Redis缓存和MySQL数据库的指标接口。微服务则根据各个服务的业务逻辑实现自定义的指标接口。然后小明采用Prometheus作为指标采集器，Grafana配置监控界面和邮件告警。这样一套微服务监控系统就搭建起来了： ​​ 定位问题 - 链路跟踪在微服务架构下，一个用户的请求往往涉及多个内部服务调用。为了方便定位问题，需要能够记录每个用户请求时，微服务内部产生了多少服务调用，及其调用关系。这个叫做链路跟踪。 我们用一个Istio文档里的链路跟踪例子来看看效果： ​​ 图片来自Istio文档 从图中可以看到，这是一个用户访问productpage页面的请求。在请求过程中，productpage服务顺序调用了details和reviews服务的接口。而reviews服务在响应过程中又调用了ratings的接口。整个链路跟踪的记录是一棵树： ​​ 要实现链路跟踪，每次服务调用会在HTTP的HEADERS中记录至少记录四项数据： traceId：traceId标识一个用户请求的调用链路。具有相同traceId的调用属于同一条链路。 spanId：标识一次服务调用的ID，即链路跟踪的节点ID。 parentId：父节点的spanId。 requestTime &amp; responseTime：请求时间和响应时间。 另外，还需要调用日志收集与存储的组件，以及展示链路调用的UI组件。 ​​ 以上只是一个极简的说明，关于链路跟踪的理论依据可详见Google的Dapper 了解了理论基础后，小明选用了Dapper的一个开源实现Zipkin。然后手指一抖，写了个HTTP请求的拦截器，在每次HTTP请求时生成这些数据注入到HEADERS，同时异步发送调用日志到Zipkin的日志收集器中。这里额外提一下，HTTP请求的拦截器，可以在微服务的代码中实现，也可以使用一个网络代理组件来实现（不过这样子每个微服务都需要加一层代理）。 链路跟踪只能定位到哪个服务出现问题，不能提供具体的错误信息。查找具体的错误信息的能力则需要由日志分析组件来提供。 日志系统 ** - 日志分析**日志分析组件应该在微服务兴起之前就被广泛使用了。即使单体应用架构，当访问数变大、或服务器规模增多时，日志文件的大小会膨胀到难以用文本编辑器进行访问，更糟的是它们分散在多台服务器上面。排查一个问题，需要登录到各台服务器去获取日志文件，一个一个地查找（而且打开、查找都很慢）想要的日志信息。 因此，在应用规模变大时，我们需要一个日志的“搜索引擎”。以便于能准确地找到想要的日志。另外，数据源一侧还需要收集日志的组件和展示结果的UI组件： ​​ 小明调查了一下，使用了大名鼎鼎的ELK日志分析组件。ELK是Elasticsearch、Logstash和Kibana三个组件的缩写。 Elasticsearch：搜索引擎，同时也是日志的存储。 Logstash：日志采集器，它接收日志输入，对日志进行一些预处理，然后输出到Elasticsearch。 Kibana：UI组件，通过Elasticsearch的API查找数据并展示给用户。 最后还有一个小问题是如何将日志发送到Logstash。一种方案是在日志输出的时候直接调用Logstash接口将日志发送过去。这样一来又（咦，为啥要用“又”）要修改代码……于是小明选用了另一种方案：日志仍然输出到文件，每个服务里再部署个Agent扫描日志文件然后输出给Logstash。 网关 - 权限控制，服务治理拆分成微服务后，出现大量的服务，大量的接口，使得整个调用关系乱糟糟的。经常在开发过程中，写着写着，忽然想不起某个数据应该调用哪个服务。或者写歪了，调用了不该调用的服务，本来一个只读的功能结果修改了数据…… 为了应对这些情况，微服务的调用需要一个把关的东西，也就是网关。在调用者和被调用者中间加一层网关，每次调用时进行权限校验。另外，网关也可以作为一个提供服务接口文档的平台。 使用网关有一个问题就是要决定在多大粒度上使用：最粗粒度的方案是整个微服务一个网关，微服务外部通过网关访问微服务，微服务内部则直接调用；最细粒度则是所有调用，不管是微服务内部调用或者来自外部的调用，都必须通过网关。折中的方案是按照业务领域将微服务分成几个区，区内直接调用，区间通过网关调用。 由于整个网上超市的服务数量还不算特别多，小明采用的最粗粒度的方案： ​​ 服务注册与发现 - 动态扩容前面的组件，都是旨在降低故障发生的可能性。然而故障总是会发生的，所以另一个需要研究的是如何降低故障产生的影响。 最粗暴的（也是最常用的）故障处理策略就是冗余。一般来说，一个服务都会部署多个实例，这样一来能够分担压力提高性能，二来即使一个实例挂了其他实例还能响应。 冗余的一个问题是使用几个冗余？这个问题在时间轴上并没有一个切确的答案。根据服务功能、时间段的不同，需要不同数量的实例。比如在平日里，可能4个实例已经够用；而在促销活动时，流量大增，可能需要40个实例。因此冗余数量并不是一个固定的值，而是根据需要实时调整的。 一般来说新增实例的操作为： 部署新实例 将新实例注册到负载均衡或DNS上 操作只有两步，但如果注册到负载均衡或DNS的操作为人工操作的话，那事情就不简单了。想想新增40个实例后，要手工输入40个IP的感觉…… 解决这个问题的方案是服务自动注册与发现。首先，需要部署一个服务发现服务，它提供所有已注册服务的地址信息的服务。DNS也算是一种服务发现服务。然后各个应用服务在启动时自动将自己注册到服务发现服务上。并且应用服务启动后会实时（定期）从服务发现服务同步各个应用服务的地址列表到本地。服务发现服务也会定期检查应用服务的健康状态，去掉不健康的实例地址。这样新增实例时只需要部署新实例，实例下线时直接关停服务即可，服务发现会自动检查服务实例的增减。 ​​ 服务发现还会跟客户端负载均衡配合使用。由于应用服务已经同步服务地址列表在本地了，所以访问微服务时，可以自己决定负载策略。甚至可以在服务注册时加入一些元数据（服务版本等信息），客户端负载则根据这些元数据进行流量控制，实现A/B测试、蓝绿发布等功能。 服务发现有很多组件可以选择，比如说Zookeeper 、Eureka、Consul、Etcd等。不过小明觉得自己水平不错，想炫技，于是基于Redis自己写了一个…… 熔断、服务降级、限流熔断 当一个服务因为各种原因停止响应时，调用方通常会等待一段时间，然后超时或者收到错误返回。如果调用链路比较长，可能会导致请求堆积，整条链路占用大量资源一直在等待下游响应。所以当多次访问一个服务失败时，应熔断，标记该服务已停止工作，直接返回错误。直至该服务恢复正常后再重新建立连接。 ​​ 图片来自《微服务设计》 服务降级 当下游服务停止工作后，如果该服务并非核心业务，则上游服务应该降级，以保证核心业务不中断。比如网上超市下单界面有一个推荐商品凑单的功能，当推荐模块挂了后，下单功能不能一起挂掉，只需要暂时关闭推荐功能即可。 限流 一个服务挂掉后，上游服务或者用户一般会习惯性地重试访问。这导致一旦服务恢复正常，很可能因为瞬间网络流量过大又立刻挂掉，在棺材里重复着仰卧起坐。因此服务需要能够自我保护——限流。限流策略有很多，最简单的比如当单位时间内请求数过多时，丢弃多余的请求。另外，也可以考虑分区限流。仅拒绝来自产生大量请求的服务的请求。例如商品服务和订单服务都需要访问促销服务，商品服务由于代码问题发起了大量请求，促销服务则只限制来自商品服务的请求，来自订单服务的请求则正常响应。 ​​ 微服务测试​微服务架构下，测试分为三个层次： 端到端测试：覆盖整个系统，一般在用户界面机型测试。 服务测试：针对服务接口进行测试。 单元测试：针对代码单元进行测试。 三种测试从上到下实施的容易程度递增，但是测试效果递减。端到端测试最费时费力，但是通过测试后我们对系统最有信心。单元测试最容易实施，效率也最高，但是测试后不能保证整个系统没有问题。 ​​ 由于端到端测试实施难度较大，一般只对核心功能做端到端测试。一旦端到端测试失败，则需要将其分解到单元测试：则分析失败原因，然后编写单元测试来重现这个问题，这样未来我们便可以更快地捕获同样的错误。 服务测试的难度在于服务会经常依赖一些其他服务。这个问题可以通过Mock Server解决： ​​ 单元测试大家都很熟悉了。我们一般会编写大量的单元测试（包括回归测试）尽量覆盖所有代码。 微服务框架指标接口、链路跟踪注入、日志引流、服务注册发现、路由规则等组件以及熔断、限流等功能都需要在应用服务上添加一些对接代码。如果让每个应用服务自己实现是非常耗时耗力的。基于DRY的原则，小明开发了一套微服务框架，将与各个组件对接的代码和另外一些公共代码抽离到框架中，所有的应用服务都统一使用这套框架进行开发。 使用微服务框架可以实现很多自定义的功能。甚至可以将程序调用堆栈信息注入到链路跟踪，实现代码级别的链路跟踪。或者输出线程池、连接池的状态信息，实时监控服务底层状态。 使用统一的微服务框架有一个比较严重的问题：框架更新成本很高。每次框架升级，都需要所有应用服务配合升级。当然，一般会使用兼容方案，留出一段并行时间等待所有应用服务升级。但是如果应用服务非常多时，升级时间可能会非常漫长。并且有一些很稳定几乎不更新的应用服务，其负责人可能会拒绝升级……因此，使用统一微服务框架需要完善的版本管理方法和开发管理规范。 另一条路 - Service Mesh另一种抽象公共代码的方法是直接将这些代码抽象到一个反向代理组件。每个服务都额外部署这个代理组件，所有出站入站的流量都通过该组件进行处理和转发。这个组件被称为Sidecar。 Sidecar不会产生额外网络成本。Sidecar会和微服务节点部署在同一台主机上并且共用相同的虚拟网卡。所以sidecar和微服务节点的通信实际上都只是通过内存拷贝实现的。 ​​ 图片来自：Pattern: Service Mesh Sidecar只负责网络通信。还需要有个组件来统一管理所有sidecar的配置。在Service Mesh中，负责网络通信的部分叫数据平面（data plane），负责配置管理的部分叫控制平面（control plane）。数据平面和控制平面构成了Service Mesh的基本架构。 ​​ 图片来自：Pattern: Service Mesh Sevice Mesh相比于微服务框架的优点在于它不侵入代码，升级和维护更方便。它经常被诟病的则是性能问题。即使回环网络不会产生实际的网络请求，但仍然有内存拷贝的额外成本。另外有一些集中式的流量处理也会影响性能。 结束、也是开始微服务不是架构演变的终点。往细走还有Serverless、FaaS等方向。另一方面也有人在唱合久必分分久必合，重新发现单体架构…… 最后，参考楼仔的座右铭：我从清晨走过，也拥抱夜晚的星辰，人生没有捷径，你我皆平凡，你好，陌生人，一起共勉。 ‍","link":"/post/global-understanding-the-evolution-of-microservices-architecture-eww7f.html"},{"title":"Go并发编程 | Mutex","text":"Go并发编程 | Mutex 本篇章志在全面、成体系的认识 Mutex，边学习的同时边总结，力求超越市面上99%的教程，超越的自信源于：全面、有简单应用、有深入源码、有演进历程、有个人理解和思考和持续维护持续修正、最重要的是站在巨人的肩膀上，汲取广大博主的分析精华。 认识到如何学好并发部分的源码：唯有加练，多看多画图，一遍不行就五遍，五遍不行就十遍。 画图工具 参考链接： 案例 初识Mutex 多个 goroutine 并发更新同一个资源，像计数器；同时更新用户的账户信息；秒杀系统；往同一个 buffer 中并发写入数据等等。如果没有互斥控制，就会出现一些异常情况，比如计数器的计数不准确、用户的账户可能出现透支、秒杀系统出现超卖、buffer 中的数据混乱，等等，后果都很严重。 这些问题怎么解决呢？对，用互斥锁，那在 Go 语言里，就是 Mutex。 并发编程中涉及一个概念，叫做临界区。临界区就是一个被共享的资源，或者说是一个整体的一组共享资源，比如对数据库的访问、对某一个共享数据结构的操作、对一个 I/O 设备的使用、对一个连接池中的连接的调用，等等。为防止访问、操作错误，使用互斥锁，限定临界区只能同时由一个线程持有。 ​​ Mutex 是使用最广泛的同步原语（Synchronization primitives，有人也叫做并发原语） 同步原语适用场景： 共享资源。并发地读写共享资源，会出现数据竞争（data race）的问题，所以需要Mutex、RWMutex 这样的并发原语来保护。 任务编排。需要 goroutine 按照一定的规律执行，而 goroutine 之间有相互等待或者依赖的顺序关系，我们常常使用 WaitGroup 或者 Channel 来实现。 消息传递。信息交流以及不同的 goroutine 之间的线程安全的数据交流，常常使用Channel 来实现。 同步原语没有固定标准的定义，通常下并发原语比同步原语更广一些，可以看做解决解决并发问题的一个数据结构。 简单来说，互斥锁 Mutex 就提供两个方法 Lock 和 Unlock：进入临界区之前调用 Lock 方法，退出临界区的时候调用 Unlock 方法： ​func(m *Mutex)Lock()​ ​func(m *Mutex)Unlock()​ 如果不用Mutex锁创建了 10 个 goroutine，同时不断地对一个变量（count）进行加 1操作，每个 goroutine 负责执行 10 万次的加 1 操作，我们期望的最后计数的结果是 10 *100000 = 1000000 (一百万)。 1234567891011121314151617181920212223242526package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func main() { var count = 0 // 使用WaitGroup等待10个goroutine完成a var wg sync.WaitGroup wg.Add(10) for i := 0; i &lt; 10; i++ { go func() { defer wg.Done() // 对变量count执行10次加1 for j := 0; j &lt; 100000; j++ { count++ } }() } // 等待10个goroutine完成 wg.Wait() fmt.Println(count)} 使用 sync.WaitGroup 来等待所有的 goroutine 执行完毕后，再输出最终的结果。 可结果发现，每次结果都不是100w，且每次结果都不一样 ​​ count++ 不是一个原子操作，它至少包含几个步骤，比如读取变量count 的当前值，对这个值加 1，把结果再保存到 count 中。因为不是原子操作，就可能有并发的问题。 比如，10 个 goroutine 同时读取到 count 的值为 100，接着各自按照自己的逻辑加 1，值变成了 101，然后把这个结果再写回到 count 变量。但是，实际上，此时我们增加的总数应该是 10 才对，这里却只增加了 1，好多计数都被“吞”掉了。这是并发访问共享数据的常见错误。 1234// count++操作的汇编代码MOVQ &quot;&quot;.count(SB), AXLEAQ 1(AX), CXMOVQ CX, &quot;&quot;.count(SB) 并发问题排查工具race detector以及原理在上述问题发生时，你可能很知道哪里出了问题，可是在实际业务环境下，可能很复杂，不容易被发现， Go 提供了一个检测并发访问共享资源是否有问题的工具： race detector，它可以帮助我们自动发现程序有没有 data race 的问题。 Go race detector 是基于 Google 的 C/C++ sanitizers 技术实现的，编译器通过探测所有的内存访问，加入代码能监视对这些内存地址的访问（读还是写）。在代码运行的时候，race detector 就能监控到对共享变量的非同步访问，出现 race 的时候，就会打印出警告信息。 这个技术在 Google 内部帮了大忙，探测出了 Chromium 等代码的大量并发问题。Go 1.1中就引入了这种技术，并且一下子就发现了标准库中的 42 个并发问题。现在，race detector 已经成了 Go 持续集成过程中的一部分。 如何使用race detector？ 在编译（compile）、测试（test）或者运行（run）Go 代码的时候，加上 race 参数，就 有可能发现并发问题。比如在上面的例子中，我们可以加上 race 参数运行，检测一下是不 是有并发问题。如果你 go run -race counter.go，就会输出警告信息。 ​​ 这个警告不但会告诉你有并发问题，而且还会告诉你哪个 goroutine 在哪一行对哪个变量有写操作，同时，哪个 goroutine 在哪一行对哪个变量有读操作，就是这些并发的读写访问，引起了 data race。 虽然这个工具使用起来很方便，但是，因为它的实现方式，只能通过真正对实际地址进行读写访问的时候才能探测，所以它并不能在编译的时候发现 data race 的问题。而且，在运行的时候，只有在触发了 data race 之后，才能检测到，如果碰巧没有触发（比如一个data race 问题只能在 2 月 14 号零点或者 11 月 11 号零点才出现），是检测不出来的。 而且，把开启了 race 的程序部署在线上，还是比较影响性能的。运行 go tool compile -race -S counter.go，可以查看计数器例子的代码，重点关注一下 count++ 前后的编译后的代码 ​go tool compile -race -S counter.go​ 是一个 Go 命令，用于编译 Go 源代码并生成汇编输出。它的各个部分含义如下： ​go tool compile​: 这是 Go 编译器的命令行工具，用于将 Go 源代码编译成机器代码或汇编代码。 ​-race​: 这是一个编译器标志，表示启用 Go 的数据竞争检测器。数据竞争检测器可以帮助检测并发程序中的竞争条件。 ​-S​: 这是一个编译器标志，表示生成汇编代码输出。它让编译器生成对应输入源代码的汇编语言表示形式。 太少了，截取部分，在编译的代码中，增加了 runtime.racefuncenter、runtime.raceread、runtime.racewrite、runtime.racefuncexit 等检测 data race 的方法。通过这些插入的指令，Go race detector 工具就能够成功地检测出 data race 问题了。总结一下，通过在编译的时候插入一些指令，在运行时通过这些插入的指令检测并发读写从而发现 data race 问题，就是这个工具的实现机制。 123456789101112131415161718192021222324252627282930313233343536373839404142(base) PS E:\\GoWork_K5\\Try\\Concurrency&gt; go tool compile -race -S no-mutex.go&quot;&quot;.main STEXT size=478 args=0x0 locals=0x70 funcid=0x0 align=0x0 0x0000 00000 (no-mutex.go:8) TEXT &quot;&quot;.main(SB), ABIInternal, $112-0 0x0000 00000 (no-mutex.go:8) CMPQ SP, 16(R14) 0x0004 00004 (no-mutex.go:8) PCDATA $0, $-2 0x0004 00004 (no-mutex.go:8) JLS 468 0x000a 00010 (no-mutex.go:8) PCDATA $0, $-1 0x000a 00010 (no-mutex.go:8) SUBQ $112, SP 0x000e 00014 (no-mutex.go:8) MOVQ BP, 104(SP) 0x0013 00019 (no-mutex.go:8) LEAQ 104(SP), BP 0x0018 00024 (no-mutex.go:8) FUNCDATA $0, gclocals·0ce64bbc7cfa5ef04d41c861de81a3d7(SB) 0x0018 00024 (no-mutex.go:8) FUNCDATA $1, gclocals·8757cff67ef48cad20738b76d6fada34(SB) 0x0018 00024 (no-mutex.go:8) FUNCDATA $2, &quot;&quot;.main.stkobj(SB) 0x0018 00024 (no-mutex.go:8) MOVQ +112(FP), AX 0x001d 00029 (no-mutex.go:8) PCDATA $1, $0 0x001d 00029 (no-mutex.go:8) NOP 0x0020 00032 (no-mutex.go:8) CALL runtime.racefuncenter(SB) 0x0025 00037 (no-mutex.go:9) LEAQ type.int(SB), AX 0x002c 00044 (no-mutex.go:9) CALL runtime.newobject(SB) 0x0031 00049 (no-mutex.go:9) MOVQ AX, &quot;&quot;.&amp;count+80(SP) 0x0036 00054 (no-mutex.go:9) PCDATA $1, $1 0x0036 00054 (no-mutex.go:9) CALL runtime.racewrite(SB) 0x003b 00059 (no-mutex.go:9) MOVQ &quot;&quot;.&amp;count+80(SP), CX 0x0040 00064 (no-mutex.go:9) MOVQ $0, (CX) 0x0047 00071 (no-mutex.go:11) LEAQ type.sync.WaitGroup(SB), AX 0x004e 00078 (no-mutex.go:11) CALL runtime.newobject(SB) 0x0053 00083 (no-mutex.go:11) MOVQ AX, &quot;&quot;.&amp;wg+72(SP) 0x0058 00088 (no-mutex.go:11) MOVL $16, BX 0x005d 00093 (no-mutex.go:11) PCDATA $1, $2 0x005d 00093 (no-mutex.go:11) NOP 0x0060 00096 (no-mutex.go:11) CALL runtime.racewriterange(SB) 0x0065 00101 (no-mutex.go:11) MOVQ &quot;&quot;.&amp;wg+72(SP), AX 0x006a 00106 (no-mutex.go:11) MOVQ $0, (AX) 0x0071 00113 (no-mutex.go:11) MOVL $0, 8(AX) 0x0078 00120 (no-mutex.go:12) MOVL $10, BX 0x007d 00125 (no-mutex.go:12) NOP 0x0080 00128 (no-mutex.go:12) CALL sync.(*WaitGroup).Add(SB) 0x0085 00133 (no-mutex.go:12) XORL AX, AX 0x0087 00135 (no-mutex.go:13) JMP 160 0x0089 00137 (no-mutex.go:14) CALL runtime.newproc(SB) 0x008e 00142 (no-mutex.go:13) MOVQ &quot;&quot;.i+40(SP), CX 0x0093 00147 (no-mutex.go:13) LEAQ 1(CX), AX 使用mutex这里的共享资源是 count 变量，临界区是 count++，只要在临界区前面获取锁，在离开临界区的时候释放锁，就能完美地解决 data race 的问题了。 12345678910111213141516171819202122232425262728293031package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func main() { // 互斥锁保护计数器 var mu sync.Mutex // 计数器的值 var count = 0 // 辅助变量，用来确认所有的goroutine都完成 var wg sync.WaitGroup wg.Add(10) // 启动10个gourontine for i := 0; i &lt; 10; i++ { go func() { defer wg.Done() // 累加10万次 for j := 0; j &lt; 100000; j++ { mu.Lock() count++ mu.Unlock() } }() } wg.Wait() fmt.Println(count)} 结果： 1000000 进程 已完成，退出代码为 0 这里有一点需要注意：Mutex 的零值是还没有 goroutine 等待的未加锁的状态，所以你不需要额外的初始化，直接声明变量（如 var mu sync.Mutex）即可。 mutex常用操作很多情况下，Mutex 会嵌入到其它 struct 中使用，比如下面的方式： 1234type Counter struct { mu sync.Mutex //自动初始化为零值 Count uint64} 在初始化嵌入的 struct 时，也不必初始化这个 Mutex 字段，不会因为没有初始化出现空指针或者是无法获取到锁的情况。 关于Mutex结构体源码： 1234567891011121314151617181920// A Mutex is a mutual exclusion lock.// The zero value for a Mutex is an unlocked mutex.//// A Mutex must not be copied after first use.type Mutex struct { state int32 sema uint32}/* 在这个结构体中，有两个字段：state int32：这个字段用于表示 Mutex 的状态。它可能具有不同的状态，例如锁定状态或未锁定状态，这里的具体数值含义会根据不同的操作系统和 CPU 架构有所不同。sema uint32：这个字段是用于实现互斥锁的信号量。sema 的值为 0 表示锁是未锁定状态。当值为 1 时表示锁是锁定状态。这个字段与 state 一起协调实现了 Mutex 的锁定和解锁操作。**/ 将mutex嵌入struct中： 123456789101112131415161718192021222324252627282930package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func main() { var counter Counter var wg sync.WaitGroup wg.Add(10) for i := 0; i &lt; 10; i++ { go func() { defer wg.Done() for j := 0; j &lt; 100000; j++ { counter.Lock() counter.Count++ counter.Unlock() } }() } wg.Wait() fmt.Println(counter.Count)}type Counter struct { sync.Mutex Count uint64} 如果嵌入的 struct 有多个字段，我们一般会把 Mutex 放在要控制的字段上面，然后使用空格把字段分隔开来。即使你不这样做，代码也可以正常编译，只不过，用这种风格去写的话，逻辑会更清晰，也更易于维护。 甚至，你还可以把获取锁、释放锁、计数加一的逻辑封装成一个方法，对外不需要暴露锁等逻辑： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func main() { // 封装好的计数器 var counter Counter var wg sync.WaitGroup wg.Add(10) // 启动10个goroutine for i := 0; i &lt; 10; i++ { go func() { defer wg.Done() // 执行10万次累加 for j := 0; j &lt; 100000; j++ { counter.Incr() // +1操作受到锁保护的方法 } }() } wg.Wait() fmt.Println(counter.Count())}// 线程安全的计数器类type Counter struct { CounterType int Name string mu sync.Mutex count uint64}// 加1的方法，内部使用互斥锁保护func (c *Counter) Incr() { c.mu.Lock() c.count++ c.mu.Unlock()}// 得到计数器的值，也需要锁保护，这么严谨的？！func (c *Counter) Count() uint64 { c.mu.Lock() defer c.mu.Unlock() return c.count} Docker issue 37583、35517、32826、30696等、kubernetes issue72361、71617等，都是后来发现的 data race 而采用互斥锁 Mutex 进行修复的。这个后续有时间研究研究 ‍ Mutex 的实现及演进之路理解Mutex的演进之路就能很清晰理解Mutex的设计思路！ 如果一上来就读最新版的源码注定的孤独且枯燥的，很可能就放弃！ 晁岳攀老师给出了“四个阶段”的Mutex演进架构： ​​ 四个阶段： “初版”的 Mutex 使用一个 flag 来表示锁是否被持有，实现比较简单； “给新人机会”：照顾到新来的 goroutine，所以会让新的 goroutine 也尽可能地先获取到锁； “多给些机会”：照顾新来的和被唤醒的 goroutine； 加入了饥饿的解决方案 CASCAS（Compare and Swap，比较并交换）是一种并发算法，通常用于实现多线程环境下的同步操作。它是一种原子操作，用于在多线程环境下实现对内存中某个位置的值进行读取、比较和更新操作。 ​​ CAS 指令将给定的值和一个内存地址中的值进行比较，如果它们是同一个值，就使用新值替换内存地址中的值，这个操作是原子性的。那啥是原子性呢？如果你还不太理解这个概念，那么在这里只需要明确一点就行了，那就是原子性保证这个指令总是基于最新的值进行计算，如果同时有其它线程已经修改了这个值，那么，CAS 会返回失败。 初版Mutex源码如果你是设计者，怎么设计一个互斥锁？ 可以通过一个 flag 变量，标记当前的锁是否被某个 goroutine 持有。 如果这个 flag 的值是 1，就代表锁已经被持有，那么，其它竞争的 goroutine 只能等待； 如果这个 flag 的值是 0，就可以通过 CAS（compare-and-swap，或者 compare-and-set）将这个 flag 设置为 1，标识锁被当前的这个 goroutine 持有了。 当然了对于我来说，一开始并不知道CAS，如果没有一定深度的并发基础我想也不会知道CAS。 ​​ ‍ 初版源码思路： 调用 Lock 请求锁的时候，通过 xadd 方法进行 CAS 操作（第 24 行），xadd 方法通过循环执行 CAS 操作直到成功，保证对 key 加 1 的操作成功完成。 如果比较幸运，锁没有被别的 goroutine 持有，那么，Lock 方法成功地将 key 设置为 1，这个 goroutine 就持有了这个锁； 如果锁已经被别的 goroutine 持有了，那么，当前的 goroutine 会把 key 加1，而且还会调用 semacquire 方法（第 27 行），使用信号量将自己休眠，等锁释放的时候，信号量会将它唤醒。 持有锁的 goroutine 调用 Unlock 释放锁时，它会将 key 减 1（第 31 行）。如果当前没有其它等待这个锁的 goroutine，这个方法就返回了。但是，如果还有等待此锁的其它goroutine，那么，它会调用 semrelease 方法（第 34 行），利用信号量唤醒等待锁的其它 goroutine 中的一个。 123456789101112131415161718192021222324252627282930313233343536// CAS操作，当时还没有抽象出atomic包func cas(val *int32, old, new int32) bool {}func semacquire(*int32) func semrelease(*int32) // 互斥锁的结构，包含两个字段type Mutex struct { key int32 // 锁是否被持有的标识 sema int32 // 信号量专用，用以阻塞/唤醒goroutine}// 保证成功在val上增加delta的值func xadd(val *int32, delta int32) (new int32) { for { v := *val if cas(val, v, v+delta) { return v + delta } } panic(&quot;unreached&quot;)}// 请求锁func (m *Mutex) Lock() { if xadd(&amp;m.key, 1) &lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; 1 { //标识加1，如果等于1，成功获取到锁 return } semacquire(&amp;m.sema) // 否则阻塞等待}func (m *Mutex) Unlock() { if xadd(&amp;m.key, -1) &lt;/span&gt; 0 { // 将标识减去1，如果等于0，则没有其它等待者 return } semrelease(&amp;m.sema) // 唤醒其它阻塞的goroutine} 所以，到这里，我们就知道了，初版的 Mutex 利用 CAS 原子操作，对 key 这个标志量进行设置。key 不仅仅标识了锁是否被 goroutine 所持有，还记录了当前持有和等待获取锁的 goroutine 的数量 ※初版源码※ 其实就可以分析出一个问题： Unlock 方法可以被任意的 goroutine 调用释放锁，即使是没持有这个互斥锁的goroutine，也可以进行这个操作。 这是因为，Mutex 本身并没有包含持有这把锁的goroutine 的信息，所以，Unlock 也不会对此进行检查。Mutex 的这个设计一直保持至今。 所以，在实际应用中，强调锁要成对出现，而且不要离得太远，遵循“谁申请、谁释放”原则。 不要在一个方法中单独申请锁，而在另外一个方法中单独释放锁，一般都会在同一个方法中获取锁和释放锁。 例如： 123456789101112131415161718type Foo struct { mu sync.Mutex count int}func (f *Foo) Bar() { f.mu.Lock() if f.count &lt; 1000 { f.count += 3 f.mu.Unlock() // 此处释放锁 return } f.count++ f.mu.Unlock() // 此处释放锁 return} 从 1.14 版本起，Go 对 defer 做了优化，采用更有效的内联方式，取代之前的生成 defer对象到 defer chain 中，defer 对耗时的影响微乎其微了，所以基本上修改成下面简洁的写法： 123456789101112func (f *Foo) Bar() { f.mu.Lock() defer f.mu.Unlock() if f.count &lt; 1000 { f.count += 3 return } f.count++ return} Lock/Unlock 总是成对紧凑出现，不会遗漏或者多调用，代码更少、更易理解。 初版的 Mutex 实现之后，Go 开发组又对 Mutex 做了一些微调，比如把字段类型变成了uint32 类型；调用 Unlock 方法会做检查；使用 atomic 包的同步原语执行原子操作等等，这些小的改动，都不是核心功能。 初版的时候可能核心人员就发现了一个核心问题：请求锁的 goroutine 会排队等待获取互斥锁。虽然这貌似很公平，但是从性能上来看，却不是最优的。因为如果我们能够把锁交给正在占用 CPU 时间片的 goroutine 的话，那就不需要做上下文的切换，在高并发的情况下，可能会有更好的性能。 有一说一如果是我，我是根本想不到这层次的。 二版：新goroutine也有机会Mutex结构体变化123456789101112131415type Mutex struct { state int32 // 将key变为了state，这一个字段被分成了三部分，代表三个数据，这个是变得复杂的核心 sema uint32 // sema主要内部用来阻塞等待的队列}// 这三个字段是真的究极难懂.// 一个字段包含多个意义，这样可以通过尽可能少的内存来实现互斥锁。// 第一位（最小的一位）mutexLocked 来表示这个锁是否被持有// 第二位 mutexWoken 代表是否有唤醒的 goroutine// 第三位 mutexWaiterShift 代表的是等待此锁的 goroutine 数量const ( mutexLocked = 1 &lt;&lt; iota // mutex is locked 是否持有锁 mutexWoken // 是否被唤醒的标记 mutexWaiterShift = iota // 阻塞等待的waiter的数量) ​​ 请求锁Lock：流程更加复杂源码： 首先，它尝试通过 atomic.CompareAndSwapInt32​ 直接获取锁，也就是将锁的状态从 0 修改为 mutexLocked​。 如果获取锁失败（因为有其他 goroutine 持有了锁），它会进入一个循环。在循环中，它会尝试修改状态以请求锁。 如果旧的状态中锁是已经被持有的（old&amp;mutexLocked != 0​），那么会增加等待者的数量。 如果 goroutine 是被唤醒的（标志为 awoke​），它会清除唤醒标志。 最后，它会通过 atomic.CompareAndSwapInt32​ 设置新的状态。如果锁之前的状态是未加锁的，则退出循环并成功获取了锁；否则，它会请求信号量（runtime.Semacquire(&amp;m.sema)​）来等待锁的释放。 123456789101112131415161718192021222324252627func (m *Mutex) Lock() { // Fast path: 幸运case，能够直接获取到锁 if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) { return } awoke := false for { old := m.state new := old | mutexLocked // 新状态加锁 if old&amp;mutexLocked != 0 { new = old + 1&lt;&lt;mutexWaiterShift //等待者数量加一 } if awoke { // goroutine是被唤醒的， // 新状态清除唤醒标志 new &amp;^= mutexWoken } if atomic.CompareAndSwapInt32(&amp;m.state, old, new) {//设置新状态 if old&amp;mutexLocked == 0 { // 锁原状态未加锁 break } runtime.Semacquire(&amp;m.sema) // 请求信号量 awoke = true } }} 如果想要获取锁的 goroutine 没有机会获取到锁，就会进行休眠，但是在锁释放唤醒之后，它并不能像先前一样直接获取到锁，还是要和正在请求锁的 goroutine 进行竞争。这会给后来请求锁的 goroutine 一个机会，也让 CPU 中正在执行的 goroutine 有更多的机会获取到锁，在一定程度上提高了程序的性能。 核心分类图： ​​ 请求锁的 goroutine 有两类，一类是新来请求锁的 goroutine，另一类是被唤醒的等待请 求锁的 goroutine。锁的状态也有两种：加锁和未加锁。我用一张表格，来说明一下 goroutine 不同来源不同状态下的处理逻辑 释放锁Unlock：也很复杂源码： 123456789101112131415161718192021222324252627282930313233343536func (m *Mutex) Unlock() { // Fast path: drop lock bit. // 尝试将持有锁的标识设置为未加锁的状态，这是通过减 1 而不是将标志位置零的方式实现。 new := atomic.AddInt32(&amp;m.state, -mutexLocked) //去掉锁标志 // 会检测原来锁的状态是否未加锁的状态，如果是 Unlock 一个未加锁的 Mutex 会直接 panic。 if (new+mutexLocked)&amp;mutexLocked == 0 { //本来就没有加锁 panic(&quot;sync: unlock of unlocked mutex&quot;) } // 到这里为什么不直接返回？ // 因为还可能有一些等待这个锁的 goroutine（有时候我也把它们称之为 waiter）需要通过信号量的方式唤醒它们中的一个。 old := new for { // 如果等待的锁的goroutine为0、或者是否有锁、是否被唤醒之一，满足的话直接返回 if old&gt;&gt;mutexWaiterShift == 0 || old&amp;(mutexLocked|mutexWoken) != 0 return } new = (old - 1&lt;&lt;mutexWaiterShift) | mutexWoken // 新状态，准备唤醒gor if atomic.CompareAndSwapInt32(&amp;m.state, old, new) { runtime.Semrelease(&amp;m.sema) return } old = m.state }/* 这里晁老师说明有两种情况: 第一种情况，如果没有其它的 waiter，说明对这个锁的竞争的 goroutine 只有一个，那就可以直接返回了；如果这个时候有唤醒的 goroutine，或者是又被别人加了锁，那么，无需我们操劳，其它 goroutine 自己干得都很好，当前的这个 goroutine 就可以放心返回了。 第二种情况，如果有等待者，并且没有唤醒的 waiter，那就需要唤醒一个等待的 waiter。在唤醒之前，需要将 waiter 数量减 1，并且将 mutexWoken 标志设置上，这样，Unlock就可以返回了。*/} 更好的理解上述源码你要知道的内容： 如何理解CSA升级后的atomic.AddInt32​操作？ 这段代码中的 new := atomic.AddInt32(&amp;m.state, -mutexLocked)​ 是使用原子操作来修改 m.state​ 中存储的锁状态信息。这里使用了 AddInt32​ 函数将 mutexLocked​ 的负值加到 m.state​ 上，实现了去掉锁标志的操作。 在 mutexLocked​ 被定义为 1 &lt;&lt; iota​ 的情况下，它的值是 1​。因此 -mutexLocked​ 的值就是 -1​，它在二进制补码中表示为所有位都为 1​。 假设 m.state​ 的初始值是 mutexLocked​，即锁已被获取，二进制表示为 0001​。 执行 new := atomic.AddInt32(&amp;m.state, -mutexLocked)​ 时，会将 -mutexLocked​（即 -1​，二进制中所有位为 1​）加到 m.state​ 上： 1234 0001 (m.state)+ 1111 (-mutexLocked)----------- 0000 (new) 这样就通过原子操作将 m.state​ 的值从 mutexLocked​（表示锁已被获取）修改为 0​（表示未加锁状态） ​for { ... }​：这是一个无限循环，表明会一直执行里面的逻辑直到满足某个条件才会退出 for循环中第一种情况关于old&gt;&gt;mutexWaiterShift == 0 || old&amp;(mutexLocked|mutexWoken) != 0​的理解？ ​old&gt;&gt;mutexWaiterShift == 0​ 右移位运算，检查等待的goroutine数量是否为零。如果为 0​，意味着没有等待的 goroutine。 ​old&amp;(mutexLocked|mutexWoken) != 0​ 检查 old​ 中是否包含锁定标志或唤醒标志，如果结果不为 0​，表示已经有其他线程持有锁或者有 goroutine 被唤醒了，因此不需要进一步唤醒等待的 goroutine。 如果上述条件成立，即等待的 goroutine 数量为 0​ 或者已经有其他线程持有锁或者有 goroutine 被唤醒，那么结束循环，Unlock可以返回了。 for循环中关于第二种情况new = (old - 1&lt;&lt;mutexWaiterShift) | mutexWoken​的理解？ 执行到这里代表上述条件不成立，需要重新计算new值 如果有等待者，并且没有唤醒的 waiter，那就需要唤醒一个等待的 waiter。在唤醒之前，需要将 waiter 数量减 1，并且将 mutexWoken 标志设置上，这样，Unlock就可以返回了。 ​new = (old - 1&lt;&lt;mutexWaiterShift) | mutexWoken​：计算新的状态，其中包括减少等待者数量并设置唤醒标志，以准备唤醒等待的 goroutine。 ​atomic.CompareAndSwapInt32(&amp;m.state, old, new)​：使用原子操作比较并交换 m.state​ 的值，将 old​ 替换为 new​。这是为了确保在并发情况下，m.state​ 没有被其他地方修改过。如果成功替换，则释放信号量并返回，表示成功唤醒等待的 goroutine。如果替换失败，说明 m.state​ 已经被其他操作修改过，需要重新获取最新的 m.state​ 值进行下一轮的检查和操作。 关于解锁部分的原子操作： 使用 atomic.CompareAndSwapInt32​ 进行原子操作，尝试将 m.state​ 的值从 old​ 更新为 new​。如果这个操作成功，表示成功修改了状态，接着会释放信号量（Semrelease​）并返回，表示成功唤醒等待的 goroutine。如果更新失败，意味着 m.state​ 已经被其他线程修改过，所以重新获取最新的 m.state​ 值作为 old​，继续循环尝试更新状态。 总体来说，这段 for 循环的核心就是：通过不断地检查 m.state​ 的状态，如果满足某些条件（没有等待的 goroutine 或者已经有其他线程持有锁或有 goroutine 被唤醒），则直接返回；如果条件不满足，则尝试修改状态并唤醒等待的 goroutine。 相对于初版的设计，这次的改动主要就是，新来的 goroutine 也有机会先获取到锁，甚至一个 goroutine 可能连续获取到锁，打破了先来先得的逻辑。但是，代码复杂度也显而易见。虽然这一版的 Mutex 已经给新来请求锁的 goroutine 一些机会，让它参与竞争，没有空的锁或者竞争失败才加入到等待队列中。但是其实还可以进一步优化。 如果说要彻底的弄明白，可能需要完整地画下图，模拟下计算过程。比如位运算部分为什么要那么计算。 三版：多给些机会在 2015 年 2 月的改动中，如果新来的 goroutine 或者是被唤醒的 goroutine 首次获取不到锁，它们就会通过自旋（spin，通过循环不断尝试，spin 的逻辑是在runtime 实现的）的方式，尝试检查锁是否被释放。在尝试一定的自旋次数后，再执行原来的逻辑。 源码： 123456789101112131415161718192021222324252627282930313233343536373839404142func (m *Mutex) Lock() { // Fast path: 幸运之路，正好获取到锁 if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) { return } awoke := false iter := 0 for { // 不管是新来的请求锁的goroutine, 还是被唤醒的goroutine，都不断尝试请求锁 old := m.state // 先保存当前锁的状态 new := old | mutexLocked // 新状态设置加锁标志 if old&amp;mutexLocked != 0 { // 锁还没被释放 // 新引入了自旋！自旋等待是为了减少因等待锁而发生的上下文切换带来的开销 if runtime_canSpin(iter) { if !awoke &amp;&amp; old&amp;mutexWoken == 0 &amp;&amp; old&gt;&gt;mutexWaiterShift != 0 atomic.CompareAndSwapInt32(&amp;m.state, old, old|mutexWoken) awoke = true } runtime_doSpin() iter++ continue // 自旋，再次尝试请求锁 } new = old + 1&lt;&lt;mutexWaiterShift } if awoke { // 唤醒状态 // 新的锁的状态异常，这个可能存在吗？ if new&amp;mutexWoken == 0 { panic(&quot;sync: inconsistent mutex state&quot;) } new &amp;^= mutexWoken // 新状态清除唤醒标记 } // 注意iter=0 if atomic.CompareAndSwapInt32(&amp;m.state, old, new) { if old&amp;mutexLocked == 0 { // 旧状态锁已释放，新状态成功持有了锁，直接 break } runtime_Semacquire(&amp;m.sema) // 阻塞等待 awoke = true // 被唤醒 iter = 0 } }} 这次的优化，增加了第 13 行到 21 行、第 25 行到第 27 行以及第 39 行。我来解释一下主要的逻辑，也就是第 13 行到 21 行。如果可以 spin 的话，第 9 行的 for 循环会重新检查锁是否释放。对于临界区代码执行非常短的场景来说，这是一个非常好的优化。因为临界区的代码耗时很短，锁很快就能释放，而抢夺锁的 goroutine 不用通过休眠唤醒方式等待调度，直接 spin 几次，可能就获得了锁。 关于自旋的理解： ​iter​ 是迭代计数器，它记录了自旋的次数或轮数 ​runtime_doSpin()​：模拟实现自旋的函数。在这个函数中，会进行一些空循环，以达到自旋的目的。 ​iter++​：迭代计数器加一，用于记录自旋的次数 ​continue​：继续下一次循环，继续尝试请求锁，即重新进入自旋状态 进入自旋的条件： ​!awoke​：确保在被唤醒后不再尝试自旋等待 ​old&amp;mutexWoken == 0​：检查当前 Mutex 的状态是否已经有其他 goroutine 进行了唤醒操作。如果已经被唤醒，则不再进行自旋 old&gt;&gt;mutexWaiterShift != 0​，目的是检查是否有等待的 goroutine 终极版：解决饥饿问题经过几次优化，Mutex 的代码越来越复杂，应对高并发争抢锁的场景也更加公平。但是总有极端情况，因为新来的 goroutine 也参与竞争，有可能每次都会被新来的 goroutine 抢获取锁的机会，在极端情况下，等待中的 goroutine 可能会一直获取不到锁，这就是饥饿问题。 2016 年 Go 1.9 中 Mutex 增加了饥饿模式，让锁变得更公平，不公平的等待时间限制在 1 毫秒，并且修复了一个大 Bug：总是把唤醒的goroutine 放在等待队列的尾部，会导致更加不公平的等待时间。 之后，2018 年，Go 开发者将 fast path 和 slow path 拆成独立的方法，以便内联，提高性能。2019 年也有一个 Mutex 的优化，虽然没有对 Mutex 做修改，但是，对于 Mutex唤醒后持有锁的那个 waiter，调度器可以有更高的优先级去执行，这已经是很细致的性能优化了。 没错，这一次优化添加了一种状态模式到state中： ​​ 核心思路Mutex 绝不容忍一个goroutine 被落下，永远没有机会获取锁。不抛弃不放弃是它的宗旨，而且它也尽可能地让等待较长的 goroutine 更有机会获取到锁。 这个时候可以脑部一下，整体的逻辑不是又多了很多很多的分支？ 即使不懂源码，也可以轻易的清楚一条链路：在除了很幸运地直接获取锁之外，还有自旋竞争之路、自旋竞争时间太长导致的饥饿之路。 饥饿时间阈值为10 的 6 次方纳秒，也就是1毫秒。 饥饿模式与正常模式的转换饥饿—&gt;正常： 正常模式下，waiter 都是进入先入先出队列，被唤醒的 waiter 并不会直接持有锁，而是要和新来的 goroutine 进行竞争。新来的 goroutine 有先天的优势，它们正在 CPU 中运行，可能它们的数量还不少，所以，在高并发情况下，被唤醒的 waiter 可能比较悲剧地获取不到锁，这时，它会被插入到队列的前面。如果 waiter 获取不到锁的时间超过阈值 1 毫秒，那么，这个 Mutex 就进入到了饥饿模式。 在饥饿模式下，Mutex 的拥有者将直接把锁交给队列最前面的 waiter。新来的 goroutine不会尝试获取锁，即使看起来锁没有被持有，它也不会去抢，也不会 spin，它会乖乖地加入到等待队列的尾部。 正常—&gt;饥饿： 此 waiter 已经是队列中的最后一个 waiter 了，没有其它的等待锁的 goroutine 了； 此 waiter 的等待时间小于 1 毫秒。 正常模式拥有更好的性能，因为即使有等待抢锁的 waiter，goroutine 也可以连续多次获取到锁。饥饿模式是对公平性和性能的一种平衡，它避免了某些 goroutine 长时间的等待锁。在饥饿模式下，优先对待的是那些一直在等待的 waiter。 饥饿源码正常模式下，waiter 都是进入先入先出队列，被唤醒的 waiter 并不会直接持有锁，而是要和新来的 goroutine 进行竞争。新来的 goroutine 有先天的优势，它们正在 CPU 中运行，可能它们的数量还不少，所以，在高并发情况下，被唤醒的 waiter 可能比较悲剧地获取不到锁，这时，它会被插入到队列的前面。 如果 waiter 获取不到锁的时间超过阈值 1 毫秒，那么，这个 Mutex 就进入到了饥饿模式。 在饥饿模式下，Mutex 的拥有者将直接把锁交给队列最前面的 waiter。新来的 goroutine不会尝试获取锁，即使看起来锁没有被持有，它也不会去抢，也不会 spin，它会乖乖地加入到等待队列的尾部。如果拥有 Mutex 的 waiter 发现下面两种情况的其中之一，它就会把这个 Mutex 转换成正常模式:正常模式拥有更好的性能，因为即使有等待抢锁的 waiter，goroutine 也可以连续多次获取到锁。 饥饿模式是对公平性和性能的一种平衡，它避免了某些 goroutine 长时间的等待锁。在饥饿模式下，优先对待的是那些一直在等待的 waiter。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122type Mutex struct { state int32 sema uint32}const ( mutexLocked = 1 &lt;&lt; iota // mutex is locked mutexWoken mutexStarving // 从state字段中分出一个饥饿标记 mutexWaiterShift = iota starvationThresholdNs = 1e6)func (m *Mutex) Lock() { // Fast path: 幸运之路，一下就获取到了锁 if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) { return } // Slow path：缓慢之路，尝试自旋竞争或饥饿状态下饥饿goroutine竞争 m.lockSlow()}func (m *Mutex) lockSlow() { var waitStartTime int64 starving := false // 此goroutine的饥饿标记 awoke := false // 唤醒标记 iter := 0 // 自旋次数 old := m.state // 当前的锁的状态 for { // 锁是非饥饿状态，锁还没被释放，尝试自旋 if old&amp;(mutexLocked|mutexStarving) &lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; mutexLocked &amp;&amp; runtime_canSp if !awoke &amp;&amp; old&amp;mutexWoken &lt;/span&gt; 0 &amp;&amp; old&gt;&gt;mutexWaiterShift != 0 atomic.CompareAndSwapInt32(&amp;m.state, old, old|mutexWoken) awoke = true } runtime_doSpin() iter++ old = m.state // 再次获取锁的状态，之后会检查是否锁被释放了 continue } new := old if old&amp;mutexStarving == 0 { new |= mutexLocked // 非饥饿状态，加锁 } if old&amp;(mutexLocked|mutexStarving) != 0 { new += 1 &lt;&lt; mutexWaiterShift // waiter数量加1 } if starving &amp;&amp; old&amp;mutexLocked != 0 { new |= mutexStarving // 设置饥饿状态 } if awoke { if new&amp;mutexWoken == 0 { throw(&quot;sync: inconsistent mutex state&quot;) } new &amp;^= mutexWoken // 新状态清除唤醒标记 } // 成功设置新状态 if atomic.CompareAndSwapInt32(&amp;m.state, old, new) { // 原来锁的状态已释放，并且不是饥饿状态，正常请求到了锁，返回 if old&amp;(mutexLocked|mutexStarving) == 0 { break // locked the mutex with CAS } // 处理饥饿状态 // 如果以前就在队列里面，加入到队列头 queueLifo := waitStartTime != 0 if waitStartTime == 0 { waitStartTime = runtime_nanotime() } // 阻塞等待 runtime_SemacquireMutex(&amp;m.sema, queueLifo, 1) // 唤醒之后检查锁是否应该处于饥饿状态 starving = starving || runtime_nanotime()-waitStartTime &gt; star old = m.state // 如果锁已经处于饥饿状态，直接抢到锁，返回 if old&amp;mutexStarving != 0 { if old&amp;(mutexLocked|mutexWoken) != 0 || old&gt;&gt;mutexWaiterSh throw(&quot;sync: inconsistent mutex state&quot;) } // 有点绕，加锁并且将waiter数减1 delta := int32(mutexLocked - 1&lt;&lt;mutexWaiterShift) if !starving || old&gt;&gt;mutexWaiterShift == 1 { delta -= mutexStarving // 最后一个waiter或者已经不饥饿了，清 } atomic.AddInt32(&amp;m.state, delta) break } awoke = true iter = 0 } else { old = m.state } }}func (m *Mutex) Unlock() { // Fast path: drop lock bit. new := atomic.AddInt32(&amp;m.state, -mutexLocked) if new != 0 { m.unlockSlow(new) }}func (m *Mutex) unlockSlow(new int32) { if (new+mutexLocked)&amp;mutexLocked &lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; 0 { throw(&quot;sync: unlock of unlocked mutex&quot;) } if new&amp;mutexStarving &lt;/span&gt; 0 { old := new for { if old&gt;&gt;mutexWaiterShift == 0 || old&amp;(mutexLocked|mutexWoken|m return } new = (old - 1&lt;&lt;mutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(&amp;m.state, old, new) { runtime_Semrelease(&amp;m.sema, false, 1) return } old = m.state } } else { runtime_Semrelease(&amp;m.sema, true, 1) } Go 语言从出生到现在已经 10 多年了，这个 Mutex 对外的接口却没有变化，依然向下兼容，即使现在 Go 出了两个版本，每个版本也会向下兼容，保持 Go 语言的稳定性，你也能领悟他们软件开发和设计的思想。还有一点，你也可以观察到，为了一个程序 20% 的特性，你可能需要添加 80% 的代码，这也是程序越来越复杂的原因。所以，最开始的时候，如果能够有一个清晰而且易于扩展的设计，未来增加新特性时，也会更加方便。 最后，两个小问题： 目前 Mutex 的 state 字段有几个意义，这几个意义分别是由哪些字段表示的？ 4个； mutexLocked代表是否被锁上 mutexWoken代表唤醒标记 mutexStarving代表是否处于饥饿 mutexWaiterShift 代表有多少go程在等待这个锁 等待一个 Mutex 的 goroutine 数最大是多少？是否能满足现实的需求？ state 是int32，有三位用来表示状态的，剩下的用来表示数量，减去状态位所以能够表示的最大值 是2^32-3^-1如果每个等待这个 Mutex 的 goroutine 占用 2KB 内存，那么最大约能支持 2KB * 1073741822 ≈1048575M≈1024G≈1T 的内存占用。这个数量对于绝大多数应用场景来说已经非 常巨大了，可以满足绝大多数实际需求。 Mutex四大易错场景使用mutex的过程中可能会犯一些错误 总体来说有四方面： Lock/Unlock 不是成对出现 Copy 已使用的 Mutex 重入 死锁 Lock/Unlock 不是成对出现Lock/Unlock 没有成对出现，就意味着会出现死锁的情况，或者是因为 Unlock 一个未加锁的 Mutex 而导致 panic。 其中缺少Unlock的场景晁老师给出了三种： 代码中有太多的 if-else 分支，可能在某个分支中漏写了 Unlock； 在重构的时候把 Unlock 给删除了； Unlock 误写成了 Lock。 乍一看每一种都很傻，这些情况下，锁被获取之后，就不会被释放了，这也就意味着，其它的 goroutine 永远都没机会获取到锁。其实很多大神也可能会犯这些错误。 缺少 Lock 的场景，这就很简单了，一般来说就是误操作删除了 Lock。 比如先前使用 Mutex 都是正常的，结果后来其他人重构代码的时候，由于对代码不熟悉，或者由于开发者的马虎，把 Lock 调用给删除了，或者注释掉了。比如下面的代码，mu.Lock() 一行代码被删除了，直接 Unlock 一个未加锁的 Mutex 会 panic： 代码中有太多的 if-else 分支，可能在某个分支中漏写了 Unlock； 在重构的时候把 Unlock 给删除了； Unlock 误写成了 Lock。 12345func foo() { var mu sync.Mutex defer mu.Unlock() fmt.Println(&quot;hello world!&quot;)} Copy 已使用的 Mutex这个点我印象很深，因为在看锁部分的源码是有留心过这部分的注释，就警告不要复制锁！ 不能复制锁的原因也比较容易想到，因为截止目前的版本，锁已经是“带状态”的了：state字段​ 如果你复制了一个已经加锁的Mutex给了新的变量，这个新的变量按理来说期望是零值。在这个时候，在并发环境下，执行它你就永远可能不知道它究竟是什么状态，因为要复制的mutex是由其它goroutine并发访问的，状态可能随时在变化。 如果不信邪，便一试究竟： 12345678910111213141516171819type Counter struct { sync.Mutex Count int}func main() { var c Counter c.Lock() defer c.Unlock() c.Count++ foo(c) // 复制锁}// 这里Counter的参数是通过复制的方式传入的func foo(c Counter) { c.Lock() defer c.Unlock() fmt.Println(&quot;in foo&quot;)} ​​ 第十一行的foo（c），这个时候传入的Couner实例已经是带状态的了 如果解决这类死锁问题？ 其实IDE编译环境下其实就有提示了，让你注意了，比如goland环境： ​​ 除此之外，go运行时还有死锁的检查机制checkdead方法​，它能够发现死锁的goroutine 但是有个重点：你肯定不想你的项目上线运行了才发现死锁问题吧！ 使用 vet 工具，把检查写在 Makefile 文件中，在持续集成的时候跑一跑，这样可以及时发现问题，及时修复。我们可以使用 go vet 检查这个 Go 文件： ​​ 使用这个工具就可以发现 Mutex 复制的问题，错误信息显示得很清楚，是在调用foo 函数的时候发生了 lock value 复制的情况，还告诉我们出问题的代码行数以及 copy lock 导致的错误 vet 工具是怎么发现 Mutex 复制使用问题的？ 检查是通过copylock分析器​静态分析实现的。这个分析器会分析函数调用、range 遍历、复制、声明、函数返回值等位置，有没有锁的值 copy 的情景，以此来判断有没有问题。 只要是实现了 Locker 接口，就会被分析。 重入一句话说明：重入锁允许同一个线程或 goroutine 多次获取同一把锁而不会造成死锁，但 Go 标准库中的 sync.Mutex​ 不支持重入。 关于重入的概念： 当一个线程获取锁时，如果没有其它线程拥有这个锁，那么，这个线程就成功获取到这个锁。之后，如果其它线程再请求这个锁，就会处于阻塞等待的状态。但是，如果拥有这把锁的线程再请求这把锁的话，不会阻塞，而是成功返回，所以叫可重入锁（有时候也叫做递归锁）。只要你拥有这把锁，你可以可着劲儿地调用，比如通过递归实现一些算法，调用者不会阻塞或者死锁。 但是问题就来了~ Mutex 不是可重入的锁！ 为啥Mutex不可重入？ 如果上述文章仔细读了肯定能知道：Mutex 的实现中没有记录哪个 goroutine 拥有这把锁。 理论上，任何 goroutine 都可以随意地 Unlock 这把锁，所以没办法计算重入条件。 重入锁示例： 12345678910111213141516171819202122232425package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func fooo(l sync.Locker) { fmt.Println(&quot;in foo&quot;) l.Lock() bar(l) l.Unlock()}func bar(l sync.Locker) { // 同一把锁，拥有这个锁的线程再次请求锁 l.Lock() fmt.Println(&quot;in bar&quot;) l.Unlock()}func main() { l := &amp;sync.Mutex{} fooo(l)} 而我初步想到的例子，这种算不算可重入错误示例呢？ 12345678910111213141516171819package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func fooo(l sync.Locker) { fmt.Println(&quot;in foo&quot;) l.Lock() l.Lock() l.Unlock()}func main() { l := &amp;sync.Mutex{} fooo(l)} 关键来了，go如何实现可重入锁？ 这其中的关键就是记住哪个goroutine持有这个锁，这处自己可以很容易想到用一个id来标识goroutine，关键就是这个id是在整个阶段什么位置产生的 实现可重入锁方案一：goroutine id获取goroutine id 又有两种方式：1.runtime.Stack 2.hacker runtime.Stack通过此方法获取栈帧信息，栈帧信息中包含goroutine id 123goroutine 1 [running]:main.main()....../main.go:19 +0xb1 第一行格式为 goroutine xxx，其中 xxx 就是 goroutine id，只要解析出这个 id 即可。 解析的方式可以是： 1234567891011func GoID() int { var buf [64]byte n := runtime.Stack(buf[:], false) // 得到id字符串 idField := strings.Fields(strings.TrimPrefix(string(buf[:n]), &quot;goroutine&quot;)) id, err := strconv.Atoi(idField) if err != nil { panic(fmt.Sprintf(&quot;cannot get goroutine id: %v&quot;, err)) } return id} hacker这是晁老师推荐的方式： 首先，我们获取运行时的 g 指针，反解出对应的 g 的结构。每个运行的 goroutine 结构的g 指针保存在当前 goroutine 的一个叫做 TLS 对象中。 第一步：我们先获取到 TLS 对象； 第二步：再从 TLS 中获取 goroutine 结构的 g 指针； 第三步：再从 g 指针中取出 goroutine id。 实现过程大致如下： 12345678910111213141516171819202122232425262728293031323334353637383940package mainimport ( &quot;fmt&quot; &quot;reflect&quot; &quot;unsafe&quot;)// 获取 TLS 对象func getTLS() unsafe.Pointer { return unsafe.Pointer(uintptr(unsafe.Pointer(&amp;struct{}{})) - unsafe.Offsetof(goFunc{}.gobuf))}// 获取 goroutine 结构的 g 指针func getGPointer(tls unsafe.Pointer) unsafe.Pointer { return *(*unsafe.Pointer)(tls)}// 从 g 指针中获取 goroutine IDfunc getGoroutineID(gPointer unsafe.Pointer) int64 { // 假设 goroutine ID 在 g 结构体的某个字段中 // 这里假设该字段为一个 int64 类型 gStruct := reflect.TypeOf(goFunc{}) idFieldOffset := gStruct.FieldByName(&quot;id&quot;).Offset idPointer := (*int64)(unsafe.Pointer(uintptr(gPointer) + idFieldOffset)) return *idPointer}func main() { tls := getTLS() gPointer := getGPointer(tls) goroutineID := getGoroutineID(gPointer) fmt.Println(&quot;Goroutine ID:&quot;, goroutineID)}// 简化的 goroutine 结构体示例，实际结构体可能与此不同type goFunc struct { gobuf unsafe.Pointer id int64 // 假设存储 goroutine ID 的字段为 int64 类型} 但是问题是不同Go版本的结构可能不同，版本有差异底层又复杂，最好是通过第三方库来获取例如：petermattis/goid​，这个支持获取多版本的goroutine id~ 实现可重入锁1234567891011121314151617181920212223242526272829303132333435363738// RecursiveMutex 包装一个Mutex,实现可重入type RecursiveMutex struct { sync.Mutex owner int64 // 当前持有锁的goroutine id recursion int32 // 这个goroutine 重入的次数}func (m *RecursiveMutex) Lock() { gid := goid.Get() // 如果当前持有锁的goroutine就是这次调用的goroutine,说明是重入,重入次数+1 if atomic.LoadInt64(&amp;m.owner) == gid { m.recursion++ return } m.Mutex.Lock() // 不是这次调用的goroutine的话就获得这个goroutine锁的第一次调用，记录下它的goroutine id,调用次数记为1 atomic.StoreInt64(&amp;m.owner, gid) m.recursion = 1}func (m *RecursiveMutex) Unlock() { gid := goid.Get() // 非持有锁的goroutine尝试释放锁，会解锁失败，必须持有这个锁的goroutine才可以进行后续解锁流程 if atomic.LoadInt64(&amp;m.owner) != gid { panic(fmt.Sprintf(&quot;wrong the owner(%d): %d!&quot;, m.owner, gid)) } // 调用次数减1 m.recursion-- if m.recursion != 0 { // 如果这个goroutine还没有完全释放，则直接返回 return } // 此goroutine最后一次调用，需要释放锁 atomic.StoreInt64(&amp;m.owner, -1) m.Mutex.Unlock()} 方案一这种方式是用 goroutine 固有的id 做 goroutine 的标识，如果不想碰固有的id可以让 goroutine 自己来提供标识(token来标识自己的身份)。这里有一点：晁老师说Go 开发者不期望你利用 goroutine id 做一些不确定的东西，所以，他们没有暴露获取 goroutine id 的方法，所以截止目前没有这种方法。 实现可重入锁方案二：token下面的代码是第二种方案。调用者自己提供一个 token，获取锁的时候把这个 token 传入，释放锁的时候也需要把这个 token 传入。通过用户传入的 token 替换方案一中goroutine id，其它逻辑和方案均一致。 123456789101112131415161718192021222324252627282930313233343536// Token方式的递归锁type TokenRecursiveMutex struct { sync.Mutex token int64 // 锁实例绑定的token标识，用于标识属于哪个goroutine recursion int32 // 锁递归调用次数 }// 请求锁，需要传入tokenfunc (m *TokenRecursiveMutex) Lock(token int64) { // atomic.LoadInt64 接收一个 *int64 类型的指针作为参数，并返回指针指向的 int64 类型变量的当前值。 // 这个操作会以原子方式读取指定的变量，确保在读取时不会被其他 goroutine 修改。 if atomic.LoadInt64(&amp;m.token) == token { //如果传入的token和持有锁的token一致， m.recursion++ return } // 传入的token不一致，说明不是递归调用加锁，第一次加锁，记录本次token，记录加锁次数=1 m.Mutex.Lock() atomic.StoreInt64(&amp;m.token, token) m.recursion = 1}// 释放锁func (m *TokenRecursiveMutex) Unlock(token int64) { if atomic.LoadInt64(&amp;m.token) != token { // 释放其它token持有的锁 panic(fmt.Sprintf(&quot;wrong the owner(%d): %d!&quot;, m.token, token)) } // 满足条件，即此锁确实属于本goroutine m.recursion-- // 当前持有这个锁的token释放锁 if m.recursion != 0 { // 还没有回退到最初的递归调用，这种情况需要回到 “根调用” 才能进行释放锁的操作，你懂得 return } atomic.StoreInt64(&amp;m.token, 0) // 没有递归调用了，释放锁 m.Mutex.Unlock()} 死锁死锁的概念其实没什么好讲的：两个或者两个以上的进程（或者线程、goroutine）在执行中，因争夺共享资源而处于一种相互等待的状态，如果没有外部的推动，他们将无法进行下去，则称系统处于死锁状态或者产生了死锁。 想解决死锁问题就必须至少破坏四个必要条件中的一个： 互斥：至少有一个资源是独享的，具有排他性，如被获得，其他goroutine必须等待 持有和等待：持有一个资源，还在等待其它资源，“吃着碗里看着锅里” 不可剥夺：资源只能由持有它的goroutine来释放 环路等待（我理解就是三角债务问题）：P1等P2，P2等P3，P3等P4，PN等P1 这里听晁老师讲的【哲学家就餐问题】耳目一新 哲学家就餐问题 多个哲学家围坐在圆桌旁吃饭，每个哲学家必须执行两个动作：思考和进餐。这些哲学家之间围坐着一些餐叉，每个哲学家之间需要使用两把餐叉来进餐。然而，问题的关键在于餐叉是共享资源，哲学家需要获取两把餐叉才能进餐，而一把餐叉只能被一个哲学家使用。 这个问题存在的主要挑战是如何避免死锁和饥饿，确保每个哲学家都能进餐，并且不会发生死锁（所有哲学家都在等待对方手中的餐叉）或饥饿（某些哲学家永远无法进餐）的情况。 经典的解决方案通常是通过引入特定的算法或规则来确保资源的安全获取，其中最常见的是 Dijkstra 提出的「哲学家就餐问题」的解决方案之一：使用资源层次结构分配资源。该算法包括以下规则： ①每个哲学家在就餐前&lt;u&gt;必须先获取左右两边的餐叉&lt;/u&gt;，这保证了只有同时拿到两把餐叉的哲学家才能进餐。 ②餐叉被定义为资源，&lt;u&gt;每个哲学家在尝试获取餐叉时，按照编号低到高的顺序来获取&lt;/u&gt;。这样可以避免循环等待（哲学家之间互相等待对方手中的餐叉）的情况。 然后晁老师又讲了个笑话：“有一次我去派出所开证明，派出所要求物业先证明我是本物业的业主，但是，物业要我提供派出所的证明，才能给我开物业证明，结果就陷入了死锁状态。”这种事情比比皆是，让你感觉很难受，同样地，系统中或者你写的代码中出现了这样的问题你也很难受，：） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 package mainimport ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot;)/** @Title main @Description 派出所与物业的死锁问题 @Author luommy 2023-12-19 10:28:14**/func main() { // 派出所证明 var psCertificate sync.Mutex // 物业证明 var poProperty sync.Mutex var wg sync.WaitGroup wg.Add(2) // 派出所处理goroutine go func() { defer wg.Done() psCertificate.Lock() defer psCertificate.Unlock() // 检查材料 time.Sleep(5 * time.Second) // 请求物业的材料 poProperty.Lock() poProperty.Unlock() }() // 物业处理goroutine go func() { defer wg.Done() // 物业处理完成 poProperty.Lock() defer poProperty.Unlock() // 检查材料 time.Sleep(5 * time.Second) // 请求派出所的证明 psCertificate.Lock() psCertificate.Unlock() }() wg.Wait() fmt.Println(&quot;成功完成！&quot;)} 著名的issue案例Docker： issue36114：重复加锁（Go标准库不支持重入锁） issue34881：重构代码的时候遇见错误直接返回，忘记释放锁 ​​ Kubernates: issue72361: 有data race数据竞争的问题，加锁解决 ​​ issue45192：是一个返回时忘记 Unlock 的典型例子，同docker issue 34881 ​​ 保证 Lock/Unlock 成对出现，尽可能采用 defer mutex.Unlock 的方式，把它们成对、紧凑地写在一起。 并发程序最难跟踪调试的就是很难重现，因为并发问题不是按照我们指定的顺序执行的，由于计算机调度的问题和事件触发的时机不同，死锁的 Bug 可能会在极端的情况下出现。 通过搜索日志、查看日志，我们能够知道程序有异常了，比如某个流程一直没有结束。这个时候，可以通过 Go pprof 工具分析，它提供了一个 block profiler 监控阻塞的goroutine。除此之外，我们还可以查看全部的 goroutine 的堆栈信息，通过它，你可以查看阻塞的 groutine 究竟阻塞在哪一行哪一个对象上了。 问题思考： 在当前的函数中Lock，然后在调用的函数中Unlock？ 锁的释放位置问题？ 还有一些其它的问题吗？ Mutex扩展功能共识：如果互斥锁被某个 goroutine 获取了，而且还没有释放，那么，其他请求这把锁的 goroutine，就会阻塞等待，直到有机会获得这把锁。有时候阻塞并不是一个很好的主意，比如你请求锁更新一个计数器，如果获取不到锁的话没必要等待，大不了这次不更新，我下次更新就好了，如果阻塞的话会导致业务处理能力的下降。 再比如，如果要监控锁的竞争情况，一个监控指标就是，等待这把锁的 goroutine 数量。我们可以把这个指标推送到时间序列数据库中，再通过一些监控系统（比如P+G）展示出来。要知道，锁是性能下降的“罪魁祸首”之一，所以，有效地降低锁的竞争，就能够很好地提高性能。因此，监控关键互斥锁上等待的 goroutine 的数量，是我们分析锁竞争的激烈程度的一个重要指标。 实现TryLock当一个 goroutine 调用这个TryLock 方法请求锁的时候，如果这把锁没有被其他 goroutine 所持有，那么，这个goroutine 就持有了这把锁，并返回 true；如果这把锁已经被其他 goroutine 所持有，或者是正在准备交给某个被唤醒的 goroutine，那么，这个请求锁的 goroutine 就直接返回false，不会阻塞在方法调用上 ​​ 在实际开发中，如果要更新配置数据，我们通常需要加锁，这样可以避免同时有多个goroutine 并发修改数据。有的时候，我们也会使用 TryLock。这样一来，当某个goroutine 想要更改配置数据时，如果发现已经有 goroutine 在更改了，其他的goroutine 调用 TryLock，返回了 false，这个 goroutine 就会放弃更改。 Java提供了TryLock方式，Go曾经也在讨论这个，但是没有做，可以使用Channel实现TryLock 1234567891011121314151617181920212223242526272829303132333435363738394041// 复制Mutex定义的常量const ( mutexLocked = 1 &lt;&lt; iota // 加锁标识位置 mutexWoken // 唤醒标识位置 mutexStarving // 锁饥饿标识位置 mutexWaiterShift = iota // 标识waiter的起始bit位置)// 扩展一个Mutex结构type Mutex struct { sync.Mutex}// 尝试获取锁func (m *Mutex) TryLock() bool { // 如果能成功抢到锁 // atomic.CompareAndSwapInt32()是一个原子比较和交换函数，它接受三个参数：指向要操作的整数值的指针、旧的期望值和新的值。它会比较指定地址处的整数值和旧的期望值，如果相等，则将其更新为新的值，并返回是否替换成功的布尔值。 if atomic.CompareAndSwapInt32((*int32)(unsafe.Pointer(&amp;m.Mutex)), 0, mutexLocked){ return true } // 如果处于唤醒、加锁或者饥饿状态，这次请求就不参与竞争了，返回false old := atomic.LoadInt32((*int32)(unsafe.Pointer(&amp;m.Mutex))) if old&amp;(mutexLocked|mutexStarving|mutexWoken) != 0 { return false } // 尝试在竞争的状态下请求锁 new := old | mutexLocked return atomic.CompareAndSwapInt32((*int32)(unsafe.Pointer(&amp;m.Mutex)), old, new)}func main() { m := &amp;Mutex{} if m.TryLock() { // 成功获取锁 // ...处理逻辑 m.Unlock() // 使用完毕后解锁 } else { // 未能获取锁 }} 第18行是一个 fast path，如果幸运，没有其他 goroutine 争这把锁，那么，这把锁就会被这个请求的 goroutine 获取，直接返回。 如果锁已经被其他 goroutine 所持有，或者被其他唤醒的 goroutine 准备持有，那么，就直接返回 false，不再请求，代码逻辑在第 23 行。 如果没有被持有，也没有其它唤醒的 goroutine 来竞争锁，锁也不处于饥饿状态，就尝试获取这把锁（第 28 行），不论是否成功都将结果返回。因为，这个时候，可能还有其他的goroutine 也在竞争这把锁，所以，不能保证成功获取这把锁。 如何验证 TryLock 的机制是否工作程序运行时会启动一个 goroutine 持有这把我们自己实现的锁，经过随机的时间才释放。主 goroutine 会尝试获取这把锁。如果前一个goroutine 一秒内释放了这把锁，那么，主 goroutine 就有可能获取到这把锁了，输出“got the lock”，否则没有获取到也不会被阻塞，会直接输出“can’t get the lock”。 1234567891011121314151617181920func try() { var mu Mutex go func() { // 启动一个goroutine持有一段时间的锁 mu.Lock() time.Sleep(time.Duration(rand.Intn(2)) * time.Second) mu.Unlock() }() time.Sleep(time.Second) ok := mu.TryLock() // 尝试获取到锁 if ok { // 获取成功 fmt.Println(&quot;got the lock&quot;) // do something mu.Unlock() return } // 没有获取到 fmt.Println(&quot;can't get the lock&quot;)} 获取等待者的数量等指标Mutex 结构中的 state 字段有很多个含义，通过 state 字段，你可以知道锁是否已经被某个 goroutine 持有、当前是否处于饥饿状态、是否有等待的 goroutine 被唤醒、等待者的数量等信息。但是，state 这个字段并没有暴露出来，所以，我们需要想办法获取到这个字段，并进行解析 如何获取到state字段？ 123456789101112131415161718const ( mutexLocked = 1 &lt;&lt; iota // 加锁标识位置 mutexWoken // 唤醒标识位置 mutexStarving // 锁饥饿标识位置 mutexWaiterShift = iota // 标识waiter的起始bit位置)type Mutex struct { sync.Mutex}func (m *Mutex) Count() int { // 获取state字段的值 v := atomic.LoadInt32((*int32)(unsafe.Pointer(&amp;m.Mutex))) v = v &gt;&gt; mutexWaiterShift // 得到等待者的数值 v = v + (v &amp; mutexLocked) // 再加上锁持有者的数量，0或者1 return int(v)} 这个例子的第 14 行通过 unsafe 操作，我们可以得到 state 字段的值。 第 15 行我们右移三位（这里的常量 mutexWaiterShift 的值为 3），就得到了当前等待者的数量。如果当前的锁已经被其他 goroutine 持有，那么，我们就稍微调整一下这个值，加上一个 1（第16行），你基本上可以把它看作是当前持有和等待这把锁的 goroutine 的总数。 state 这个字段的第一位是用来标记锁是否被持有，第二位用来标记是否已经唤醒了一个等待者，第三位标记锁是否处于饥饿状态，通过分析这个 state 字段我们就可以得到这些状态信息。我们可以为这些状态提供查询的方法，这样就可以实时地知道锁的状态了。 123456789101112131415161718// 锁是否被持有func (m *Mutex) IsLocked() bool { state := atomic.LoadInt32((*int32)(unsafe.Pointer(&amp;m.Mutex))) return state&amp;mutexLocked == mutexLocked}// 是否有等待者被唤醒func (m *Mutex) IsWoken() bool { state := atomic.LoadInt32((*int32)(unsafe.Pointer(&amp;m.Mutex))) return state&amp;mutexWoken == mutexWoken}// 锁是否处于饥饿状态func (m *Mutex) IsStarving() bool { state := atomic.LoadInt32((*int32)(unsafe.Pointer(&amp;m.Mutex))) return state&amp;mutexStarving == mutexStarving} 使用Mutex实现一个线程安全的队列通过 Slice 实现的队列不是线程安全的，出队（Dequeue）和入队（Enqueue）会有 data race 的问题。这个时候，Mutex 就要隆重出场了，通过它，我们可以在出队和入队的时候加上锁的保护。 12345678910111213141516171819202122232425262728type SliceQueue struct { data []interface{} mu sync.Mutex}func NewSliceQueue(n int) (q *SliceQueue) { return &amp;SliceQueue{data: make([]interface{}, 0, n)}}// Enqueue 把值放在队尾func (q *SliceQueue) Enqueue(v interface{}) { q.mu.Lock() q.data = append(q.data, v) q.mu.Unlock()}// Dequeue 移去队头并返回func (q *SliceQueue) Dequeue() interface{} { q.mu.Lock() if len(q.data) == 0 { q.mu.Unlock() return nil } v := q.data[0] q.data = q.data[1:] q.mu.Unlock() return v} 因为标准库中没有线程安全的队列数据结构的实现，所以，你可以通过 Mutex 实现一个简单的队列。通过 Mutex 我们就可以为一个非线程安全的 data interface{}实现线程安全的访问。 我的思考：使用Mutex实现一个线程安全的映射Map？ 总结思维导图总结…后续补充… 思考问题如果 Mutex 已经被一个 goroutine 获取了锁，其它等待中的 goroutine 们只能一直等待。那么，等这个锁释放后，等待中的 goroutine 中哪一个会优先获取 Mutex 呢？ 首先一点：等待的goroutine们是以FIFO排队的 1）当Mutex处于正常模式时，若此时没有新goroutine与队头goroutine竞争，则队头goroutine获得。若有新goroutine竞争大概率新goroutine获得。 2）当队头goroutine竞争锁失败达到1ms后，它会将Mutex调整为饥饿模式。进入饥饿模式后，锁的所有权会直接从解锁goroutine移交给队头goroutine，此时新来的goroutine直接放入队尾。 3）当一个goroutine获取锁后，如果发现自己满足下列条件中的任何一个则将锁切换回正常模式： 条件①：它是队列中最后一个；条件②：它等待锁的时间少于1ms 以上简略翻译自https://golang.org/src/sync/mutex.go 中注释Mutex fairness. ‍","link":"/post/go-concurrent-programming-mutex-2f1e0r.html"},{"title":"Go并发编程 | Channel","text":"本部分的内容比较特殊，channel这部分我认为是很重要的，可以说是比较特殊的一种设计，除了晁老师的讲解我还会结合其他博主、专家的分析以查缺补漏。参考不限于小徐、小白、煎鱼等知名博主，希望站在巨人的肩上总结和深造。其实这些博主总结都很详细，希望日后有时间做一个专题纵向对比下，哪哪个关键点博主的文章更加透彻，在依据他们的总结做出我个人的理解和重点提取。 重要记录： 2023.12 首次编写主要参考晁老师和《Go语言进阶之旅》，覆盖全面，建立了channel底层认知体系 Channel Channel 是 Go 语言内建的 first-class 类型，也是 Go 语言与众不同的特性之一。 Channel的设计来源于CSP理论，中文直译为通信顺序进程，或者叫做交换信息的循序进程，是用来描述并发系统中进行交互的一种模式。CSP 允许使用进程组件来描述系统，它们独立运行，并且只通过消息传递的方式通信。 Channel 类型是 Go 语言内置的类型，你无需引入某个包，就能使用它。虽然 Go 也提供了传统的并发原语，但是它们都是通过库的方式提供的，你必须要引入 sync 包或者atomic 包才能使用它们，而 Channel 就不一样了，它是内置类型，使用起来非常方便。 Channel 和 Go 的另一个独特的特性 goroutine 一起为并发编程提供了优雅的、便利的、与传统并发控制不同的方案，并演化出很多并发模式。 CSP‍ Channel设计思想Channel的设计思想：以通信方式共享内存,不要以共享内存方式通信 ​​ Channel使用场景 数据交流：当作并发的 buffer 或者 queue，解决生产者 - 消费者问题。多个goroutine 可以并发当作生产者（Producer）和消费者（Consumer）。 数据传递：一个 goroutine 将数据交给另一个 goroutine，相当于把数据的拥有权 (引用) 托付出去。 信号通知：一个 goroutine 可以将信号 (closing、closed、data ready 等) 传递给另一个或者另一组 goroutine 。 任务编排：可以让一组 goroutine 按照一定的顺序并发或者串行的执行，这就是编排的功能。 锁：利用 Channel 也可以实现互斥锁的机制。 Channel基本用法有三种：只能发、只能收、既能发又能收。通常也可以说是两种，单向channel和双向channel。 ​ChannelType = ( &quot;chan&quot; | &quot;chan&quot; &quot;&lt;-&quot; | &quot;&lt;-&quot; &quot;chan&quot; ) ElementType​ 习惯上将channel简写为chan，命名上也是沿用如此 正确示例： 123chan string // 可以发送接收stringchan&lt;- struct{} // 只能发送struct{}&lt;-chan int // 只能从chan接收int 核心记忆法：​这个箭头总是射向左边的，元素类型总在最右边。如果箭头指向 chan，就表示可以往chan 中塞数据；如果箭头远离 chan，就表示 chan 会往外吐数据。 channel中的元素可以是任意类型： 1234chan&lt;- chan intchan&lt;- &lt;-chan int&lt;-chan &lt;-chan intchan (&lt;-chan int) “&lt;-”有个规则，总是尽量和左边的chan结合 1234chan&lt;- （chan int） // &lt;- 和第一个chan结合chan&lt;- （&lt;-chan int） // 第一个&lt;-和最左边的chan结合，第二个&lt;-和左边第二个chan结合&lt;-chan （&lt;-chan int） // 第一个&lt;-和最左边的chan结合，第二个&lt;-和左边第二个chan结合chan (&lt;-chan int) // 因为括号的原因，&lt;-和括号内第一个chan结合 有缓冲和无缓冲channel： 通过 make，我们可以初始化一个 chan，未初始化的 chan 的零值是 nil。你可以设置它的容量，比如下面的 chan 的容量是 9527，我们把这样的 chan 叫做 buffered chan；如果没有设置，它的容量是 0，我们把这样的 chan 叫做 unbuffered chan。 ​make(chan int, 9527)​ 如果 chan 中还有数据，那么，从这个 chan 接收数据的时候就不会阻塞，如果 chan 还未满（“满”指达到其容量），给它发送数据也不会阻塞，否则就会阻塞。unbuffered chan 只有读写都准备好之后才不会阻塞，这也是很多使用 unbuffered chan 时的常见Bug。 还有一个知识点需要记住：nil 是 chan 的零值，是一种特殊的 chan，对值是 nil 的 chan 的发送接收调用者总是会阻塞。 发送数据1ch &lt;- 2000 往 chan 中发送一个数据使用“ch&lt;-”，发送数据是一条语句: 这里的 ch 是 chan int 类型或者是 chan &lt;-int。 接收数据从 chan 中接收一条数据使用“&lt;-ch”，接收数据也是一条语句： 123x := &lt;-ch // 把接收的一条数据赋值给变量x 2000foo(&lt;-ch) // 把接收的一个的数据作为参数传给函数 2000传给foo函数作为参数&lt;-ch // 丢弃接收的一条数据 2000丢了 这里的 ch 类型是 chan T 或者 &lt;-chan T。接收数据时，还可以返回两个值。第一个值是返回的 chan 中的元素，很多人不太熟悉的是第二个值。第二个值是 bool 类型，代表是否成功地从 chan 中读取到一个值，如果第二个参数是 false，chan 已经被 close 而且 chan 中没有缓存的数据，这个时候，第一个值是零值。所以，如果从 chan 读取到一个零值，可能是 sender 真正发送的零值，也可能是closed 的并且没有缓存元素产生的零值。 如何解决零值问题？在 Go 中，当从通道中接收到一个零值时，可能有多种原因。为了区分 sender 真正发送的零值和通道关闭时产生的零值，有几种方法可以尝试解决这个问题： 1. 使用额外的信号 可以在发送零值之前在通道中发送一个额外的信号，比如使用一个布尔型的通道来表示是否关闭： 1234567891011121314151617181920212223ch := make(chan int)closeFlag := make(chan bool)// Sendergo func() { // Send zero value or actual value if value != 0 { ch &lt;- value } else { close(ch) closeFlag &lt;- true }}()// Receiverselect {case value := &lt;-ch: // Received value fmt.Println(&quot;Received:&quot;, value)case &lt;-closeFlag: // Channel closed without value fmt.Println(&quot;Channel closed&quot;)} 2. 使用带缓冲的通道 带缓冲的通道可以在关闭前允许缓存元素。通过检查通道的长度，可以区分发送的零值和通道关闭时产生的零值： 12345678910111213141516171819202122232425ch := make(chan int, 1)// Sendergo func() { if value != 0 { ch &lt;- value } else { close(ch) }}()// Receiverselect {case value := &lt;-ch: // Received value fmt.Println(&quot;Received:&quot;, value)default: // Channel is closed or no value received if len(ch) == 0 { fmt.Println(&quot;Channel closed without value&quot;) } else { // Process received value fmt.Println(&quot;Received:&quot;, &lt;-ch) }} 这些方法可以帮助区分发送的零值和通道关闭时产生的零值。 其他操作Go 内建的函数 close、cap、len 都可以操作 chan 类型：close 会把 chan 关闭掉，cap返回 chan 的容量，len 返回 chan 中缓存的还未被取走的元素数量。 send 和 recv 都可以作为 select 语句的 case clause，如下面的例子：实际中并不常见 12345678910func main() { var ch = make(chan int, 10) for i := 0; i &lt; 10; i++ { select { case ch &lt;- i: case v := &lt;-ch: fmt.Println(v) } }} chan 还可以应用于 for-range 语句中，比如： 123for v := range ch { fmt.Println(v)} 或者是忽略读取的值，只是清空 chan： 12for range ch {} 在 Go 中，通道没有直接提供清空的内置方法。关于清空操作我的思考： 使用循环读取并丢弃通道中的值：可以使用循环读取通道中的值，并不处理这些值，即丢弃它们。当通道中的所有值都被读取后，通道就变为空了。 123456789for { select { case &lt;-ch: // 丢弃通道中的值 // Do nothing default: // 通道为空或已经被清空 return }} 重新创建通道：关闭现有的通道，然后创建一个新的通道。这种方式适用于不再需要之前通道中的值的情况。 1234567func emptyChannel(ch chan int) chan int { close(ch) return make(chan int)}// 使用新的空通道newCh := emptyChannel(ch) 这些方法可以在不同情况下清空通道。但需要注意的是，直接关闭并重新创建通道会中断任何等待写入或读取的操作。因此，在应用这些方法时，需要小心谨慎，确保不会影响到正在进行的操作。 Channel实现原理要懂channel的实现原理必须懂其数据结构和基本大局，即channel的本质是一个有锁的环形队列。 Channel数据结构chan 类型的数据结构如下图所示，它的数据类型是runtime.hchan。 ” 十一个字段+锁 “ 123456789101112131415// src/runtime/chan.gotype hchan struct { qcount uint dataqsiz uint buf unsafe.Pointer elemsize uint16 closed uint32 elemtype *_type sendx uint recvx uint recvq waitq sendq waitq lock mutex} ​​ ​​ qcount：代表 chan 中已经接收但还没被取走的元素的个数。内建函数 len 可以返回这个字段的值。同样可理解为队列中的元素总数量 dataqsiz：循环队列的长度（大小）；chan 使用一个循环队列来存放元素，循环队列很适合这种生产者 - 消费者的场景（我很好奇为什么这个字段省略 size 中的 e） buf：【存放元素的循环队列的 buffer】指向长度为 dataqsiz 的底层数组，仅有当 channel 为缓冲型的才有意义 elemtype 和 elemsize：chan 中元素的类型和 size。因为 chan 一旦声明，它的元素类型是固定的，即普通类型或者指针类型，所以元素大小也是固定的 closed：是否关闭 sendx：已发送元素在循环队列中的索引位置。处理发送数据的指针在 buf 中的位置。一旦接收了新的数据，指针就会加上elemsize，移向下一个位置。buf 的总大小是 elemsize 的整数倍，而且 buf 是一个循环列表 recvx：已接收元素在循环队列中的索引位置。处理接收请求时的指针在 buf 中的位置。一旦取出数据，此指针会移动到下一个位置 recvq：接受者的 sudog 等待队列（缓冲区不足时阻塞等待的 goroutine）。chan 是多生产者多消费者的模式，如果消费者因为没有数据可读而被阻塞了，就会被加入到 recvq 队列中 sendq：发送者的 sudog 等待队列。如果生产者因为 buf 满了而阻塞，会被加入到 sendq 队列中 因此可见 recvq 和 sendq 是整个的核心，只要是channel阻塞了底层就是它们俩搞的 Channel中真正的核心经常会忽略一个点：recvq​和sendq​ 的数据类型——waitq，waitg的背后是sudog的结构 ​recvq​ 和 sendq​，其表现为等待队列，其类型为 runtime.waitq​ 的双向链表结构： 1234type waitq struct { first *sudog last *sudog} 且无论是 first​ 属性又或是 last​，其类型都为 runtime.sudog​ 结构体： 12345678type sudog struct { g *g next *sudog prev *sudog elem unsafe.Pointer ...} g：指向当前的 goroutine。 next：指向下一个 g。 prev：指向上一个 g。 elem：数据元素，可能会指向堆栈。 sudog 是 Go 语言中用于存放协程状态为阻塞的 goroutine 的双向链表抽象，你可以直接理解为一个正在等待的 goroutine 就可以了。 Channel生命周期要理解Channel 设计的完整核心无非就是按照它的使用流程（生命周期）——创建、发送、接受、关闭来推敲。头脑中清楚channel在这四个过程中大致做了什么工作，基本就能理解它了 创建Chan核心就是：根据 chan 的容量的大小和元素的类型不同，初始化不同的存储空间 创建 channel 的演示代码： 1ch := make(chan string) 其在编译器翻译后对应 runtime.makechan​ 或 runtime.makechan64​ 方法：其实只关注 makechan 就好了，因为 makechan64 只是做了 size 检查，底层还是调用makechan 实现的 12345// 通用创建方法func makechan(t *chantype, size int) *hchan// 类型为 int64 的进行特殊处理 缓冲区很大？！func makechan64(t *chantype, size int64) *hchan channel 的基本单位是 hchan​ 结构体，那么在创建 channel 时，究竟还需要做什么，这可以进一步帮助你理解其中的结构 分析一下 makechan​ 方法，就能知道了。 源码如下： 123456789101112131415161718192021222324252627282930// src/runtime/chan.gofunc makechan(t *chantype, size int) *hchan { elem := t.elem // 检查代码 mem, _ := math.MulUintptr(elem.size, uintptr(size)) var c *hchan // 分配内存 switch { case mem == 0: // chan的size或者元素的size是0，不必创建buf c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.ptrdata == 0: // 元素不是指针，分配一块连续的内存给hchan数据结构和buf c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // 元素包含指针，那么单独分配buf c = new(hchan) c.buf = mallocgc(mem, elem, true) } // 元素大小、类型、容量都记录下来 c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(&amp;c.lock, lockRankHchan) return c} 创建 channel 的逻辑主要分为三大块： 当前 channel 不存在缓冲区，也就是元素大小为 0 的情况下，就会调用 mallocgc​​ 方法分配一段连续的内存空间。 当前 channel 存储的类型存在指针引用，就会连同 hchan​​ 和底层数组同时分配一段连续的内存空间。 通用情况，即不包含指针的情况，默认分配相匹配的连续内存空间。 最终，针对不同的容量和元素类型，这段代码分配了不同的对象来初始化 hchan 对象的字段，返回 hchan 对象。 需要注意到一块特殊点，那就是 channel 的创建都是调用的 mallocgc 方法，也就是 channel 都是创建在堆上的。因此 channel 是会被 GC 回收的，自然也不总是需要 close 方法来进行显示关闭了。 从整体上来讲，makechan​ 方法的逻辑比较简单，就是创建 hchan​ 并分配合适的 buf​ 大小的堆上内存空间。 发送数据channel 发送数据的演示代码： 123go func() { ch &lt;- &quot;加练&quot;}() 其在编译器翻译后对应 runtime.chansend1​ 方法： 123func chansend1(c *hchan, elem unsafe.Pointer) { chansend(c, elem, true, getcallerpc())} 其作为编译后的入口方法，实则指向真正的实现逻辑，也就是 chansend​ 方法。 整体： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { // 第一部分，nil channel if c &lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; nil { if !block { return false } gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(&quot;unreachable&quot;) } // 第二部分，如果chan没有被close,并且chan满了，直接返回 if !block &amp;&amp; c.closed &lt;/span&gt; 0 &amp;&amp; full(c) { return false } // 第三部分，chan已经被close的情景,再往里面发送数据的话会panic lock(&amp;c.lock) // 开始加锁 if c.closed != 0 { unlock(&amp;c.lock) panic(plainError(&quot;send on closed channel&quot;)) } // 第四部分，从接收队列中出队一个等待的receiver // 如果等待队列中有等待的 receiver，那么这段代码就把它从队列中弹出， // 然后直接把数据交给它（通过 memmove(dst, src, t.size)），而不需要放入到 buf 中，速度可以更快一些。 if sg := c.recvq.dequeue(); sg != nil { send(c, sg, ep, func() { unlock(&amp;c.lock) }, 3) return true } // 第五部分，buf还没满 if c.qcount &lt; c.dataqsiz { qp := chanbuf(c, c.sendx) if raceenabled { raceacquire(qp) racerelease(qp) } typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(&amp;c.lock) return true } // 第六部分，buf满的情况。如果buf满了，发送者的 goroutine 就会加入到发送者的等待队列中，直到被唤醒。 // 这个时候，数据或者被取走了，或者 chan 被 close 了。 // chansend1不会进入if块里，因为chansend1的block=true if !block { unlock(&amp;c.lock) return false } // 省略一些调试相关 ...}func full(c *hchan) bool { if c.dataqsiz &lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; 0 { return c.recvq.first &lt;/span&gt; nil } return c.qcount &lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; c.dataqsiz} 前置处理在第一部分中，我们先看看 chan 发送的一些前置判断和处理： 12345678910111213141516171819202122func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { // nil channel if c &lt;/span&gt; nil { if !block { return false } gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(&quot;unreachable&quot;) } // 省略一些调试相关 ...}func full(c *hchan) bool { if c.dataqsiz &lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; 0 { return c.recvq.first &lt;/span&gt; nil } return c.qcount == c.dataqsiz} 一开始 chansend​ 方法在会先判断当前的 channel 是否为 nil。若为 nil，在逻辑上来讲就是向 nil channel 发送数据，就会调用 gopark​ 方法使得当前 Goroutine 休眠，进而出现死锁崩溃，表象就是出现 panic​ 事件来快速失败。 紧接着会对非阻塞的 channel 进行一个上限判断，看看是否快速失败。 失败的场景如下： 若非阻塞且未关闭，同时底层数据 dataqsiz 大小为 0（缓冲区无元素），则会返回失败。-无缓冲管道写入失败 若是 qcount 与 dataqsiz 大小相同（缓冲区已满）时，则会返回失败。 -有缓存管道但缓存满了写入失败 上互斥锁在完成了 channel 的前置判断后，即将在进入发送数据的处理前，channel 会进行上锁： 1234func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... lock(&amp;c.lock)} 上锁后就能保住并发安全。另外我们也可以考虑到，这种场景会相对依赖单元测试的覆盖，因为一旦没考虑周全，漏上锁了，基本就会出问题。 直接发送在正式开始发送前，加锁之后，会对 channel 进行一次状态判断（是否关闭）： 1234567891011121314func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... // chan已经被关闭了，这个时候写往里写入数据会panic，我相信大家会瞬间浮现报错的场景 if c.closed != 0 { unlock(&amp;c.lock) panic(plainError(&quot;send on closed channel&quot;)) } // 存在阻塞过程中的缓冲数据 if sg := c.recvq.dequeue(); sg != nil { send(c, sg, ep, func() { unlock(&amp;c.lock) }, 3) return true }} 这种情况是最为基础的，也就是当前 channel 有正在阻塞等待的接收方，那么只需要直接发送就可以了 缓冲发送非直接发送，那么就考虑第二种场景，判断 channel 缓冲区中是否还有空间： 12345678910111213141516171819func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... if c.qcount &lt; c.dataqsiz { qp := chanbuf(c, c.sendx) typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(&amp;c.lock) return true } if !block { unlock(&amp;c.lock) return false }} 会对缓冲区进行判定（qcount​ 和 dataqsiz​ 字段），以此识别缓冲区的剩余空间。紧接进行如下操作： 调用 chanbuf​ 方法，以此获得底层缓冲数据中位于 sendx 索引的元素指针值。 调用 typedmemmove​ 方法，将所需发送的数据拷贝到缓冲区中。 数据拷贝后，对 sendx 索引自行自增 1。同时若 sendx 与 dataqsiz 大小一致，则归 0（环形队列）。 自增完成后，队列总数同时自增 1。解锁互斥锁，返回结果。 至此针对缓冲区的数据操作完成。但若没有走进缓冲区处理的逻辑，则会判断当前是否阻塞 channel，若为非阻塞，将会解锁并直接返回失败。 配合图示如下： ​​​​ 阻塞发送在进行了各式各样的层层筛选后，接下来进入阻塞等待发送的过程： 1234567891011121314151617181920212223func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) atomic.Store8(&amp;gp.parkingOnChan, 1) gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) KeepAlive(ep)} 调用 getg​ 方法获取当前 goroutine 的指针，用于后续发送数据。 调用 acquireSudog​ 方法获取 sudog​ 结构体，并设置当前 sudog 具体的待发送数据信息和状态。 调用 c.sendq.enqueue​ 方法将刚刚所获取的 sudog​ 加入待发送的等待队列。 调用 gopark​ 方法挂起当前 goroutine（会记录执行位置），状态为 waitReasonChanSend，阻塞等待 channel。 调用 KeepAlive​ 方法保证待发送的数据值是活跃状态，也就是分配在堆上，避免被 GC 回收。 配合图示如下： ​​ 在当前 goroutine 被挂起后，其将会在 channel 能够发送数据后被唤醒： 12345678910111213141516171819202122func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... // 从这里开始唤醒，并恢复阻塞的发送操作 if mysg != gp.waiting { throw(&quot;G waiting list is corrupted&quot;) } gp.waiting = nil gp.activeStackChans = false if gp.param &lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; nil { if c.closed &lt;/span&gt; 0 { throw(&quot;chansend: spurious wakeup&quot;) } panic(plainError(&quot;send on closed channel&quot;)) } gp.param = nil if mysg.releasetime &gt; 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) return true} 唤醒 goroutine（调度器在停止 g 时会记录运行线程和方法内执行的位置）并完成 channel 的阻塞数据发送动作后。进行基本的参数检查，确保是符合要求的（纵深防御），接着开始取消 mysg 上的 channel 绑定和 sudog 的释放。 至此完成所有类别的 channel 数据发送管理。","link":"/post/go-parallel-programming-channel-z13snhx.html"},{"title":"Go并发编程 | 总纲","text":"Go并发编程 | 总纲 本专题参考以晁岳攀老师为主，以培养全面、系统的Go并发编程学习体系。 期间可能也会掺杂一些个人理解或者其他的参考资料进行补充和校正。 持续更新中······ 预计2024年初更新完 ​​ 基本介绍基本并发原语：Mutex、RWMutex、Waitgroup、Cond、Pool、Context 等标准库中的并发原语，这些都是传统的并发原语，在其它语言中可能也会有。一通百通 原子操作：原子操作是其它并发原语的基础，学会了你就可以自己创造新的并发原语。 Channel：Channel 类型是 Go 语言独特的类型，因为比较新，较难以掌握，掌握它的基本用法、处理场景和应用模式，避免踩坑。 扩展并发原语：目前来看，Go 开发组不准备在标准库中扩充并发原语了，但是还有一些并发原语应用广泛，比如信号量、SingleFlight、循环栅栏、ErrGroup 等。掌握了它们，就可以在处理一些并发问题时，取得事半功倍的效果。 分布式并发原语：分布式并发原语是应对大规模的应用程序中并发问题的并发类型。我主要会介绍使用 etcd 实现的一些分布式并发原语，比如 Leader 选举、分布式互斥锁、分布式读写锁、分布式队列等，在处理分布式场景的并发问题时，特别有用。这一部分结合了分布式的知识，将并发和分布式结合可能会受益匪浅。 目标学习主线层面，主要是四大步骤，包括​基础用法、实现原理、易错场景、知名项目中的 Bug。每一个模块，我都会带着你按照这四个步骤来学习，目的就是带你熟知每一种并发原语的实现机制和适用场景 实际上，针对同一种场景，也许存在很多并发原语都适用的情况，但是一定是有最合适的那一个。所以，你必须非常清楚每种并发原语的实现机制和适用场景，千万不要被网上的一些文章误导，万事皆用 Channel 奔着精通的方向： 深入学习下 Go 并发原语的源代码。你会发现很多独到的设计，比如Mutex 为了公平性考量的设计、sync.Map 为提升性能做的设计，以及很多并发原语的异常状况的处理方式。尤其是这些异常状况，常常是并发编程中程序 panic 的原因。 没有做过大型并发项目：可能不理解并发原语的重要性。——&gt;参看知名项目中犯过的错，站在巨人的肩膀上分析总结 怎么算并发编程学的好，精通了？ 晁岳攀老师给出的建议是：自己创造并发原语库和实现机制以及适用场景。 ‍","link":"/post/go-parallel-programming-chief-outline-znxfpt.html"},{"title":"","text":"Golang 📄 Go典型使用·实例 📑 重学系列 📄 Go经验小记 📄 练习两年半释疑 📄 Go新版本特性 📄 最常用的 Go CLI 命令 📄 重学Go语言 | 如何在Go中使用Context 📑 Go并发编程 📄 Go语言实现的可读性更高的并发神库 🥗 Golang门面担当 📄 GMP模型的知识体系 📄 Go 语言五大日志库 📄 Go语言——延迟函数defer的使用 📄 Go语言——Map的底层介绍及扩缩容机制 📄 Go语言——切片(Slice)的坑 📄 Go语言——内存管理 📑 专项 📄 Gorm框架的思考 📄 Go与操作系统的线程（进程）间通信 📄 Go单机锁与同步原语 📄 深入解析go channel各状态下的操作结果 📄 解密Go协程的栈内存管理 📄 Golang实现延迟队列（DelayQueue） 📄 go channel的应用案例 📑 Gin深入理解 📄 Gin框架流程原理以及上下文、内存的思考 📄 我给 gin 提交了一行代码 - 墨天轮 📄 由Gin路由原理引发的一系列思考 📄 Gin路由设计及源码分析：httprouter路由实现原理 📄 Gin生命周期 📑 Google书签整理 📄 Go Modules 依赖管理，这篇总结的挺全 📄 最全Go select底层原理，一文学透高频用法 📄 剑指 Offer 38. 字符串的排列 📄 阅读破10万的学Go建议，不管初学还是进阶Go都值得一看！ 📍 日常小结 📄 【Golang】来几道题以加强切片知识 📄 goland-IDEOlog 插件 📄 Rob Pike谈Google Go 📄 Go的数组与切片傻傻分不清楚 📄 Go获取IP地址 📄 json字段控制 📄 Go 语言 iota 的神奇力量 📄 日志框架 📄 Go语言的%d,%p,%v等占位符的使用 📄 Go Web开发的五大利器 📄 Go中常见的IO模式 📄 Go常见错误第16篇：any的常见错误和最佳实践 - 掘金 📄 GORM框架 📄 标准库-time包 📄 高阶函数编程Go语言中的函数一等公民 📄 【Golang】怎样优雅的清空切片 📄 Go web项目布局 📄 死锁、活锁、饥饿、自旋锁（Go 语言描述） 📄 2023.3.8小结 📄 《100 Go Mistakes and How to Avoid Them》 📑 进阶篇 📄 Context包源码解读 📄 nethttp库源码解读 📄 Golang 单机锁实现原理 📄 进阶篇之函数篇 📄 优秀后端都应该具备的开发好习惯 📄 进阶篇之结构体篇 📄 Go Channel底层原理 📄 go语言反射的使用 📄 time包 📄 json字段控制 📄 深入浅出Go调度器中的GMP模型 📄 有无缓冲管道 📄 一文让你理解go语言的Context 📄 GO语言实现设计模式【上】 📄 GO语言实现设计模式【下】 📑 框架 📑 Go-zero 📄 玩转 Go 链路追踪 📄 go-zero 是如何追踪你的请求链路 📄 GoFrame学习之路 📑 RPC与GRPC 📄 RPC 📄 grpcurl的使用 📄 grpcui安装使用 ‍","link":"/post/golang-2sensm.html"},{"title":"Goland 快捷键、模板与一些规范","text":"Goland 快捷键、模板与一些规范题记： 快捷键的使用可以大大提高效率，良好的注释对项目后续的开发维护工作也是十分必要。本文档旨在明确项目开发过程中go代码的注释规范，并提供基于goland的注释模板设置指导。便于开发人员快速配置环境，高效、合规开展工作。 规范部分随缘整理….. 我的配置 Go文件采用goland自带的go文件和代码模板 123456789101112/** * @Description //TODO * @Author luommy * @Date ${DATE} ${TIME} **/package ${GO_PACKAGE_NAME}// 我不习惯加这个func main() {} ​​​​ ‍ 方法、结构体、接口注释采用插件实现 ‍ 自定义快捷键与注释模板​自定义快捷键：CTRL+J​ ​​ 添加新建类的注释模板路径：File -&gt; Settings -&gt; File and Code Templates ​​ 添加如下信息： 123456789package ${GO_PACKAGE_NAME}/** * @Description //TODO * @Author luommy * @Date ${DATE} ${TIME} **/func main() {} 添加方法注释模板‍ Live Templates ​ go templates 内置函数‍ 在模板变量中使用的预定义函数有很多都用不到的，感觉并不好用，没啥能用到的场景 Item Description ​lowercaseAndDash(String)​​ 将驼峰字符串转换为小写，并插入连接符作为分隔符。例如,lowercaseAndDash(MyExampleName)​​返回my-example-name​​. ​snakeCase(String)​​ 将驼峰字符串转化为蛇形字符串，例如,snakeCase(fooBar)​​返回foo_bar​​. ​spaceSeparated(String)​​ 将字符串转化为小写并插入空格作为分隔符，例如,spaceSeparated(fooBar)​​returnsfoo bar​​. ​underscoresToCamelCase(String)​​ 将蛇形字符串转化为驼峰字符串. 例如,underscoresToCamelCase(foo_bar)​​返回fooBar​​. ​underscoresToSpaces(sParameterWithSpaces)​​ 将字符串中的下划线替换为空格. 例如,underscoresToSpaces(foo_bar)​​返回foo bar​​. ​camelCase(String)​​ 将字符串转化为驼峰法. 例如,camelCase(my-text-file)​​,camelCase(my text file)​​, 和camelCase(my_text_file)​​均返回myTextFile​​. ​capitalize(String)​​ 将参数的第一个字母大写。 ​capitalizeAndUnderscore(sCamelCaseName)​​ 根据驼峰法分割参数，将各个部分转化为大写，并插入下划线。例如,capitalizeAndUnderscore(FooBar)​​返回FOO_BAR​​. ​classNameComplete()​​ 这个表达式代替了在变量位置完成类名。 ​clipboard()​​ 返回系统剪贴板的内容。 ​complete()​​ 引用代码完成。 ​completeSmart()​​ 调用变量位置的智能类型完成。 ​concat(expressions...)​​ 返回传递给函数的所有字符串作为参数的级联。 ​date(sDate)​​ 以指定的格式返回当前系统日期。如果没有参数，则以默认的系统格式返回当前日期。 ​decapitalize(sName)​​ 用相应的小写字母替换参数的第一个字母。 ​enum(sCompletionString1,sCompletionString2,...)​​ 返回在模板扩展时建议完成的逗号分隔字符串的列表。 ​escapeString(sEscapeString)​​ Escapes the string specified as the parameter. ​expectedType()​​ Returns the expected type of the expression into which the template expands. Makes sense if the template expands in the right part of an assignment, afterreturn​​, etc. ​fileName()​​ 返回当前文件（包括扩展名） ​fileNameWithoutExtension()​​ 返回当前文件（不包括扩展名） ​firstWord(sFirstWord)​​ 返回作为参数传递的字符串的第一个单词。 ​lineNumber()​​ 返回当前行数。 ​substringBefore(String,Delimiter)​​ 删除指定分隔符之后的扩展名，只返回文件名。这对测试文件名是有帮助的，例如,substringBefore($FileName$,&quot;.&quot;)​​returnscomponent-test​​incomponent-test.js​​). ​time(sSystemTime)​​ 以指定的格式返回当前系统时间。 ​timestamp()​​ 返回当前系统时间戳 ​user()​​ 返回当前用户 ‍ ‍ 中英互译插件目前觉得挺好用的 支持自定义快捷键：setting-keymap-plugins 习惯设置Alt+T 一键互译 ​​ 注释插件：Goanno插件市场安装，通过tools进行配置——Goanno Setting ​​​​ 方法、接口、结构体注释模板配置​​ 配置内容如下： 123456789101112131415161718192021222324Normal Method 配置内容：/** @Title ${function_name} @Description ${todo} @Author luommy ${date} @Param ${params} @Return ${return_types} **/interface配置内容// ${interface_name} Interface Method 配置内容// @Title ${function_name} // @Description ${todo} // @Author luommy ${date} // @Param ${params} // @Return ${return_types} Struct配置内容// ${struct_name} Struct Field 不做配置 配置完成点击submit 验证注释 在方法、结构体、接口上 win使用快捷键: ctrl +alt +/​ mac使用快捷键:control + commond + /​ 可自动生成注释 如下截图: ​ AI 插件 CodeGeeX官网 几个重点的功能： 多语言代码之间翻译，无缝转换 注释生成代码/自动补全代码 AI问答","link":"/post/goland-shortcut-key-template-and-some-specifications-z2131hx.html"},{"title":"Golang测试","text":"Golang测试相关引言Go 语言的单元测试默认采用官方自带的测试框架，通过引入 testing 包，通过执行 go test​ 命令来实现单元测试功能。 在源代码包目录内，所有以 _test.go​ 为后缀名的源文件会被 go test​ 认定为单元测试的文件，这些单元测试的文件不会包含在 go build​ 的源代码构建中，而是单独通过 go test 来编译并执行。 不写测试的开发不是好程序员。TDD（Test Driven Development）在国内其实都不太关注测试这一部分。 这篇文章主要介绍下在Golang中如何做单元测试和基准测试，同时自己测试时也发现了一些问题。 单元测试Go 单元测试的基本规范Go 单元测试的基本规范如下： 每个测试函数都必须导入 testing 包。测试函数的命名类似func TestName(t *testing.T)​，入参必须是 *testing.T​类型 测试函数的函数名必须以大写的 Test 开头，后面紧跟函数名，要么是大写，要么就是下划线，比如 func TestName(t *testing.T)​ 或者 func Test_name(t *testing.T)​ 都是可以的， 但是 func Testname(t *testing.T)​这不会被检测到，一定要注意 通常情况下，需要将测试文件和源代码放在同一个包内。一般测试文件的命名，都是 {source_filename}_test.go​，比如源代码文件是ai.go ，那么就会在 ai.go 的相同目录下，再建立一个 ai_test.go 的单元测试文件去测试 ai.go 文件里的相关方法。 当运行 go test 命令时，go test 会遍历当前目录下所有的 *_test.go​ 中符合上述命名规则的函数，然后生成一个临时的 main 包用于调用相应的测试函数，然后构建并运行、报告测试结果，最后清理测试中生成的临时文件。 常用命令单元测试： ​go test ​ ​go test -v​ ​go test -v -run=&quot;方法名关键字&quot;​ 覆盖率测试： ​go test -cover -coverprofile=输出文件名字​ ​go tool cover -html=输出文件名字​ -v参数: 是在测试结果中补充被测函数名称和运行时间 -run参数: 它对应一个正则表达式，只有函数名匹配上的测试函数才会被go test​命令执行。 测试示例这里参考七米的示例： 定义一个split​的包，包中定义了一个Split​函数，函数功能是为了从一个s字符串中去除sep，具体实现如下： 123456789101112131415161718192021// split/split.gopackage splitimport &quot;strings&quot;// split package with a single split function.// Split slices s into all substrings separated by sep and// returns a slice of the substrings between those separators.func Split(s, sep string) (result []string) { i := strings.Index(s, sep) for i &gt; -1 { result = append(result, s[:i]) s = s[i+1:] //这里有bug，没有考虑到sep为多个字符的情况：s = s[i+len(sep):] 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return} 然后 ***在相同的目录下 *** 写测试函数（可以写多个测试函数，运行时可加run参数正则匹配对应的被测函数，如果不加参数默认测试全部的）： 1234567891011121314151617// split/split_test.gopackage splitimport ( &quot;reflect&quot; &quot;testing&quot;)// 这里注意函数名规范：Test+首字母大写的被测函数名func TestSplit(t *testing.T) { // 测试函数名必须以Test开头，必须接收一个*testing.T类型参数 got := Split(&quot;a:b:c&quot;, &quot;:&quot;) // 程序输出的结果 want := []string{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;} // 期望的结果 if !reflect.DeepEqual(want, got) { // 因为slice不能比较直接，借助反射包中的方法比较 t.Errorf(&quot;expected:%v, got:%v&quot;, want, got) // 测试失败输出错误提示 }} 此时的目录结构： 1234split $ ls -ltotal 16-rw-r--r-- 1 liwenzhou staff 408 4 29 15:50 split.go-rw-r--r-- 1 liwenzhou staff 466 4 29 16:04 split_test.go 然后在此目录下执行go test​， 结果如下： 123split $ go testPASSok github.com/Q1mi/studygo/code_demo/test_demo/split 0.005s PASS标志测试成功 一般都习惯加下 -v 参数 再写一个测试函数TestMoreSplit，具体内容已省略，失败的情况： 123456789split $ go test -v=&lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; RUN TestSplit--- PASS: TestSplit (0.00s)&lt;/span&gt;= RUN TestMoreSplit--- FAIL: TestMoreSplit (0.00s) split_test.go:21: expected:[a d], got:[a cd]FAILexit status 1FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.005s 测试用例组这实际环境下通常要考虑一些更全面的东西，比如中文切割。 参考七米的例子：有多个测试用例组 12345678910111213141516171819202122func TestSplit(t *testing.T) { // 定义一个测试用例类型 type test struct { input string sep string want []string } // 定义一个存储测试用例的切片 tests := []test{ {input: &quot;a:b:c&quot;, sep: &quot;:&quot;, want: []string{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}}, {input: &quot;a:b:c&quot;, sep: &quot;,&quot;, want: []string{&quot;a:b:c&quot;}}, {input: &quot;abcd&quot;, sep: &quot;bc&quot;, want: []string{&quot;a&quot;, &quot;d&quot;}}, {input: &quot;沙河有沙又有河&quot;, sep: &quot;沙&quot;, want: []string{&quot;河有&quot;, &quot;又有河&quot;}}, } // 遍历切片，逐一执行测试用例 for _, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(&quot;expected:%v, got:%v&quot;, tc.want, got) } }} go test 执行后，发现并不能直观的看出 1234567split $ go test -v=&lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; RUN TestSplit--- FAIL: TestSplit (0.00s) split_test.go:42: expected:[河有 又有河], got:[ 河有 又有河]FAILexit status 1FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s 这里有一个小技巧：使用​%#v​的格式化方式 t.Errorf(“expected:%v, got:%#v”, tc.want, got) 然后就很直观了： 1234567split $ go test -v&lt;/span&gt;= RUN TestSplit--- FAIL: TestSplit (0.00s) split_test.go:42: expected:[]string{&quot;河有&quot;, &quot;又有河&quot;}, got:[]string{&quot;&quot;, &quot;河有&quot;, &quot;又有河&quot;}FAILexit status 1FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s 然后就再优化下被测函数或者干脆直接把测试函数的期望改了 改测试函数期望：want: []string{&quot;&quot;,&quot;河有&quot;, &quot;又有河&quot;}​ 改被测函数：比如在result = append(result, s)​之前判断下len(s)是否大于0,或者考虑下strings.Index的合理性… 子测试如果在测试中有大量的测试用例，不好区分哪个测试用例失效了，这种情况下要用到子测试，其实也可以用map的方式 map： 12345678910111213141516171819func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 &quot;simple&quot;: {input: &quot;a:b:c&quot;, sep: &quot;:&quot;, want: []string{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}}, &quot;wrong sep&quot;: {input: &quot;a:b:c&quot;, sep: &quot;,&quot;, want: []string{&quot;a:b:c&quot;}}, &quot;more sep&quot;: {input: &quot;abcd&quot;, sep: &quot;bc&quot;, want: []string{&quot;a&quot;, &quot;d&quot;}}, &quot;leading sep&quot;: {input: &quot;沙河有沙又有河&quot;, sep: &quot;沙&quot;, want: []string{&quot;河有&quot;, &quot;又有河&quot;}}, } for name, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(&quot;name:%s expected:%#v, got:%#v&quot;, name, tc.want, got) // 将测试用例的name格式化输出 } }} 子测试：t.Run() 123456789101112131415161718192021func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 &quot;simple&quot;: {input: &quot;a:b:c&quot;, sep: &quot;:&quot;, want: []string{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}}, &quot;wrong sep&quot;: {input: &quot;a:b:c&quot;, sep: &quot;,&quot;, want: []string{&quot;a:b:c&quot;}}, &quot;more sep&quot;: {input: &quot;abcd&quot;, sep: &quot;bc&quot;, want: []string{&quot;a&quot;, &quot;d&quot;}}, &quot;leading sep&quot;: {input: &quot;沙河有沙又有河&quot;, sep: &quot;沙&quot;, want: []string{&quot;河有&quot;, &quot;又有河&quot;}}, } for name, tc := range tests { t.Run(name, func(t *testing.T) { // 使用t.Run()执行子测试 got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(&quot;expected:%#v, got:%#v&quot;, tc.want, got) } }) }} 再go test： 123456789101112131415split $ go test -v=&lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; RUN TestSplit&lt;/span&gt;= RUN TestSplit/leading_sep=&lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; RUN TestSplit/simple&lt;/span&gt;= RUN TestSplit/wrong_sep=== RUN TestSplit/more_sep--- FAIL: TestSplit (0.00s) --- FAIL: TestSplit/leading_sep (0.00s) split_test.go:83: expected:[]string{&quot;河有&quot;, &quot;又有河&quot;}, got:[]string{&quot;&quot;, &quot;河有&quot;, &quot;又有河&quot;} --- PASS: TestSplit/simple (0.00s) --- PASS: TestSplit/wrong_sep (0.00s) --- PASS: TestSplit/more_sep (0.00s)FAILexit status 1FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s 补充： 可以通过-run=RegExp​来指定运行的测试用例，还可以通过/​来指定要运行的子测试用例，例如：go test -v -run=Split/simple​只会运行simple​对应的子测试用例。 意料之外的情况将被测函数NewLRU与测试函数NewLRU_test的目录下有其他的文件，且这些有文件在main包中，这种情况下，main会被检测到，导致其它不相干的函数也被检测到了。 如果目录像我这种：我有一个LRU算法函数和一个测试LRU的测试函数 ​​ 直接go test会报错： ​​ 解决方式： 执行文件：go test .\\NewLRU.go .\\NewLRU_test.go -v​ ,可这也太麻烦了 移到的新包下，比如新建个test目录，将NewLRU.go​与NewLRU_test.go​一进去 通常业界也观察到，大部分测试文件和被测文件是放在同一个目录下的 覆盖率测试测试覆盖率是你的代码被测试套件覆盖的百分比。通常我们使用的都是语句的覆盖率，也就是在测试中至少被运行一次的代码占总代码的比例。 Go提供内置功能来检查你的代码覆盖率。我们可以使用go test -cover​来查看测试覆盖率。例如： 123PASScoverage: 100.0% of statementsok main/test 0.818s o还提供了一个额外的-coverprofile​参数，用来将覆盖率相关的记录信息输出到一个文件。例如： ​go test -cover -coverprofile=c​ 执行go tool cover -html=c​，使用cover​工具来处理生成的记录信息，该命令会打开本地的浏览器窗口生成一个HTML报告 ​​ 每个用绿色标记的语句块表示被覆盖了，而红色的表示没有被覆盖。 基准测试 我一般理解为性能测试，在一定工作负载下检测程序性能的一种方法。 后续补充…..","link":"/post/golang-test-related-mbnma.html"},{"title":"Go怎么让协程跑一半就退出？","text":"Go怎么让协程跑一半就退出？ go中runtime.Goexit()可以让跑一半的协程退出？ 我的第一反应是创建了一个context.Context​对象，并通过context.WithCancel​函数创建一个可以取消的上下文。然后，在协程中使用select​语句监听ctx.Done()​通道，当接收到退出信号时，协程会执行退出操作。 参考小白了解到了一些不一样的东西 我的： 1234567891011121314151617181920212223242526272829303132333435package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;time&quot;)func main() { ctx, cancel := context.WithCancel(context.Background()) defer cancel() go func(ctx context.Context) { for { select { case &lt;-ctx.Done(): fmt.Println(&quot;协程收到退出信号，退出&quot;) return default: // 执行协程任务 fmt.Println(&quot;正在执行任务...&quot;) time.Sleep(time.Second) } } }(ctx) // 等待一段时间后取消协程 time.Sleep(3 * time.Second) cancel() // 等待协程退出 time.Sleep(time.Second) fmt.Println(&quot;主线程退出&quot;)} 我们平时创建一个协程，跑一段逻辑，代码大概长这样。 1234567891011121314151617181920212223package main import ( &quot;fmt&quot; &quot;time&quot;)func Foo() { fmt.Println(&quot;打印1&quot;) defer fmt.Println(&quot;打印2&quot;) fmt.Println(&quot;打印3&quot;)} func main() { go Foo() fmt.Println(&quot;打印4&quot;) time.Sleep(1000*time.Second)} // 这段代码，正常运行会有下面的结果打印4打印1打印3打印2 注意这上面”​打印2​”是在defer​中的，所以会在函数结束前打印。因此后置于”​打印3​”。 那么今天的问题是，如何让Foo()​函数跑一半就结束，比如说跑到打印2，就退出协程（不让defer中的“3”打印）。输出如下结果 123打印4打印1打印2 如何实现？ 答：在”打印2”后面插入一个 runtime.Goexit()​​， 协程就会直接结束。并且结束前还能执行到defer​​里的​打印2​。 12345678910111213141516171819202122232425package main import ( &quot;fmt&quot; &quot;runtime&quot; &quot;time&quot;)func Foo() { fmt.Println(&quot;打印1&quot;) defer fmt.Println(&quot;打印2&quot;) runtime.Goexit() // 加入这行代码 可实现打印2结束退出 fmt.Println(&quot;打印3&quot;)} func main() { go Foo() fmt.Println(&quot;打印4&quot;) time.Sleep(1000*time.Second)} // 输出结果打印4打印1打印2 可以看到打印3这一行没出现了，协程确实提前结束了。 一图梗概​​ runtime.Goexit()是什么？看一下内部实现。 1234567891011121314func Goexit() { // 以下函数省略一些逻辑... gp := getg() for { // 获取defer并执行 d := gp._defer reflectcall(nil, unsafe.Pointer(d.fn), deferArgs(d), uint32(d.siz), uint32(d.siz)) } goexit1()} func goexit1() { mcall(goexit0)} 从代码上看，runtime.Goexit()​会先执行一下defer​里的方法，这里就解释了开头的代码里为什么在defer里的打印2能正常输出。 然后代码再执行goexit1​。本质就是对goexit0​的简单封装。 我们可以把代码继续跟下去，看看goexit0​做了什么。 123456789101112131415161718192021222324// goexit continuation on g0.func goexit0(gp *g) { // 获取当前的 goroutine _g_ := getg() // 将当前goroutine的状态置为 _Gdead casgstatus(gp, _Grunning, _Gdead) // 全局协程数减一 if isSystemGoroutine(gp, false) { atomic.Xadd(&amp;sched.ngsys, -1) } // 省略各种清空逻辑... // 把g从m上摘下来。 dropg() // 把这个g放回到p的本地协程队列里，放不下放全局协程队列。 gfput(_g_.m.p.ptr(), gp) // 重新调度，拿下一个可运行的协程出来跑 schedule()} 这段代码，信息密度比较大。 很多名词可能让人一脸懵。 简单描述下，Go语言里有个GMP模型的说法，M​是内核线程，G​也就是我们平时用的协程goroutine​，P​会在G和M之间​做工具人，负责调度​G​到M​上运行。 ​ 既然是​调度​，也就是说不是每个G​都能一直处于运行状态，等G不能运行时，就把它存起来，再调度下一个能运行的G过来运行。 暂时不能运行的G，P上会有个本地队列去存放这些这些G，P的本地队列存不下的话，还有个全局队列，干的事情也类似。 了解这个背景后，再回到 goexit0​ 方法看看，做的事情就是将当前的协程G置为_Gdead​状态（说白了这个runtime.Goexit()就是让这个线程去死），即：然后把它从M上摘下来，尝试放回到P的本地队列中。然后重新调度一波，获取另一个能跑的G（看看其它有没有能跑的协程），拿出来跑。 ​ 所以简单总结一下：只要执行 goexit 这个函数，当前协程就会退出，同时还能调度下一个可执行的协程出来跑。 这就是为什么runtime.Goexit()​能让协程只执行一半就结束了。 goexit的用途我理解是：堆栈底层 “御用” 退出方法 正经人谁会没事跑一半协程就结束呢？所以goexit​的真实用途是啥？ 有个​小细节​，不知道大家平时debug的时候有没有关注过。 ​​ 为了说明问题，这里先给出一段代码。 123456789101112131415package main import ( &quot;fmt&quot; &quot;time&quot;)func Foo() { fmt.Println(&quot;打印1&quot;)} func main() { go Foo() fmt.Println(&quot;打印3&quot;) time.Sleep(1000*time.Second)} 这是一段非常简单的代码，输出什么完全不重要。通过go​关键字启动了一个goroutine​执行Foo()​，里面打印一下就结束，主协程sleep​很长时间，只为​死等​。 这里我们新启动的协程里，在Foo()​函数内随便打个断点。然后debug​一下。 ​​ 会发现，这个协程的堆栈底部是从runtime.goexit()​里开始启动的。 如果大家平时有注意观察，会发现，​其实所有的堆栈底部，都是从这个函数开始的​。我们继续跟跟代码。 goexit是什么？从上面的debug​堆栈里点进去会发现，这是个汇编函数，可以看出调用的是runtime​包内的 goexit1()​ 函数。 1234567// The top-most function running on a goroutine// returns to goexit+PCQuantum.TEXT runtime·goexit(SB),NOSPLIT,$0-0 BYTE $0x90 // NOP CALL runtime·goexit1(SB) // does not return // traceback from goexit1 must hit code range of goexit BYTE $0x90 // NOP 于是跟到了pruntime/proc.go​里的代码中。 1234// 省略部分代码func goexit1() { mcall(goexit0)} 是不是很熟悉，这不就是我们开头讲runtime.Goexit()​里内部执行的goexit0​吗。 为什么每个堆栈底部都是这个方法？因为只要创建协程就有newproc1​这个方法，它获取当前协程G所在的调度器P，然后创建一个新G，并在栈底插入一个goexit 我们首先需要知道的是，函数栈的执行过程，是先进后出。 假设我们有以下代码 1234567891011func main() { B()} func B() { A()} func A() { } 上面的代码是main运行B函数，B函数再运行A函数，代码执行时就跟下面的动图那样。 ​ 这个是先进后出的过程，也就是我们常说的函数栈，执行完子函数A() 后，就会回到父函数B() 中，执行完B()后，最后就会回到main() 。这里的栈底是main()​，如果在栈底插入的是 goexit​ 的话，那么当程序执行结束的时候就都能执行到goexit​里去。 结合前面讲过的内容，我们就能知道，此时栈底的goexit​，会在协程内的业务代码跑完后被执行到，从而实现协程退出，并调度下一个可执行的G来运行。 ——那么问题又来了，栈底插入​&lt;u&gt;*goexit*&lt;/u&gt;​这件事是谁做的，什么时候做的？ 直接说答案，这个在runtime/proc.go​里有个newproc1​方法，只要是创建协程都会用到这个方法（main是主协程也不例外）。 里面有个地方是这么写的： 1234567891011121314151617func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) { // 获取当前g _g_ := getg() // 获取当前g所在的p _p_ := _g_.m.p.ptr() // 创建一个新 goroutine newg := gfget(_p_) // 底部插入goexit newg.sched.pc = funcPC(goexit) + sys.PCQuantum newg.sched.g = guintptr(unsafe.Pointer(newg)) // 把新创建的g放到p中 runqput(_p_, newg, true) // ...} 主要的逻辑是获取当前协程G所在的调度器P，然后创建一个新G，并在栈底插入一个goexit。 所以我们每次debug的时候，就都能看到函数栈底部有个goexit函数。 main函数也是个协程，栈底也是goexit？关于main函数栈底是不是也有个goexit​，我们对下面代码断点看下。直接得出结果。 ​![图片](https://cdn.jsdelivr.net/gh/luommy/myblogimg@img/myblog/202312121420770.png &quot;null&quot;)​ main函数栈底也是goexit()​ 从 asm_amd64.s​可以看到Go程序启动的流程，这里提到的 runtime·mainPC​ 其实就是 runtime.main​. 12345// create a new goroutine to start programMOVQ $runtime·mainPC(SB), AX // 也就是runtime.mainPUSHQ AXPUSHQ $0 // arg sizeCALL runtime·newproc(SB) 通过runtime·newproc​创建runtime.main​协程，然后在runtime.main​里会启动main.main​函数，这个就是我们平时写的那个main函数了。 123456789// runtime/proc.gofunc main() { // 省略大量代码 fn := main_main // 其实就是我们的main函数入口 fn() } //go:linkname main_main main.mainfunc main_main() 结论是，其实main函数也是由newproc创建的，只要通过newproc创建的goroutine，栈底就会有一个goexit。 os.Exit()和runtime.Goexit()有什么区别最后再回到开头的问题，实现一下首尾呼应。 开头的面试题，除了runtime.Goexit()​，是不是还可以改为用os.Exit()​？ 同样都是带有”退出”的含义，两者退出的对象不同。os.Exit()​ 指的是整个进程退出；而runtime.Goexit()​指的是协程退出。 可想而知，改用os.Exit()​ 这种情况下，defer里的内容就不会被执行到了。 123456789101112131415161718192021222324package main import ( &quot;fmt&quot; &quot;os&quot; &quot;time&quot;)func Foo() { fmt.Println(&quot;打印1&quot;) defer fmt.Println(&quot;打印2&quot;) os.Exit(0) fmt.Println(&quot;打印3&quot;)} func main() { go Foo() fmt.Println(&quot;打印4&quot;) time.Sleep(1000*time.Second)} // 输出结果打印4打印1 总结 通过 runtime.Goexit()​可以做到提前结束协程，且结束前还能执行到defer的内容 ​runtime.Goexit()​其实是对goexit0​的封装，只要执行 goexit0​这个函数，当前协程就会退出，同时还能调度下一个可执行的协程出来跑。 通过newproc​可以创建出新的goroutine​，它会在函数栈底部插入一个goexit​ ​os.Exit()​ 指的是整个进程退出；而runtime.Goexit()​指的是协程退出。两者含义有区别！ ‍","link":"/post/gozen-me-rang-xie-cheng-pao-yi-ban-jiu-tui-chu-xipuv.html"},{"title":"高并发设计思考体系","text":"高并发设计思考体系 读了苏三的博客，进一步对高并发设计整体的思考多了许多 。 在原文章上加入一些自己了理解和新的认识。 “ 高并发不会是区别大厂、小厂工程师的标准，却是 检验技术实力的一道关。” 如何设计一个​高并发系统 ？ 瞬间联想秒杀系统？50W QPS？数据库、缓存、消息队列、分布式服务如何演进的？ 这个问题真的可以无限无限难，这个问题也是激发我学习的动力，后续会花一部分时间专研这部分，本文章目的是扩展下高并发设计的解决思路，即总纲。 对于高并发的设计理念有了比较系统的理解与认识，尤其是对于某些架构方面的认识，越来越觉得高并发、微服务、分布式这些获取比较庞大知识体系其实是相互共存的，懂了一方面理解另一方面也会更加容器，也更容易形成知识体系。 本部分与 Go并发编程 直接联系，当然了并发编程不一定局限于某种语言，思想和实践方式都有比较类似之处。 ‍ 关联文章： 并发编程的业务场景 参考文章： 苏三博客 并发设计解决宏图​​ 从前端——后端——运维 每一个步骤和环节都有着处理的方式，眼界瞬间就提升了，不是因为你是后端，局限于后端的部分，或者是前端。 解决思路1 页面静态化对于高并发系统的页面功能，我们必须要做静态化​设计。 如果并发访问系统的用户非常多，每次用户访问页面的时候，都通过服务器动态渲染，会导致服务端承受过大的压力，而导致页面无法正常加载的情况发生。 使用Freemarker​或Velocity​模板引擎，实现页面静态化功能。 以商城官网首页为例，我们可以在Job​中，每隔一段时间，查询出所有需要在首页展示的数据，汇总到一起，使用模板引擎生成到html文件当中。 然后将该html​文件，通过shell​脚本，自动同步到前端页面相关的服务器上。 有一说一这部分是 前端 的部分工作，并不了解，问了几个前端的朋友也没有了解过的，可能大厂的前端才可能用? 不得而知。 2 CDN加速虽说页面静态化可以提升网站网页的访问速度，但还不够，因为用户分布在全国各地，有些人在北京，有些人在成都，有些人在深圳，地域相差很远，他们访问网站的网速各不相同。 如何才能让用户最快访问到活动页面呢？ 这就需要使用CDN，它的全称是Content Delivery Network，即内容分发网络。 ​ 使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。 CDN加速的基本原理是：将网站的静态内容（如图片、CSS、JavaScript文件等）复制并存储到分布在全球各地的服务器节点上。 ——当用户请求访问网站时，CDN系统会根据用户的地理位置，自动将内容分发给离用户最近的服务器，从而实现快速访问。 国内常见的CDN提供商有阿里云CDN、腾讯云CDN、百度云加速等，它们提供了全球分布的节点服务器，为全球范围内的网站加速服务。 这个东西在大学期间就有所耳闻，其实也很好理解。 3 缓存在高并发的系统中，缓存​可以说是必不可少的技术之一。 目前缓存有两种： 基于应用服务器的内存缓存，也就是我们说的二级缓存。 使用缓存中间件，比如：Redis、Memcached等，这种是分布式缓存。 这两种缓存各有优缺点。 二级缓存的性能更好，但因为是基于应用服务器内存的缓存，如果系统部署到了多个服务器节点，可能会存在数据不一致的情况。 而Redis或Memcached虽说性能上比不上二级缓存，但它们是分布式缓存，避免多个服务器节点数据不一致的问题。 缓存的用法一般是这样的：使用缓存之后，可以减轻访问数据库的压力，显著的提升系统的性能。 有些业务场景，甚至会分布式缓存和二级缓存一起使用。 比如获取商品分类数据，流程如下：​ 不过引入缓存，虽说给我们的系统性能带来了提升，但同时也给我们带来了一些新的问题，比如：《数据库和缓存双向数据库一致性问题》、《缓存穿透、击穿和雪崩问题》等。要结合实际业务场景，切记不要为了缓存而缓存。 4 异步有时候，我们在高并发系统当中，某些接口的业务逻辑，没必要都同步执行。 比如有个用户请求接口中，需要做业务操作，发站内通知，和记录操作日志。为了实现起来比较方便，通常我们会将这些逻辑放在接口中同步执行，势必会对接口性能造成一定的影响。 接口内部流程图如下： 这个接口表面上看起来没有问题，但如果你仔细梳理一下业务逻辑，会发现只有业务操作才是核心逻辑，其他的功能都是非核心逻辑。 在这里有个原则就是：核心逻辑可以同步执行，同步写库。非核心逻辑，可以异步执行，异步写库。 上面这个例子中，发站内通知和用户操作日志功能，对实时性要求不高，即使晚点写库，用户无非是晚点收到站内通知，或者运营晚点看到用户操作日志，对业务影响不大，所以完全可以异步处理。 通常异步主要有两种：多线程 和 mq。 这部分内容在项目中经常能用到，比如一个OA系统在同步业务处理的时候需要把业务内容同步给邮箱让用户知晓，但这个同步的过程不是核心业务，也就是晚点通知、早点通知都是无所谓的，不影响系统的功能。 4.1 线程池使用线程池改造之后，接口逻辑如下： 发站内通知和用户操作日志功能，被提交到了两个单独的线程池中。 这样接口中重点关注的是业务操作，把其他的逻辑交给线程异步执行，这样改造之后，让接口性能瞬间提升了。 但使用线程池有个小问题就是：如果服务器重启了，或者是需要被执行的功能出现异常了，无法重试，会丢数据。 那么这个问题该怎么办呢？ =&gt;这个问题：我联想到了幂等性、MQ 、持久化等方面。比较肤浅的思路就是将需要被执行的任务持久化到磁盘或者数据库中，当任务执行失败或服务器重启之后，可以重新读取任务，并进行重试。也许后续会更新…. 4.2 mq使用mq改造之后，接口逻辑如下： 对于发站内通知和用户操作日志功能，在接口中并没真正实现，它只发送了mq消息到mq服务器。然后由mq消费者消费消息时，才真正的执行这两个功能。 这样改造之后，接口性能同样提升了，因为发送mq消息速度是很快的，我们只需关注业务操作的代码即可。 思路非常合理 5 多线程处理在高并发系统当中，用户的请求量很大。 假如我们现在用mq处理业务逻辑。 一下子有大量的用户请求，产生了大量的mq消息，保存到了mq服务器。 而mq的消费者，消费速度很慢。 =》可能会导致大量的消息积压问题。 从而严重影响数据的实时性。 我们需要对消息的消费者做优化。 最快的方式是使用多线程​消费消息，比如：改成线程池消费消息。 当然核心线程数、最大线程数、队列大小 和 线程回收时间，一定要做成配置的，后面可以根据实际情况动态调整。 这样改造之后，我们可以快速解决消息积压问题。 除此之外，在很多数据导入场景，用多线程导入数据，可以提升效率。 温馨提醒一下：使用多线程消费消息，可能会出现消息的顺序问题。如果你的业务场景中，需要保证消息的顺序，则要用其他的方式解决问题。 ‍ 这部分其实就是多线程处理业务了，不但业务开了线程池，同时的消息消费的处理也开线程池来处理 ‍ 6 分库分表有时候，高并发系统的吞吐量受限的不是别的，而是数据库。 当系统发展到一定的阶段，用户并发量大，会有大量的数据库请求，需要占用大量的数据库连接，同时会带来磁盘IO的性能瓶颈问题。 此外，随着用户数量越来越多，产生的数据也越来越多，一张表有可能存不下。由于数据量太大，sql语句查询数据时，即使走了索引也会非常耗时。 这时该怎么办呢？ 答：需要做分库分表​。 如下图所示： 图中将用户库拆分成了三个库，每个库都包含了四张用户表。 如果有用户请求过来的时候，先根据用户id路由到其中一个用户库，然后再定位到某张表。 路由的算法挺多的： 根据id取模，比如：id=7，有4张表，则7%4=3，模为3，路由到用户表3。 给id指定一个区间范围，比如：id的值是0-10万，则数据存在用户表0，id的值是10-20万，则数据存在用户表1。 一致性hash算法 分库分表主要有两个方向：垂直​和水平​。 说实话垂直方向（即业务方向）更简单。 在水平方向（即数据方向）上，分库和分表的作用，其实是有区别的，不能混为一谈。 分库：是为了解决数据库连接资源不足问题，和磁盘IO的性能瓶颈问题。 分表：是为了解决单表数据量太大，sql语句查询数据时，即使走了索引也非常耗时问题。此外还可以解决消耗cpu资源问题。 分库分表：可以解决 数据库连接资源不足、磁盘IO的性能瓶颈、检索数据耗时 和 消耗cpu资源等问题。 如果在有些业务场景中，用户并发量很大，但是需要保存的数据量很少，这时可以只分库，不分表。 如果在有些业务场景中，用户并发量不大，但是需要保存的数量很多，这时可以只分表，不分库。 如果在有些业务场景中，用户并发量大，并且需要保存的数量也很多时，可以分库分表。 关于分库分表更详细的内容，苏三里面讲的更深入《阿里二面：为什么分库分表？》 同时也关联我之前分库分表的思考：微服务下分库分表的思考 7 池化技术其实不光是高并发系统，为了性能考虑，有些低并发的系统，也在使用池化技术​，比如：数据库连接池、线程池等。 池化技术是多例设计模式​的一个体现。 我们都知道创建​和销毁​数据库连接是非常耗时耗资源的操作。 如果每次用户请求，都需要创建一个新的数据库连接，势必会影响程序的性能。 为了提升性能，我们可以创建一批数据库连接，保存到内存中的某个集合中，缓存起来。 这样的话，如果下次有需要用数据库连接的时候，就能直接从集合中获取，不用再额外创建数据库连接，这样处理将会给我们提升系统性能。当然用完之后，需要及时归还。 目前常用的数据库连接池有：Druid、C3P0、hikari和DBCP等。 8 读写分离不知道你有没有听说过二八原则​，在一个系统当中可能有80%是读数据请求，另外20%是写数据请求。 不过这个比例也不是绝对的。 我想告诉大家的是，一般的系统读数据请求会远远大于写数据请求。 如果读数据请求和写数据请求，都访问同一个数据库，可能会相互抢占数据库连接，相互影响。 我们都知道，一个数据库的数据库连接数量是有限，是非常宝贵的资源，不能因为读数据请求，影响到写数据请求吧？ 这就需要对数据库做读写分离​了。 于是，就出现了主从读写分离架构：考虑刚开始用户量还没那么大，选择的是一主一从的架构，也就是常说的一个master​，一个slave​。 所有的写数据请求，都指向主库。一旦主库写完数据之后，立马异步同步给从库。这样所有的读数据请求，就能及时从从库中获取到数据了（除非网络有延迟）。 但这里有个问题就是：如果用户量确实有些大，如果master挂了，升级slave为master，将所有读写请求都指向新master。 但此时，如果这个新master根本扛不住所有的读写请求，该怎么办？ 这就需要一主多从的架构了：上图中我列的是一主两从，如果master挂了，可以选择从库1或从库2中的一个，升级为新master。假如我们在这里升级从库1为新master，则原来的从库2就变成了新master的的slave了。 调整之后的架构图如下：这样就能解决上面的问题了。 除此之外，如果查询请求量再增大，我们还可以将架构升级为一主三从、一主四从…一主N从等。 9 索引在高并发的系统当中，用户经常需要查询数据，对数据库增加索引​，是必不可少的一个环节。 尤其是表中数据非常多时，加了索引，跟没加索引，执行同一条sql语句，查询相同的数据，耗时可能会相差N个数量级。 虽说索引能够提升SQL语句的查询速度，但索引也不是越多越好。 在insert数据时，需要给索引分配额外的资源，对insert的性能有一定的损耗。 我们要根据实际业务场景来决定创建哪些索引，索引少了，影响查询速度，索引多了，影响写入速度。 很多时候，我们需要经常对索引做优化。 可以将多个单个索引，改成一个联合索引。 删除不要索引。 使用explain关键字，查询SQL语句的执行计划，看看哪些走了索引，哪些没有走索引。 要注意索引失效的一些场景。 必要时可以使用force index来强制查询sql走某个索引。 如果你想进一步了解explain的详细用法，可以看看我的另一篇文章《explain | 索引优化的这把绝世好剑，你真的会用吗？》。 如果你想进一步了解哪些情况下索引会失效，可以看看我的另一篇文章《聊聊索引失效的10种场景，太坑了》。 这一部分涉及到了： 索引失效 索引优化 10 批处理有时候，我们需要从指定的用户集合中，查询出有哪些是在数据库中已经存在的。 实现代码可以这样写： 123456789public List&lt;User&gt; queryUser(List&lt;User&gt; searchList) { if (CollectionUtils.isEmpty(searchList)) { return Collections.emptyList(); } List&lt;User&gt; result = Lists.newArrayList(); searchList.forEach(user -&gt; result.add(userMapper.getUserById(user.getId()))); return result;} 这里如果有50个用户，则需要循环50次，去查询数据库。我们都知道，每查询一次数据库，就是一次远程调用。 如果查询50次数据库，就有50次远程调用，这是非常耗时的操作。 那么，我们如何优化呢？ 答：批处理​。 具体代码如下： 1234567public List&lt;User&gt; queryUser(List&lt;User&gt; searchList) { if (CollectionUtils.isEmpty(searchList)) { return Collections.emptyList(); } List&lt;Long&gt; ids = searchList.stream().map(User::getId).collect(Collectors.toList()); return userMapper.getUserByIds(ids);} 提供一个根据用户id集合批量查询​用户的接口，只远程调用一次，就能查询出所有的数据。 这里有个需要注意的地方是：id集合的大小要做限制，最好一次不要请求太多的数据。要根据实际情况而定，建议控制每次请求的记录条数在500以内。 12345678910111213func queryUsers(searchList []User) []User { if len(searchList) == 0 { return []User{} } var ids []int for _, user := range searchList { ids = append(ids, user.ID) } return getUserByIDList(ids)} 这种批处理的方式还是看业务应用场景比较好理解 11 集群系统部署的服务器节点，可能会down机，比如：服务器的磁盘坏了，或者操作系统出现内存不足问题。 为了保证系统的高可用，我们需要部署多个节点，构成一个集群​，防止因为部分服务器节点挂了，导致系统的整个服务不可用的情况发生。 集群有很多种： 应用服务器集群 数据库集群 中间件集群 文件服务器集群 我们以中间件Redis​为例。 在高并发系统中，用户的数据量非常庞大时，比如用户的缓存数据总共大小有40G，一个服务器节点只有16G的内存。 这样需要部署3台服务器节点。 该业务场景，使用普通的master/slave模式，或者使用哨兵模式都行不通。 40G的数据，不能只保存到一台服务器节点，需要均分到3个master服务器节点上，一个master服务器节点保存13.3G的数据。 当有用户请求过来的时候，先经过路由，根据用户的id或者ip，每次都访问指定的服务器节点。这用就构成了一个集群。 但这样有风险，为了防止其中一个master服务器节点挂掉，导致部分用户的缓存访问不了，还需要对数据做备份。 这样每一个master，都需要有一个slave，做数据备份。 如果master挂了，可以将slave升级为新的master，而不影响用户的正常使用。 本质还是结合了主从的思想，这里有个问题，一个节点就代表一个服务器吗？ 12 负载均衡如果我们的系统部署到了多台服务器节点。那么哪些用户的请求，访问节点a，哪些用户的请求，访问节点b，哪些用户的请求，访问节点c？ 我们需要某种机制，将用户的请求，转发到具体的服务器节点上。 这就需要使用负载均衡​机制了。 在linux下有Nginx​、LVS​、Haproxy​等服务可以提供负载均衡服务。 在SpringCloud微服务架构中，大部分使用的负载均衡组件就是Ribbon​、OpenFegin​或SpringCloud Loadbalancer​。 硬件方面，可以使用F5​实现负载均衡。它可以基于交换机实现负载均衡，性能更好，但是价格更贵一些。 常用的负载均衡策略有： ​轮询​：每个请求按时间顺序逐一分配到不同的服务器节点，如果服务器节点down掉，能自动剔除。 ​weight权重​：weight代表权重默认为1，权重越高，服务器节点被分配到的概率越大。weight和访问比率成正比，用于服务器节点性能不均的情况。 ​ip hash​：每个请求按访问ip的hash结果分配, 这样每个访客固定访问同一个服务器节点，它是解诀Session共享的问题的解决方案之一。 ​最少连接数​：把请求转发给连接数较少的服务器节点。轮询算法是把请求平均的转发给各个服务器节点，使它们的负载大致相同；但有些请求占用的时间很长，会导致其所在的服务器节点负载较高。这时least_conn方式就可以达到更好的负载均衡效果。 ​最短响应时间​：按服务器节点的响应时间来分配请求，响应时间短的服务器节点优先被分配。 从划分大类上还可以分为动态负载均衡、静态负载均衡；动态负载均衡考虑服务器当前状态，静态不考虑 13 限流对于高并发系统，为了保证系统的稳定性，需要对用户的请求量做限流​。 特别是秒杀系统中，如果不做任何限制，绝大部分商品可能是被机器抢到，而非正常的用户，有点不太公平。 所以，我们有必要识别这些非法请求，做一些限制。那么，我们该如何现在这些非法请求呢？ 目前有两种常用的限流方式： 基于nginx限流 基于redis限流 13.1 对同一用户限流为了防止某个用户，请求接口次数过于频繁，可以只针对该用户做限制。限制同一个用户id，比如每分钟只能请求5次接口。 13.2 对同一ip限流有时候只对某个用户限流是不够的，有些高手可以模拟多个用户请求，这种nginx就没法识别了。 这时需要加同一ip限流功能。限制同一个ip，比如每分钟只能请求5次接口。 但这种限流方式可能会有误杀的情况，比如同一个公司或网吧的出口ip是相同的，如果里面有多个正常用户同时发起请求，有些用户可能会被限制住。 13.3 对接口限流别以为限制了用户和ip就万事大吉，有些高手甚至可以使用代理，每次都请求都换一个ip。 这时可以限制请求的接口总次数。在高并发场景下，这种限制对于系统的稳定性是非常有必要的。但可能由于有些非法请求次数太多，达到了该接口的请求上限，而影响其他的正常用户访问该接口。看起来有点得不偿失。 13.4 加验证码相对于上面三种方式，加验证码的方式可能更精准一些，同样能限制用户的访问频次，但好处是不会存在误杀的情况。 ​通常情况下，用户在请求之前，需要先输入验证码。用户发起请求之后，服务端会去校验该验证码是否正确。只有正确才允许进行下一步操作，否则直接返回，并且提示验证码错误。 此外，验证码一般是一次性的，同一个验证码只允许使用一次，不允许重复使用。 普通验证码，由于生成的数字或者图案比较简单，可能会被破解。优点是生成速度比较快，缺点是有安全隐患。 还有一个验证码叫做：移动滑块​​，它生成速度比较慢，但比较安全，是目前各大互联网公司的首选。 限流应用很常见 14 服务降级前面已经说过，对于高并发系统，为了保证系统的稳定性，需要做限流。 但光做限流还不够。 我们需要合理利用服务器资源，保留核心的功能，将部分非核心的功能，我们可以选择屏蔽或者下线掉。 我们需要做服务降级​。 我们在设计高并发系统时，可以预留一些服务降级的开关。 比如在秒杀系统中，核心的功能是商品的秒杀，对于商品的评论功能，可以暂时屏蔽掉。 在服务端的分布式配置中心，比如：apollo中，可以增加一个开关，配置是否展示评论功能，默认是true。 前端页面通过服务器的接口，获取到该配置参数。 如果需要暂时屏蔽商品评论功能，可以将apollo中的参数设置成false。 此外，我们在设计高并发系统时，还可以预留一些兜底方案。 比如某个分类查询接口，要从redis中获取分类数据，返回给用户。但如果那一条redis挂了，则查询数据失败。 这时候，我们可以增加一个兜底方案。 如果从redis中获取不到数据，则从apollo中获取一份默认的分类数据。 目前使用较多的熔断降级中间件是：Hystrix​ 和 Sentinel​。 Hystrix是Netflix开源的熔断降级组件。 Sentinel是阿里中间件团队开源的一款不光具有熔断降级功能，同时还支持系统负载保护的组件。 二者的区别如下图所示： 降级确实了解的不多 15 故障转移在高并发的系统当中，同一时间有大量的用户访问系统。 如果某一个应用服务器节点处于假死状态，比如CPU使用率100%了，用户的请求没办法及时处理，导致大量用户出现请求超时的情况。 如果这种情况下，不做任何处理，可能会影响系统中部分用户的正常使用。 这时我们需要建立故障转移​机制。 当检测到经常接口超时，或者CPU打满，或者内存溢出的情况，能够自动重启那台服务器节点上的应用。 在SpringCloud微服务当中，可以使用Ribbon​做负载均衡器。 Ribbon是Spring Cloud中的一个负载均衡器组件，它可以检测服务的可用性，并根据一定规则将请求分发至不同的服务节点。在使用Ribbon时，需要注意以下几个方面： 设置请求超时时间，当请求超时时，Ribbon会自动将请求转发到其他可用的服务上。 设置服务的健康检查，Ribbon会自动检测服务的可用性，并将请求转发至可用的服务上。 此外，还需要使用Hystrix​做熔断处理。 Hystrix是SpringCloud中的一个熔断器组件，它可以自动地监测所有通过它调用的服务，并在服务出现故障时自动切换到备用服务。在使用Hystrix时，需要注意以下几个方面： 设置断路器的阈值，当故障率超过一定阈值后，断路器会自动切换到备用服务上。 设置服务的超时时间，如果服务在指定的时间内无法返回结果，断路器会自动切换到备用服务上。到其他的能够正常使用的服务器节点上。 16 异地多活有些高并发系统，为了保证系统的稳定性，不只部署在一个机房当中。 为了防止机房断电，或者某些不可逆的因素，比如：发生地震，导致机房挂了。 需要把系统部署到多个机房。 我们之前的游戏登录系统，就部署到了深圳、天津和成都，这三个机房。 这三个机房都有用户的流量，其中深圳机房占了40%，天津机房占了30%，成都机房占了30%。 如果其中的某个机房突然挂了，流量会被自动分配到另外两个机房当中，不会影响用户的正常使用。 这就需要使用异地多活​​架构了。 用户请求先经过第三方的DNS服务器解析，然后该用户请求到达路由服务器，部署在云服务器上。 路由服务器，根据一定的算法，会将该用户请求分配到具体的机房。 问题也来了：异地多活的难度是多个机房需要做数据同步，如何保证数据的一致性？ 这个好像是多数据中心的问题，是否可以参考多集群下的数据同步，缓存同步同理。比如，通常一个mysql集群有一主多从构成。用户的数据都是写入主库Master，Master将数据写入到本地二进制日志binary log中。从库Slave启动一个IO线程(I/O Thread)从主从同步binlog，写入到本地的relay log中，同时slave还会启动一个SQL Thread，读取本地的relay log，写入到本地，从而实现数据同步。 17 压测高并发系统，在上线之前，必须要做的一件事是做压力测试​。 我们先要预估一下生产环境的请求量，然后对系统做压力测试，之后评估系统需要部署多少个服务器节点。 比如预估有10000的qps，一个服务器节点最大支持1000pqs，这样我们需要部署10个服务器节点。 但假如只部署10个服务器节点，万一突增了一些新的用户请求，服务器可能会扛不住压力。 因此，部署的服务器节点，需要把预估用户请求量的多一些，比如：按3倍的用户请求量来计算。 这样我们需要部署30个服务器节点。 压力测试的结果跟环境有关，在dev环境或者test环境，只能压测一个大概的趋势。 想要更真实的数据，我们需要在pre环境，或者跟生产环境相同配置的专门的压测环境中，进行压力测试。 目前市面上做压力测试的工具有很多，比如开源的有：Jemter、LoaderRunnder、Locust等等。 收费的有：阿里自研的云压测工具PTS。 18 监控监控系统！ 为了出现系统或者SQL问题时，能够让我们及时发现，我们需要对系统做监控。 目前业界使用比较多的开源监控系统是：Prometheus​。 它提供了 监控​ 和 预警​ 的功能。 架构图如下：​ 我们可以用它监控如下信息： 接口响应时间 调用第三方服务耗时 慢查询sql耗时 cpu使用情况 内存使用情况 磁盘使用情况 数据库使用情况 等等······ 它的界面大概长这样子： 可以看到mysql当前qps，活跃线程数，连接数，缓存池的大小等信息。 如果发现数据量连接池占用太多，对接口的性能肯定会有影响。 这时可能是代码中开启了连接忘了关，或者并发量太大了导致的，需要做进一步排查和系统优化。 截图中只是它一小部分功能，如果你想了解更多功能，可以访问 Prometheus的官网 其实，高并发的系统中，还需要考虑安全问题，比如： 遇到用户不断变化ip刷接口怎办？ 遇到用户大量访问缓存中不存在的数据，导致缓存雪崩怎么办？ 如果用户发起ddos攻击怎么办？ 用户并发量突增，导致服务器扛不住了，如何动态扩容？ promethues+grafana 搭建起来确实可以监测很多方面的数据，服务器运行、数据库运行等等均可以很直观的展现，很利于微服务下的项目管控。 ‍","link":"/post/high-combined-design-thinking-system-11vul.html"},{"title":"My Blog ’s Plan","text":"My Blog ’s Plan 早在之前就有过建立自己的博客，但没有坚持下来。现重操旧业，坚持每天输出一篇Blog 整体思路，标签用来关键词联想与提示，或者自成体系的一套内容，分类为专题系列。 ‍ timeline：时间线，结合思源dailynote记录 标签：主要是一些主题性相关的，可能会很杂 例如：十大排序算法Ten-Sorts​​、杂记​​、英语​​、前沿技术​​ 分类： Golang 重学Go（偏基础重新巩固） Golang门面担当（常用的一些底层或者核心） 并发编程（并发相关） 数据结构与算法（偏入门，但是成体系） 架构设计师（面向考试与架构理解） MySQL Redis MQ ELK Docker 分布式 微服务 计算机底层（操作系统、网络、等一些综合性的理解） 🏳️‍🌈 秉持原则: ​ 拒绝无脑CV，深恶痛绝 CSDN 低劣文章，没头没尾，浪费时间 对于容易搜索的到内容有三点：一是要站在巨人的肩膀上总结和理解；二是确实很重点的部分才值得重复做；三自己的新学到内容，可能很粗浅，没有深刻领会，领会后可删除 将精力放在核心上，而不是排版、工具等无意义的点上 详情： A detailed list ‍ ‍","link":"/post/my-blog-content-planning-and-current-plan-z1tjjzq.html"},{"title":"分布式晋级之路","text":"我的分布式目标 理论 理解分布式的产生意义 分布式CAP、BASE理论 分布式算法 Paxos Raft 一致性Hash ZAB 分布式事务与锁 分布式事务背景与理论 分布式事务解决方案 分布式锁应用 实现分布式锁 分布式服务 API网关 服务注册与发现 分布式监控 负载均衡 容器化和服务 微服务技术栈 Service Mesh 分布式存储 分库分表 读写分离 NoSQL应用 ES应用 分布式消息队列 消息队列应用场景 消费顺序、消息重复消费、消费模式 Kafka高性能 RocketMQ 分布式缓存 ….. 分布式高可用 常见技术手段 限流策略 服务性能指标 监控系统 日志系统 ….. ‍","link":"/post/my-distributed-goals-ktyod.html"},{"title":"微服务下分库分表的思考","text":"微服务下分库分表的思考 分库分表的这个名词再常见不过了，一开始的理解不够，随着对mysql理解的加深。 关于这个问题目前我的整体前置思路是：Mysql的瓶颈在哪里——InnoDB存储引擎——MySQL单表存储的瓶颈以及瓶颈推演与测试——分库分表，内容不一定绝对，但是思路是完整、有迹可循的。 关联文章：Mysql单表存储数据量瓶颈推演（2000W左右） 目录 什么是分库分表？ 为什么需要分库分表？ 如何分库分表？ 什么时候开始考虑分库分表？ 分库分表会导致哪些问题？如何解决？ 分库分表中间件？ PS: 这里有个小插曲，关于分区的问题，这一点我是在架构师考试备考（数据库）中遇到的问题，mysql为什么好像从来没有听说分区的相关内容，经查资料才了解到数据库按理说是支持分区的，所谓的是将一个表按照一定规则水平划分成多个子表，每个子表存储一部分数据。分区是针对单个表的，个人理解上是物理上可能跨磁盘、跨系统，但本质上还是一张逻辑表，主要的目的是提高查询效率和管理大型表的数据，减少索引长度和IO操作等问题。MySQL不支持分区，但是可以通过其他方式来实现分区的功能。比如，可以通过应用程序来实现分区的功能，或者使用其他数据库管理系统，比如Oracle、SQL Server等，它们都支持分区。MySQL不支持分区是由于历史原因、成本问题和性能问题所致。追根揭底还是技术栈范围扩大后，很多概念是宏观的，很难保证一致性。比如说只聊数据库，会聊关系模式、关系代数、-（前面这些都是关系数据库讲的，我只用Redis的话谈什么这些）数据库设计、聊数据库优化技术，这些东西太过宏观，像是“基础”可我本质上还是觉得完全就不是一类东西，只能算是时代的眼泪，曾经的思想参考。 1. 什么是分库分表分库：就是一个数据库分成多个数据库，部署到不同机器。单体架构下就没有分库分表，比较单一，SOA到微服务的发展中演进的 ​​ 分表：就是一个数据库表分成多个表。 ​​ 2.为什么需要分库分表呢？如果业务量剧增，数据库可能会出现性能瓶颈，这时候我们就需要考虑拆分数据库。从这几方面来看： 磁盘存储（很容易想到，这边便硬件层面的因素） 业务量剧增，MySQL单机磁盘容量会撑爆，拆成多个数据库，磁盘使用率大大降低。 并发连接支撑（这个是mysql本身的瓶颈） 我们知道数据库连接是有限的。在高并发的场景下，大量请求访问数据库，MySQL单机是扛不住的！当前非常火的微服务架构出现，就是为了应对高并发。它把订单、用户、商品等不同模块，拆分成多个应用，并且把单个数据库也拆分成多个不同功能模块的数据库（订单库、用户库、商品库），以分担读写压力。 为什么需要分表？ 数据量太大的话，SQL的查询就会变慢。如果一个查询SQL没命中索引，千百万数据量的表可能会拖垮这个数据库。 即使SQL命中了索引，如果表的数据量超过一千万的话，查询也是会明显变慢的。这是因为索引一般是B+树结构，数据千万级别的话，B+树的高度会增高，查询就变慢啦。 Mysql单表存储数据量瓶颈推演（2000W左右） MySQL的B+树的高度怎么计算的呢？ InnoDB存储引擎最小储存单元是页，一页大小就是16k。B+树叶子存的是数据，内部节点存的是键值+指针。索引组织表通过非叶子节点的二分查找法以及指针确定数据在哪个页中，进而再去数据页中找到需要的数据，B+树结构图如下： ​​ 假设B+树的高度为2的话，即有一个根结点和若干个叶子结点。这棵B+树的存放总记录数为=根结点指针数*单个叶子节点记录行数。 如果一行记录的数据大小为1k，那么单个叶子节点可以存的记录数 =16k/1k =16​ 非叶子节点内存放多少指针呢？我们假设主键ID为bigint类型，长度为8字节(面试官问你int类型，一个int就是32位，4字节)，而指针大小在InnoDB源码中设置为6字节，所以就是 8+6=14 ​​字节，16k/14B =16*1024B/14B = 1170​ 因此，一棵高度为2的B+树，能存放1170 * 16=18720​条这样的数据记录。 同理一棵高度为3​的B+树，能存放1170 *1170 *16 =21902400​，大概可以存放两千万左右的记录。B+树高度一般为1-3层，如果B+到了4层，查询的时候会多查磁盘的次数，SQL就会变慢。 因此单表数据量超过千万-&gt;就需要考虑分表啦。 这个地方从两种角度分析：颗粒度不一样，层面也不一样 1.页的细节角度 16K的页内结构 ​​ ​​ 这种角度： 非叶子节点内指向其他页的数量为 x 叶子节点内能容纳的数据行数为 y B+ 数的层数为 z 页的结构，索引也也不例外，都会有 File Header (38 byte)、Page Header (56 Byte)、Infimum + Supermum（26 byte）、File Trailer（8byte）, 再加上页目录，大概 1k 左右。 我们就当做它就是 1K, 那整个页的大小是 16K, 剩下 15k 用于存数据，在索引页中主要记录的是主键与页号，主键我们假设是 Bigint (8 byte), 而页号也是固定的（4Byte）, 那么索引页中的一条数据的大小=8+4=12byte。15K=15*1024B 所以 非叶子节点内指向其他页的数量x ​=15*1024/12≈1280 行。 叶子节点和非叶子节点的结构是一样的，同理，能放数据的空间也是 15k。 但是叶子节点中存放的是真正的行数据，这个影响的因素就会多很多，比如，字段的类型，字段的数量。每行数据占用空间越大，页中所放的行数量就会越少。 这边我们暂时按一条行数据 1k 来算，那一页就能存下 15 条，Y = 15*1024/1000 ≈15。 算到这边了，是不是心里已经有谱了啊。 Total 总数据行数 根据上述的公式，Total =x^(z-1) *y，已知 x=1280，y=15： 假设 B+ 树是两层，那就是 z = 2， Total = （1280 ^1 ）*15 = 19200 假设 B+ 树是三层，那就是 z = 3， Total = （1280 ^2） *15 = 24576000 （约 2.45kw） ​​ 2.磁盘块角度（上述即是）： ​​ ‍ 3. 如何分库分表 水平即行，垂直即列，一横一竖，横即拆行，竖即拆列 3.1 垂直拆分​​ 3.1.1 垂直分库在业务发展初期，业务功能模块比较少，为了快速上线和迭代，往往采用单个数据库来保存数据。数据库架构如下： ​​ 但是随着业务蒸蒸日上，系统功能逐渐完善。这时候，可以按照系统中的不同业务进行拆分，比如拆分成用户库、订单库、积分库、商品库，把它们部署在不同的数据库服务器，这就是垂直分库。 垂直分库，将原来一个单数据库的压力分担到不同的数据库，可以很好应对高并发场景。数据库垂直拆分后的架构如下： ​​ 3.1.2 垂直分表如果一个单表包含了几十列甚至上百列，管理起来很混乱，每次都select *​的话，还占用IO资源。这时候，我们可以将一些不常用的、数据较大或者长度较长的列拆分到另外一张表。 比如一张用户表，它包含user_id、user_name、mobile_no、age、email、nickname、address、user_desc​，如果email、address、user_desc​等字段不常用，我们可以把它拆分到另外一张表，命名为用户详细信息表。这就是垂直分表： ​​ 3.2 水平拆分3.2.1 水平分库水平分库是指，将表的数据量切分到不同的数据库服务器上，每个服务器具有相同的库和表，只是表中的数据集合不一样。它可以有效的缓解单机单库的性能瓶颈和压力。 用户库的水平拆分架构如下： ​​ 3.2.2 水平分表如果一个表的数据量太大，可以按照某种规则（如hash取模、range​），把数据切分到多张表去。 一张订单表，按时间range​拆分如下： ​​ 3.3. 水平分库分表策略分库分表策略一般有几种，使用与不同的场景： range范围 hash取模 range+hash取模混合 3.3.1 range范围range，即范围策略划分表。比如我们可以将表的主键，按照从0~1000万​的划分为一个表，1000~2000万​划分到另外一个表，以此类推。如下图： ​​ 当然，有时候我们也可以按​时间范围来划分，如不同年月的订单放到不同的表，它也是一种range的划分策略。 这种方案的优点： 这种方案有利于扩容，不需要数据迁移。假设数据量增加到5千万，我们只需要水平增加一张表就好啦，之前0~4000万​的数据，不需要迁移。 缺点： 这种方案会有热点问题，因为订单id是一直在增大的，也就是说最近一段时间都是汇聚在一张表里面的。比如最近一个月的订单都在1000万~2000​万之间，平时用户一般都查最近一个月的订单比较多，请求都打到order_1​表啦，这就导致表的数据热点问题。 3.3.2 hash取模hash取模策略：指定的路由key（一般是user_id、订单id作为key）对分表总数进行取模，把数据分散到各个表中。 比如原始订单表信息，我们把它分成4张分表： ​​ 比如id=1，对4取模，就会得到1，就把它放到第1张表，即t_order_0​; id=3，对4取模，就会得到3，就把它放到第3张表，即t_order_2​; 这种方案的优点： hash取模的方式，不会存在明显的热点集中问题。 缺点： 如果一开始按照hash取模分成4个表了，未来某个时候，表数据量又到瓶颈了，需要扩容，这就比较棘手了。比如你从4张表，又扩容成8​张表，那之前id=5​的数据是在（5%4=1​，即第一张表），现在应该放到（5%8=5​，即第5​张表），也就是说历史数据要做迁移了。 3.3.3 range+hash取模混合既然range存在热点数据问题，hash取模扩容迁移数据比较困难，我们可以综合两种方案一起嘛，取之之长，弃之之短。 比较简单的做法就是，在拆分库的时候，我们可以先用range范围方案，比如订单id在04000万的区间，划分为订单库1，id在4000万8000万的数据，划分到订单库2,将来要扩容时，id在8000万~1.2亿的数据，划分到订单库3。然后订单库内，再用hash取模的策略，把不同订单划分到不同的表。 ​​ 4. 什么时候考虑分库分表？前提：能不能切分就不要分，分了会极大的导致系统的复杂性。避免”过度设计”和”过早优化”。 在分库分表之前，不要为分而分，先尽力去做力所能及的事情，例如：升级硬件、升级网络、读写分离、索引优化等等。当数据量达到单表的瓶颈时候，再考虑分库分表。 4.1 什么时候分表？ 如果你的系统处于快速发展时期，如果每天的订单流水都新增几十万，并且，订单表的查询效率明变慢时，就需要规划分库分表了。一般B+树索引高度是2~3层最佳，如果数据量千万级别，可能高度就变4层了，数据量就会明显变慢了。不过业界流传，一般500万数据就要考虑分表了，属于提前考虑，留好空间。 4.2 什么时候分库 这一点我的理解是看业务量，微服务架构下，微服务特别多，或者很多重要的业务要维护高并发高可用的要求，就要分库，比如说重点的业务单体抽取出来单独分库。 业务发展很快，还是多个服务共享一个单体数据库，数据库成为了性能瓶颈，就需要考虑分库了。比如订单、用户等，都可以抽取出来，新搞个应用（其实就是微服务思想），并且拆分数据库（订单库、用户库）。 综合来讲，考虑分库分表的无非以下方面： ① 数据量快速增长，当业务中数据量急速增长时 ② 维护困难时：比如，备份时较为困难，单表太大，备份时需要大量的磁盘IO和网络IO；对一个很大的表进行DDL修改时，MySQL会锁住全表，这个时间会很长，这段时间业务不能访问此表，影响很大。如果使用pt-online-schema-change，使用过程中会创建触发器和影子表，也需要很长的时间。在此操作过程中，都算为风险时间。将数据表拆分，总量减少，有助于降低这个风险；经常访问与更新，就更有可能出现锁等待，将数据切分，用空间换时间，变相降低访问压力。 ③ 业务需求：需要对某些字段垂直拆分，本质上还是因用户增多导致的业务特点发生了变化 举个例子，假如项目一开始设计的用户表如下： 12345id bigint #用户的IDname varchar #用户的名字last_login_time datetime #最近登录时间personal_info text #私人信息..... #其他信息字段 在项目初始阶段，这种设计是满足简单的业务需求的，也方便快速迭代开发。而当业务快速发展时，用户量从10w激增到10亿，用户非常的活跃，每次登录会更新 last_login_name 字段，使得 user 表被不断update，压力很大。 而其他字段：id, name, personal_info 是不变的或很少更新的，此时在业务角度，就要将 last_login_time 拆分出去，新建一个 user_time 表。 personal_info 属性是更新和查询频率较低的，并且text字段占据了太多的空间。这时候，就要对此垂直拆分出 user_ext 表了 ④ 安全性角度：鸡蛋不要放在一个篮子里。在业务层面上垂直切分，将不相关的业务的数据库分隔，因为每个业务的数据量、访问量都不同，不能因为一个业务把数据库搞挂而牵连到其他业务。利用水平切分，当一个数据库出现问题时，不会影响到100%的用户，每个库只承担业务的一部分数据，这样整体的可用性就能提高。 5. 分库分表会导致哪些问题分库分表之后，也会存在一些问题： 事务问题 跨库关联 排序问题 分页问题 分布式ID 数据迁移、扩容问题 5.1 事务问题 分库分表后，假设两个表在不同的数据库，那么本地事务已经无效啦，需要使用分布式事务了。 5.2 跨库关联 跨节点Join的问题：解决这一问题可以分两次查询实现； 冗余处理，反规范化设计： 增加冗余列(复制某一列的数据) 这一点是真的好用 增加派生列(计算总和，平均值..) 表合并(把部分来自不同表的常用列合并成新表) 表分割(把数据拆分为常用和不常用，行拆分是比如订单信息，列拆分比如账户信息&lt;额外的住址之类的并不常用，减少查询压力&gt;) 题中要求是商品信息冗余，所以应该采用&lt;增加冗余列&gt;的方法 5.3 排序问题 跨节点的count,order by,group by以及聚合函数等问题：可以分别在各个节点上得到结果后在应用程序端进行合并。 5.4 分页问题 方案1：在个节点查到对应结果后，在代码端汇聚再分页。 方案2：把分页交给前端，前端传来pageSize和pageNo，在各个数据库节点都执行分页，然后汇聚总数量前端。这样缺点就是会造成空查，如果分页需要排序，也不好搞。 5.5 分布式ID 据库被切分后，不能再依赖数据库自身的主键生成机制啦，最简单可以考虑UUID，或者使用雪花算法生成分布式ID。 UUID: UUID标准形式包含32个16进制数字，分为5段，形式为8-4-4-4-12的32个字符，例如：550e8400-e29b-41d4-a716-446655440000 UUID是主键是最简单的方案，本地生成，性能高，没有网络耗时。但缺点也很明显，由于UUID非常长，会占用大量的存储空间；另外，作为主键建立索引和基于索引进行查询时都会存在性能问题，在InnoDB下，UUID的无序性会引起数据位置频繁变动，导致分页。 雪花算法： Twitter的snowflake算法解决了分布式系统生成全局ID的需求，生成64位的Long型数字，组成部分： 第一位未使用 接下来41位是毫秒级时间戳，41位的长度可以表示69年的时间 5位datacenterId，5位workerId。10位的长度最多支持部署1024个节点 最后12位是毫秒内的计数，12位的计数顺序号支持每个节点每毫秒产生4096个ID序列 ​​ 好处：毫秒数在高位，生成的ID整体上按时间趋势递增；不依赖第三方系统，稳定性和效率较高，理论上QPS约为409.6w/s（1000*2^12），并且整个分布式系统内不会产生ID碰撞；可根据自身业务灵活分配bit位。 不足：强依赖机器时钟，如果时钟回拨，则可能导致生成ID重复。 综上，结合数据库和snowflake的唯一ID方案，可以参考业界较为成熟的解法：Leaf——美团点评分布式ID生成系统，并考虑到了高可用、容灾、分布式下时钟等问题。 数据迁移、扩容问题： 当业务高速发展，面临性能和存储的瓶颈时，才会考虑分片设计，此时就不可避免的需要考虑历史数据迁移的问题。一般做法是先读出历史数据，然后按指定的分片规则再将数据写入到各个分片节点中。 此外还需要根据当前的数据量和QPS，以及业务发展的速度，进行容量规划，推算出大概需要多少分片（一般建议单个分片上的单表数据量不超过1000W）。 如果采用数值范围分片，只需要添加节点就可以进行扩容了，不需要对分片数据迁移。如果采用的是数值取模分片，则考虑后期的扩容问题就相对比较麻烦。 6. 分库分表中间件目前流行的分库分表中间件比较多： cobar Mycat Sharding-JDBC（当当） Atlas TDDL（淘宝） vitess（谷歌开发的数据库中间件） ​ ​ 参考资料：如何分库分表！ ‍","link":"/post/my-thinking-about-the-mysql-database-table-zmjl3t.html"},{"title":"Nacos相关记录","text":"Nacos相关记录 Nacos1.X与2.X有差异，目前基本使用2.X版本，也是推荐的版本 Nacos初次尝试…. ‍ ‍ 权限认证🔒开启权限认证： 注意 Nacos是一个内部微服务组件，需要在可信的内部网络中运行，不可暴露在公网环境，防止带来安全风险。 Nacos提供简单的鉴权实现，为防止业务错用的弱鉴权体系，不是防止恶意攻击的强鉴权体系。 如果运行在不可信的网络环境或者有强鉴权诉求，请参考官方简单实现做进行自定义插件开发。 修改nacos配置文件 ——这个时候再访问nacos控制台页面，则会直接报错。 因此，还需要再设置两个属性（数值可随便填） 12nacos.core.auth.server.identity.key=authKeynacos.core.auth.server.identity.value=nacosSecurty 这两个属性是auth的白名单，用于标识来自其他服务器的请求。 添加好这两个属性时页面就能正常访问了。 还需要再其他服务的配置文件中加上如下配置，这也就是服务注册的权限 （修改代码方式）注意：密码不要有特殊符号不然会报错 12spring.cloud.nacos.username=nacospring.cloud.nacos.password=nacos 🤡 此外还需要配置： NACOS_AUTH_TOKEN token 默认:SecretKey012345678901234567890123456789012345678901234567890123456789 ‍","link":"/post/nacos-configuration-and-use-zm1qbo.html"},{"title":"重学Go语言 | 如何在Go中使用Context","text":"重学Go语言 | 如何在Go中使用Context我们知道在开发Go应用程序时，尤其是网络应用程序，需要启动大量的Goroutine​来处理请求： ​​ 不过Goroutine​被创建之后，除非执行后正常退出或者触发panic​退出，Go并没有提供在一个Goroutine​中关闭另一个Goroutine​的机制。 有没有一种方法，可以从一个Goroutine​中通知另一个Goroutine​退出执行呢？这时候就该Context​登场了！ 什么是Context？​Context​，中文叫做上下文​，Go语言在1.7​版本中新增的context​包中定义了Context​，Context​本质是一个接口，这个接口一共定义了四个方法： 123456type Context interface { Deadline() (deadline time.Time, ok bool) Done() &lt;-chan struct{} Err() error Value(key any) any} ​Dateline()​：获取定时关闭的时间。 ​Done()​：从一个channel​获取关闭的信号 ​Err()​：获取错误信息。 ​Value()​：根据key​从Context​取值 Context的作用为什么要使用Context​？或者说设计Context​的目的是什么？ 想像一下这样的场景，当用户开启浏览器访问我们的Web​服务时，我们可能会开启多个Goroutine​来处理用户的请求，这些Goroutine​需要读取不同的资源，最终返回给用户，但如果用户在我们的Web服务还没处理完成就关闭浏览器，断开连接，而此时Web不知道用户已经关闭请求，仍然在处理并返回最终并不会被接收的数据。 ​Context​设计的目的就是可以从上游的Goroutine​发送信息给下游的Goroutine​，回到处理用户请求的场景，当处理请求的Goroutine​发现用户断开连接时，通过Context​发送停止执行的信息，而下游的Goroutine​得到停止信号时就返回，避免资源的浪费。 概括起来，Context​的作用主要体现在两个方面： 在Goroutine​之间传递关闭信息，定时关闭，超时关闭，手动关闭。 在Goroutine​之间传递数据。 Context的使用下面我们来讲讲Context​的基本使用。 创建Context任何上下文都是从一个空白的Context​开始的，创建一个空白的Context​有两种方式： 使用context.Background()​： 1ctx := context.Backgroud() 使用contenxt.TODO()​: 1ctx := context.TODO() 当然大部分时候，我们不需要自己创建一个空白的Context​，比如在处理HTTP​请求时： 123http.HandleFunc(&quot;/hello&quot;, func(w http.ResponseWriter, r *http.Request) { ctx := r.Context()}) 上面的代码中，可以从http.Request​中获取Context​实例，而http.Request​的实际上也是调用context.Backgroud()​: 1234567//net/http/request.gofunc (r *Request) Context() context.Context { if r.ctx != nil { return r.ctx } return context.Background()} 实例讲解为了讲解Context​是如何在Goroutine​之间传递信号与数据，我们通过下面的案例进行说明： 这段代码是一个读取文件内容的函数ReadFile，它接受一个上下文对象ctx、文件名fileName和一个通道result用于返回读取到的文件内容。首先，通过os.Open函数打开文件，并在出现错误时返回。然后，创建一个空的totalResult切片，用于存储整个文件的内容。接下来，进入一个无限循环，在循环中使用select语句监听上下文的完成事件。如果上下文被取消或超时，将空切片[]byte{}发送到result通道，并返回函数。如果上下文没有完成，则继续执行循环体。在每次循环中，使用file.Read函数读取1024字节的数据到切片b中，并检查是否遇到了文件结束（EOF）错误。如果是，则将切片b的内容追加到totalResult切片中，并跳出循环。如果没有遇到文件结束错误，将切片b的内容追加到totalResult切片中，然后继续循环。当循环结束后，将完整的totalResult切片发送到result通道中，函数执行完毕。 1234567891011121314151617181920212223242526func ReadFile(ctx context.Context, fileName string, result chan&lt;- []byte) { file, err := os.Open(fileName) if err != nil { return } totalResult := make([]byte, 0) for { select { case &lt;-ctx.Done(): result &lt;- []byte{} return // default分支为空，这是为了确保在没有收到上下文完成事件时，循环不会阻塞在select语句上 default: } b := make([]byte, 1024) //每次循环读取1024字节的数据到切片b中 _, err := file.Read(b) if err == io.EOF { totalResult = append(totalResult, b...) break } totalResult = append(totalResult, b...) } result &lt;- totalResult} 在上面的程序中，我们调用Context​的Done()​方法，该方法会返回一个Channel​，而使用select​语句则可以让我们在处理业务的同时，监听上游Goroutine​是否有传递取消执行的信息。 手动取消：WithCancel设置Context传递停止信号空白的Context​并不能发挥什么作用，要达到手动取消执行的目的，需要调用context​包下的WithCancel​函数进行封装，封装返回一个新的context​以及一个取消的句柄cancel​函数： 12ctx := context.Background()ctx, cancel := context.WithCancel(ctx) 下面是完整的使用方法：将context对象传入到协程函数中，手动调用cancel进行取消 1234567891011121314151617181920package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;io&quot; &quot;os&quot; &quot;time&quot;)func main() { ctx := context.Background() ctx, cancel := context.WithCancel(ctx) result := make(chan []byte) // 100ms后此协程被手动cancel取消 go ReadFile(ctx, &quot;./test.tar&quot;, result) time.Sleep(100 * time.Millisecond) cancel() fmt.Println(&lt;-result)} 在上面的程序中，我们创建一个Context​之后，传给了ReadFile​函数，并且在暂停100毫秒后调用cancel()​函数，达到手动取消另一个Goroutine​的目的。 截止时间：WithDeadline给Context设置一个截止时间除了手动取消，也可以调用context​包下的WithDeadline()​函数给Context​加一个截止时间，这样在某个时间点，Context​会自动发出取消信号： 12afterTime := time.Now().Add(30 * time.Millisecond)ctx, cancel := context.WithDeadline(context.Background(), afterTime) 下面是完整的示例： 123456789101112131415161718package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;io&quot; &quot;os&quot; &quot;time&quot;)func main() { afterTime := time.Now().Add(30 * time.Millisecond) ctx, cancel := context.WithDeadline(context.Background(), afterTime) defer cancel() result := make(chan []byte) go ReadFile(ctx, &quot;./test.tar&quot;, result) fmt.Println(&lt;-result)} 超时取消：WithTimeout给Context一个超时时间调用context​包下的WithTimeout()​函数可以为Context​加一个超时限制，这对于我们编写超时控制程序非常有帮助： 1ctx, cancel := context.WithTimeout(context.Background(), 10*time.Millisecond) 下面是完整的调用程序： 1234567891011121314151617package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;io&quot; &quot;os&quot; &quot;time&quot;)func main() { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Millisecond) defer cancel() result := make(chan []byte) go ReadFile(ctx, &quot;./test.tar&quot;, result) fmt.Println(&lt;-result)} 传递数据：使用Context传递数据调用context​包下的WithValue()​函数可以生成一个携带数据的Context​，这个机制方便我们跟踪一个处理流程中的Goroutine​： 1 ctx, cancel := context.WithValue(context.Background(), &quot;testKey&quot;,&quot;testValue&quot;) 下游的Goroutine​就可以通过Context​的Value()​函数来获取上游传递下来的值了。 123func MyGoroutine(ctx context.Context){ ctx.Value(&quot;testKey&quot;)} 思考：这里联想下业务场景 使用Context的几点建议 ​Context​不要放在结构体中，需要以参数方式传递 ​Context​作为函数参数时，一般放在第一位，作为函数的第一个参数 使用 context.Background​函数生成根节点的Context​ ​Context ​要传值必要的值，不要什么都传 ​Context​ 是多协程安全的，可以在多个协程中使用 小结至此，应该对Context​有所了解了吧，总的来说，通过Context​可以做到： WithCancel手动控制下游Goroutine​取消执行。 WithDeadline定时控制下游Goroutine​取消执行。 WithTimeout超时控制下游Goroutine​取消执行。 WithValue传递数据给下游的Goroutine​。","link":"/post/re-learning-go-language-how-to-use-context-in-go-ckuaa.html"},{"title":"重学Go语言 | Map","text":"重学Go语言 | Map Go重学系列不追求底层和深度，只求重温下没有使用过或者使用过的一些操作，要注意的细节，以及彻底掌握的一种前提思路。后续讲重学与底层部分练习在一起进行双链操作。 重学篇在自己重温Go的同时，也希望努力做最好的初学教程 本篇思路：什么是map、map的格式与数据类型限制、map的特征、如何创建与初始化等 Map简述123Go语言中的map(映射、字典、哈希表)是一种内置的数据结构，它是一个无序的key-value对的集合，比如以身份证号作为唯一键来标识一个人的信息。Go语言中并没有提供一个set类型，但是map中的key也是不相同的，可以用map实现类似set的功能。 从表面上看map大致是这样的： ​​ ​​ 从底层看： ​map​是一个无序的键值对(key-value​)集合，其底层数据结构是一个哈希表，通过哈希函数，将key​转换为对哈希表中的索引，将value​存储到索引对应的位置，在map​中查找、删除、查找value的时间复杂度O(1)​。 ​​ map格式与数据类型限制关键点：key数据类型限制 map的value可以是Go支持的任意数据类型，而key则所有限制： key的数据类型必须是可以使用​ = ​和​ != ​进行比较 所以，key不能是函数、切片、map，因此这些数据类型不能进行比较，另外，而数组和结构体则可以作为map的key，不过，如果数组的元素包含函数、切片、map，则数组不能作为map的key，结构体的字段如果有以上三者，也同样不能作为map的key。 12345678910111213141516type Test struct { ID string Name string}//正确m := map[Test]int{}type Test struct { ID string Name string scores []int}//报错m := map[Test]int{} key的值必须是唯一，同一个map中不能相同的两个key 12345m := map[string]string{ &quot;name&quot;:&quot;小明&quot;, &quot;age&quot;:&quot;24&quot;, &quot;name&quot;:&quot;小墨&quot;//报错，不能有相同的key} 123//在Go语言中，用 map[KeyType]ValueType 表示一个map，//其中，KeyType 表示 key 的数据类型，ValueType 表示 value 的数据类型：map[keyType]valueType 12345// ※ 重点 ※ // 在一个 map 里所有的键都是唯一的，而且必须是支持 == 和 != 操作符的类型，// 切片、函数以及包含切片的结构类型这些类型由于具有引用语义，不能作为映射的键，// 使用这些类型会造成编译错误：dict := map[ []string ]int{} //err, invalid map key type []string —✩ ✰ ✪ ✫— map值可以是任意类型，没有限制。map里所有键的数据类型必须是相同的，值也必须如此，但键和值的数据类型可以不相同。 注意：​map是无序的，我们无法决定它的返回顺序，所以，每次打印结果的顺序有可能不同。 –&gt;这个地方就有深度了，为什么map遍历是无序的？ Java中的set也是如此吗？为什么？原理是否一样？ map的特征推导总结： map是无序的 map的key是唯一的 map是引用数据类型，因此在使用前必须初始化 函数，切片，map等数据类型不能作为map的key。key必须是可比较的类型，可以是结构体和数组，不能是切片、函数、map，且结构体和数组中也不能包含以上三者中的任何 创建及初始化Map很多教程都是直接告诉你如何初始化，可能只教一种实现方式，不教所以然，这样很片面 未经初始化由于map​是引用数据类型，其底层引用一个哈希表，因此未经初始化(即未分配到内存空间)的map​无法直接使用： 1234var m map[string]intm[&quot;a&quot;] = 1 //未初始化，报错 初始化方式初始化map有两种方式： 使用make函数 字面量初始化 刘丹冰：New与Make 理论上New也是可行的，但是十分不推荐 make函数内置函数make可以为map​类型的变量分配内存： 123m := make(map[string]string)m[&quot;name&quot;] = &quot;test&quot;m[&quot;age&quot;] = &quot;12&quot; 此外还可以指定容量初始化： 这种方式有好处也有劣势，好处是省去了频繁地扩容，坏处是如果一次性建立很大的容量但实际上并不需要会造成资源浪费 12m := make(map[int]string, 10) //第2个参数指定容量“10”fmt.Println(m) //map[] 这种方法指定了map的初始创建容量。与slice类似，后期在使用过程中，map可以自动扩容。 只不过map更方便一些，不用借助类似append的函数，直接赋值即可。如，m[17] = “Nami”。赋值过程中，key如果与已有map中key重复，会将原有map中key对应的value覆盖。但是！对于map而言，可以使用len()函数，但不能使用 cap() 函数。 map是一种可以动态增长的数据结构，但由于其底层实现的复杂性，能直接获取其容量，只能使用len()​函数来获取其元素数量。 字面量初始化 初始化同时赋值 通过字面量初始化map​时，可以给map​中的key​和value​赋初始值： 1234567891011// 这样直接指定初值，要保证key不重复。// 注意！这是常用的方式，使用“：”自动推导的方式m := map[string]string{ &quot;name&quot;:&quot;test&quot;, &quot;age&quot;:&quot;12&quot;,}// 当然了，还有一种不常用的方式：// 定义的同时完成初始化，但有一说一这种方式太不简洁了var m1 map[int]string = map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;} fmt.Println(m1) //map[1:Luffy 2:Sanji] 只初始化，不赋值 常用： 如果不想给map初始化数据，也可以声明一个空的map类型，自动分配了内存： 12// 这里的`m`是一个字符串键和字符串值类型的map，但你也可以根据需要更改其键和值类型。m := map[string]string{} 注意，空的map已经初始化好了，只是没有存入值而已，而直接声明一个map变量时，该map变量为nil，这两者不一样 不常用： 声明一个未初始化的map的方式如下： 1var m map[string]string 这里的m​是一个字符串键和字符串值类型的map变量，但它没有被初始化，因此不能直接使用，如果你尝试在其上执行读或写操作，会触发panic错误。要使用未初始化的map，还需要使用make函数对其进行初始化，如下所示： 12// var这种方式也不常用，因为还要再写这一行完成内存分配才能使用m = make(map[string]string) 这样，你的map就可以安全地使用了。 非要用new初始化123456789101112131415161718192021package mainimport &quot;fmt&quot;func main() { // 使用 new 创建 map myMap := new(map[string]int) // 初始化 map *myMap = make(map[string]int) // 向 map 中添加键值对 (*myMap)[&quot;apple&quot;] = 1 (*myMap)[&quot;banana&quot;] = 2 (*myMap)[&quot;orange&quot;] = 3 // 遍历 map 并打印键值对 for key, value := range *myMap { fmt.Println(key, &quot;:&quot;, value) }} ‍ 常用操作赋值与访问赋值赋值上来说：有直接初始化的同时直接赋值，也有先初始化，再赋值这两种 12345678910m1 := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;}m1[1] = &quot;Nami&quot; //修改m1[3] = &quot;Zoro&quot; //追加， go底层会自动为map分配空间fmt.Println(m1) //map[1:Nami 2:Sanji 3:Zoro]m2 := make(map[int]string, 10) //创建map，指定容量m2[0] = &quot;aaa&quot;m2[1] = &quot;bbb&quot;fmt.Println(m2) //map[0:aaa 1:bbb]fmt.Println(m2[0], m2[1]) //aaa bbb 访问通过key，可以访问map变量中的value： 1234567rank : = map[string]int{ &quot;PHP&quot;:90, &quot;Go&quot;:99 &quot;Java&quot;:95}fmt.Println(rank[&quot;PHP&quot;]) 如果对应的key不存在，则会返回对应value数据类型的空值，比如value为string，则返回空字符串： 123456789//因为value为intfmt.Println(rank[&quot;Python&quot;]) // 输出：0m := map[string]string{&quot;name&quot;:&quot;小张&quot;}//由于value为string类型fmt.Println(m[&quot;age&quot;]) //输出空字符串 判断key是否存在如果我们想在通过key访问map之前就确定对应的key是否存在，有另外一种写法： 1v, ok := m[k] 上面的表达式中，有第二个返回值ok​，该值为boolean类型，当key存在时，ok​的值为true，v​为对应的value​；否则为ok​为false​,v​为空值。 12345678910// 方法一v,ok := rank[k]if ok { fmt.Println(v)}// 方法二，这种更常见if v,ok := rank[k];ok{ fmt.Println(v)} 有时候可能需要知道对应的元素是否真的是在map之中。可以使用下标语法判断某个key是否存在。map的下标语法将产生两个值，其中第二个是一个布尔值，用于报告元素是否真的存在。如果key存在，第一个返回值返回value的值。第二个返回值为 true。 12345678910// 方法一：v,ok := rank[k]if ok { fmt.Println(v)}// 方法二：这种可能更常见if v,ok := rank[k];ok{ fmt.Println(v)} 遍历使用for-range​语句可以遍历map​，获得map​的key​和value​： 1234567m := map[string]string{ &quot;name&quot;:&quot;xiaoming&quot;, &quot;age&quot;:&quot;18岁&quot;}for k,v := range m{ fmt.Println(k,v)} 注意： Map的迭代顺序是不确定的，并且不同的[哈希函数]实现可能导致不同的遍历顺序。在实践中，遍历的顺序是随机的，每一次遍历的顺序都不相同。这是故意的，每次都使用随机的遍历顺序可以强制要求程序不会依赖具体的哈希函数实现。 1234567891011121314m1 := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;}//遍历1，第一个返回值是key，第二个返回值是valuefor k, v := range m1 { fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, v)//1 ----&gt; Luffy//2 ----&gt; yoyo }//遍历2，第一个返回值是key，第二个返回值是value（可省略）for k := range m1 { fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, m1[k])//1 ----&gt; Luffy//2 ----&gt; Sanji } 删除要删除map的value，可以使用Go内置的delete​函数，该函数格式如下： 1func delete(m map[KeyType]ValueType, key Type) 该函数的第一个参数是我们要操作的map类型的变量，第二个参数表示要删除哪个key： 12345678m := map[string]int{ &quot;a&quot;:1, &quot;b&quot;:2}fmt.Println(m)delete(m,&quot;a&quot;)fmt.Println(m) 使用delete()函数，指定key值可以方便的删除map中的k-v映射。 12345678910111213141516m1 := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;, 3: &quot;Zoro&quot;}for k, v := range m1 { //遍历，第一个返回值是key，第二个返回值是value fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, v) }//1 ----&gt; Sanji//2 ----&gt; Sanji//3 ----&gt; Zorodelete(m1, 2) //删除key值为2的mapfor k, v := range m1 { fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, v) }//1 ----&gt; Luffy//3 ----&gt; Zoro 123456789// 使用delete删除一个不存在的keydelete(m1, 5) //删除key值为5的mapfor k, v := range m1 { fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, v) }//1 ----&gt; Luffy//3 ----&gt; Zoro Map输出结果依然是原来的样子，且不会有任何错误提示。 delete()操作是安全的，即使元素不在map中也没有关系；如果查找删除失败将返回value类型对应的零值。 获取长度map中无法获取容量，也就是无法使用cap函数，它的容量是动态变化的，内部是通过哈希表实现的，底层实现与数组、切片不同；同时，他们的扩容机制【slice与map】有何不同？ 要获取map的长度，同样是用内置的len​函数： 1234567var user = map[string]string{ &quot;id&quot;: &quot;0001&quot;, &quot;name&quot;: &quot;小张&quot;, &quot;age&quot;: &quot;18岁&quot;,}fmt.Println(len(user)) //输出：3 Map嵌套由于map​的value​并没有数据类型的限制，所以value​也可以是另一个map​类型： 理论上map嵌套map可以一直嵌套下去，但一般不会这么做的。很丑！ 123456mm := map[string]map[int]string{ &quot;a&quot;: {1: &quot;test1&quot;}, &quot;b&quot;: {2: &quot;test2&quot;}, &quot;c&quot;: {2: &quot;test3&quot;},}fmt.Println(mm) Map做函数参数与slice 相似，在函数间传递映射并不会制造出该映射的一个副本，不是[值传递]，而是引用传递 12345678910111213141516171819func DeleteMap(m map[int]string, key int) { delete(m, key) //删除key值为2的map for k, v := range m { fmt.Printf(&quot;len(m)=%d, %d ----&gt; %s\\n&quot;, len(m), k, v)}//len(m)=2, 1 ----&gt; Luffy//len(m)=2, 3 ----&gt; Zoro}func main() { m := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;, 3: &quot;Zoro&quot;} DeleteMap(m, 2) //删除key值为2的map for k, v := range m { fmt.Printf(&quot;len(m)=%d, %d ----&gt; %s\\n&quot;, len(m), k, v)}//len(m)=2, 1 ----&gt; Luffy//len(m)=2, 3 ----&gt; Zoro} Map做函数返回值返回的依然是引用 123456789101112131415func test() map[int]string {// m1 := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;, 3: &quot;Zoro&quot;} m1 := make(map[int]string, 1) // 创建一个初创容量为1的map m1[1] = &quot;Luffy&quot; m1[2] = &quot;Sanji&quot; // 自动扩容 m1[67] = &quot;Zoro&quot; m1[2] = &quot;Nami&quot; // 覆盖 key值为2 的map fmt.Println(&quot;m1 = &quot;, m1) return m1}func main() { m2 := test() // 返回值 —— 传引用 fmt.Println(&quot;m2 = &quot;, m2)} 输出： 12m1 = map[1:Luffy 2:Nami 67:Zoro]m2 = map[2:Nami 67:Zoro 1:Luffy] Map的排序​map​是无序的，所以每次遍历map​输出的顺序都不一定相同 1234567891011var user = map[string]string{ &quot;id&quot;: &quot;0001&quot;, &quot;name&quot;: &quot;小张&quot;, &quot;age&quot;: &quot;18岁&quot;,}for k, v := range user { fmt.Println(k, &quot;:&quot;, v)}for k, v := range user { fmt.Println(k, &quot;:&quot;,v)} 要有序地遍历一个map类型的变量，可以这么做： 利用切片将map中的key固定排序，这样就稳定了顺序 1234567891011121314order := []string{}for k, _ := range user { order = append(order, k)}// 对user中的key进行升序排序// sort.Strings(order)for _, v := range order { fmt.Println(user[v])}for _, v := range order { fmt.Println(user[v])} Map之间无法比较，只能与nil比较​map​类型变量之间不能进行比较，map​只能与nil​进行比较： 12345678910111213var m map[int]string//判断是否等于nilif m == nil{ fmt.Println(&quot;m hasn't been initialized&quot;)}m1 := map[string]string{&quot;name&quot;: &quot;小明&quot;}m2 := map[string]string{&quot;name&quot;: &quot;小明&quot;}//报错if m1 == m2 { fmt.Println(&quot;相等&quot;)} 这个地方我看到有的教程说：“nil map 和空 map 是相等的，只是 nil map 不能添加元素。”这个map之间不能被比较何来相等一说？对此有疑问 不能对Map的value进行取址操作取址操作作用： 当对数组和切片进行取址操作时，我们可以实现参数传递、修改元素和实现数据结构等功能。以下是一些示例： 参数传递： 12345678910111213package mainimport &quot;fmt&quot;func modifyElement(arr *[3]int) { (*arr)[0] = 100}func main() { array := [3]int{1, 2, 3} modifyElement(&amp;array) fmt.Println(array) // 输出 [100 2 3]} 在这个例子中，通过对数组元素取址，将数组作为指针传递给函数modifyElement​，在函数内部可以直接修改数组元素的值。 修改元素： 1234567891011package mainimport &quot;fmt&quot;func main() { slice := []int{1, 2, 3} fmt.Println(slice) // 输出 [1 2 3] slicePtr := &amp;slice (*slicePtr)[0] = 100 fmt.Println(slice) // 输出 [100 2 3]} 在这个例子中，通过对切片元素取址，可以直接修改切片中的元素值。 实现数据结构： 1234567891011121314151617package mainimport &quot;fmt&quot;type Node struct { Value int Next *Node}func main() { node1 := Node{Value: 1} node2 := Node{Value: 2} node1.Next = &amp;node2 fmt.Println(node1.Value) // 输出 1 fmt.Println(node1.Next.Value) // 输出 2} 在这个例子中，我们通过对结构体中的指针字段进行取址操作，实现了链表数据结构。 这些示例展示了如何使用数组和切片的取址操作来实现参数传递、修改元素和实现数据结构，从而充分展示了取址操作的用处。 Go的数组和切片允许对元素进行取址操作，但不允许对map的元素进行取址操作： 12345678910111213//对数组元素取址a := [3]int{1, 2, 3}fmt.Println(&amp;a[1])//对切片元素取址s := []int{1, 2, 3}fmt.Println(&amp;s[1])//对map元素取址，错误m := map[string]string{ &quot;test&quot;:&quot;test&quot;,}fmt.Println(&amp;m[&quot;test&quot;]) 为什么Go允许数组、切片进行取址操作，但要限制对map的元素取址呢？ 因为Go可以在添加新的键值对时更改键值对的内存位置。Go将在后台执行此操作，以将检索键值对的复杂性保持在恒定水平。因此，地址可能会变得无效，Go宁愿禁止访问一个可能无效的地址。 对map不允许直接进行取址操作的主要原因是为了避免潜在的数据竞争和安全问题。 在Go语言中，map是通过哈希表实现的，它的内部结构相对复杂。当我们对map进行取址操作时，实际上是获取了一个指向底层哈希表的指针。这样一来，如果在获取指针后对map进行了添加、删除或重新分配内存等操作，会导致指针指向的地址发生变化，从而可能使之前获取到的指针失效或指向无效的内存区域。这就会造成潜在的数据不一致性和安全隐患。 为了避免这种情况，Go语言禁止对map进行取址操作，并且在对map进行增删改等操作时，可能会触发内部的扩容和重新散列等操作，从而导致map的底层数据结构发生变化。如果允许对map进行取址操作，那么在并发访问的情况下，就可能引发竞态条件，导致数据不一致或安全问题。 相反，对数组和切片进行取址操作是允许的。这是因为数组和切片的内部结构相对简单，取址操作不会引发底层数据结构的变化。同时，数组和切片的元素是可以被直接修改的，因此可以对它们进行取址操作来实现参数传递、修改元素和实现数据结构等功能。在使用数组和切片的取址操作时，仍然需要注意并发安全和正确性问题，但相对于map，由于其内部结构的简单性，风险较小。 总而言之，在Go语言中，不允许对map进行取址操作是为了避免数据竞争和安全问题，而数组和切片则具备相对简单的内部结构和更明确的使用方式，因此允许进行取址操作。 补充 本部分有待商榷 什么时候需要用指针封装map在 Go 中，大部分情况下，不需要对 map 进行指针封装。因为 map 本身是引用类型，在函数传递时传递的是指向底层数据结构的指针。这意味着在函数间传递 map 时，传递的是其引用而不是值的拷贝，因此可以直接对 map 进行操作而无需额外封装为指针。 然而，有些情况下可能需要对 map 进行指针封装： 1. 需要在多个函数中修改同一个 map如果需要在多个函数中修改同一个 map 并且希望这些修改对所有函数都生效，可以使用指针封装。因为 map 是引用类型，传递指向 map 的指针可以确保在不同函数间共享相同的 map 实例，对 map 的修改可以在不同函数中体现出来。 2. 减少 map 的拷贝开销在某些情况下，当 map 很大并且需要在函数之间传递时，将 map 传递为指针可以减少拷贝的开销。大的 map 传递值拷贝可能会消耗较多的内存和时间，而传递指针则只需传递地址。 3. 需要避免 map 自身的nil指针问题在某些场景下，需要避免 map 本身可能出现 nil 指针的问题。通过使用指针封装 map，可以在 nil map 的情况下返回一个非 nil 的 map，避免出现空 map 操作时的错误。 当需要在多个函数中修改同一个 map 并确保修改对所有函数生效时，使用指针封装可以很有用。以下是示例： 1234567891011121314151617181920212223242526package mainimport &quot;fmt&quot;type MapStruct struct { Data map[string]int}func NewMapStruct() *MapStruct { return &amp;MapStruct{ Data: make(map[string]int), }}func AddData(m *MapStruct, key string, value int) { m.Data[key] = value}func main() { mapInstance := NewMapStruct() AddData(mapInstance, &quot;Key1&quot;, 10) AddData(mapInstance, &quot;Key2&quot;, 20) fmt.Println(mapInstance.Data) // 输出：map[Key1:10 Key2:20]} 上述代码中，AddData​ 函数接收一个 MapStruct​ 的指针作为参数，并向其 map 字段中添加键值对。在 main​ 函数中调用 AddData​ 两次来修改同一个 MapStruct​ 实例的 map，并最终输出修改后的 map 数据。 对于第二种情况，如果需要在多个函数之间传递大型的 map，通过指针封装可以避免大的 map 被复制多次，节省内存和提高效率。以下是一个例子： 1234567891011121314151617package mainimport &quot;fmt&quot;func ProcessMap(m *map[string]int) { // 对传入的 map 进行处理 (*m)[&quot;Key1&quot;] = 100 (*m)[&quot;Key2&quot;] = 200}func main() { myMap := map[string]int{} ProcessMap(&amp;myMap) fmt.Println(myMap) // 输出：map[Key1:100 Key2:200]} 在这个示例中，ProcessMap​ 函数接收一个 map[string]int​ 类型的指针，并对传入的 map 进行操作。在 main​ 函数中调用 ProcessMap​ 函数，并传递了 myMap​ 的地址。通过指针传递，避免了对 myMap​ 进行复制，直接在原始的 map 上进行修改。 第三种情况，当需要避免 map 本身可能出现 nil 指针问题时，可以通过指针封装来确保 map 不会为 nil。以下是一个例子： 1234567891011121314151617181920212223242526package mainimport &quot;fmt&quot;type MapWrapper struct { Map *map[string]string}func NewMapWrapper() *MapWrapper { m := make(map[string]string) return &amp;MapWrapper{Map: &amp;m}}func main() { mapWrapper := NewMapWrapper() // 检查 map 是否为 nil，如果是则初始化一个空的 map if *(mapWrapper.Map) == nil { *mapWrapper.Map = make(map[string]string) } (*mapWrapper.Map)[&quot;Key1&quot;] = &quot;Value1&quot; (*mapWrapper.Map)[&quot;Key2&quot;] = &quot;Value2&quot; fmt.Println(*mapWrapper.Map) // 输出：map[Key1:Value1 Key2:Value2]} 在这个例子中，MapWrapper​ 结构体包含一个指向 map[string]string​ 类型的指针。NewMapWrapper​ 函数返回一个初始化过的 MapWrapper​ 实例，其中的 map 指针初始化为一个空的 map。然后在 main​ 函数中对 map 进行操作，并确保了 map 不会是 nil。 关于map中的key可以是任意类型为了说明值可以是任意类型的，这里给出了一个使用 func() int 作为值的 map——mf： 123456789101112package mainimport &quot;fmt&quot;func main() { mf := map[int]func() int{ 1: func() int { return 10 }, 2: func() int { return 20 }, 5: func() int { return 50 }, } fmt.Println(mf)} 输出结果为：map[1:0x53b9a0 2:0x53b9c0 5:0x53b9e0] 整形key的value值都被映射到了对应的函数地址 用slice作为map的值既然一个 key 只能对应一个 value，而 value 又是一个原始类型，那么如果一个 key 要对应多个值怎么办？例如，当我们要处理unix机器上的所有进程，以父进程（pid 为整形）作为 key，所有的子进程（以所有子进程的 pid 组成的切片）作为value。通过将 value 定义为 []int 类型或者其他类型的切片，就可以优雅的解决这个问题。 这里有一些定义这种 map 的例子： 12mp1 := make(map[int][]int)mp2 := make(map[int]*[]int) 示例中的 mp1​ 和 mp2​ 都是用来处理一个 key 对应多个值的情况。 ​mp1 := make(map[int][]int)​：这创建了一个 map，其中每个 key 是一个整数，而每个 value 是一个整数切片。这样，对于每个 key，你可以将其值设置为包含多个整数的切片，实现了一个 key 对应多个值的存储。 ​mp2 := make(map[int]*[]int)​：这个 map 与上述的 mp1​ 稍有不同，它的 value 是指向整数切片的指针。这意味着每个 key 对应一个指向整数切片的指针，这样可以通过指针修改对应 key 的整数切片。 举例说明： 1234567891011121314151617181920212223package mainimport &quot;fmt&quot;func main() { // 定义一个 map，每个 key 对应一个整数切片 mp1 := make(map[int][]int) // 添加多个值到同一个 key 对应的切片中 mp1[1] = append(mp1[1], 10) mp1[1] = append(mp1[1], 20) fmt.Println(mp1) // 输出: map[1:[10 20]] // 定义一个 map，每个 key 对应一个整数切片的指针 mp2 := make(map[int]*[]int) // 添加多个值到同一个 key 对应的切片中 slice := []int{30, 40} mp2[2] = &amp;slice fmt.Println(*mp2[2]) // 输出: [30 40]} 这些示例展示了如何使用这两种方式来实现一个 key 对应多个值的存储，并在需要时向切片中添加多个值。 在使用上，map[int][]int​ 和 map[int]*[]int​ 这两种方式有一些不同点： 数据所有权： ​map[int][]int​：在这种情况下，切片是直接存储在 map 的值中，而不是指针。这意味着每个 key 对应的值都是独立的，修改一个 key 对应的切片不会影响其他 key。 ​map[int]*[]int​：这种情况下，map 中的值是指向切片的指针。多个 key 可能共享相同的切片，因为它们都指向同一个切片。修改一个 key 对应的切片可能会影响其他 key。 内存管理和性能： ​map[int][]int​：由于切片是直接存储在 map 中的值，这可能会增加内存使用量，尤其当值的数量较多时。但是，访问和修改数据的性能可能更好，因为它们是直接存储在 map 中的。 ​map[int]*[]int​：这种情况下，指针存储在 map 中的值，实际的切片数据存储在堆上。它可能需要更多的内存，但在某些情况下，它可以减少复制和传递数据的开销，因为多个 key 可以共享相同的切片数据。 安全性： ​map[int][]int​：由于切片是直接存储在 map 中的值，对一个 key 对应的切片的修改不会影响其他 key。这可以提供更好的安全性，因为每个 key 拥有独立的数据。 ​map[int]*[]int​：当多个 key 共享同一个切片时，一个 key 的修改可能会影响其他 key，可能导致意外的数据修改。在多个地方使用同一个切片时，需要更小心地管理数据的修改。 因此，在使用时，需要根据具体的场景和需求来选择适当的方式。如果需要独立的数据副本或更好的数据安全性，map[int][]int​ 可能更适合；如果需要共享数据、降低内存占用或减少复制开销，map[int]*[]int​ 可能更适合。 这个差异的优缺点有待商榷 当使用 map[int]*[]int​ 的方式时，多个 key 可能共享相同的切片。因此，对一个 key 对应的切片的修改可能会影响其他 key。 举个例子： 12345678910111213141516171819202122package mainimport ( &quot;fmt&quot;)func main() { myMap := make(map[int]*[]int) // 创建一个切片并将其地址存储在 map[1] 中 slice := []int{10, 20} myMap[1] = &amp;slice // 将 map[2] 指向 map[1] 所指向的切片 myMap[2] = myMap[1] // 修改 map[1] 中的切片 *myMap[1] = append(*myMap[1], 30) // 输出 map[2]，查看是否受到影响 fmt.Println(*myMap[2]) // 输出：[10 20 30]} 在这个示例中，我们创建了一个 map[int]*[]int​ 类型的 map，将一个切片的地址存储在 map[1] 中。接着，我们将 map[2] 的值指向了 map[1] 所指向的同一个切片。 然后，我们修改了 map[1] 中的切片，通过在 map[1] 所指向的切片中添加一个新元素。接着，我们打印了 map[2] 的值，发现其值也发生了变化，因为 map[2] 实际上指向了 map[1] 所指向的同一个切片。 这就说明了当使用 map[int]*[]int​ 时，一个 key 的切片修改会影响到其他 key，因为它们指向的是相同的切片。 我的尝试： 指针所指向的地址相同的情况： 12345678910111213141516171819202122232425262728293031323334package mainimport &quot;fmt&quot;func main() { // 定义一个 map，每个 key 对应一个整数切片 mp1 := make(map[int][]int) // 添加多个值到同一个 key 对应的切片中 mp1[1] = append(mp1[1], 10) mp1[1] = append(mp1[1], 20) fmt.Println(mp1) // 输出: map[1:[10 20]] // 定义一个 map，每个 key 对应一个整数切片的指针 mp2 := make(map[int]*[]int) // 添加多个值到同一个 key 对应的切片中 slice1 := []int{30, 40} mp2[2] = &amp;slice1 mp2[3] = mp2[2] fmt.Println(*mp2[2]) fmt.Println(*mp2[3]) *mp2[2] = append(*mp2[2], 50) fmt.Println(*mp2[2]) // 输出: [30 40] fmt.Println(*mp2[3]) // 打印 mp2[2] 和 mp2[3] 指针所指向的地址 fmt.Printf(&quot;Address of mp2[2]: %p\\n&quot;, mp2[2]) fmt.Printf(&quot;Address of mp2[3]: %p\\n&quot;, mp2[3])} 结果: ​​ ‍ 指针所指向的地址不相同的情况： 1234567891011121314151617181920212223242526272829303132package mainimport &quot;fmt&quot;func main() { // 定义一个 map，每个 key 对应一个整数切片 mp1 := make(map[int][]int) // 添加多个值到同一个 key 对应的切片中 mp1[1] = append(mp1[1], 10) mp1[1] = append(mp1[1], 20) fmt.Println(mp1) // 输出: map[1:[10 20]] // 定义一个 map，每个 key 对应一个整数切片的指针 mp2 := make(map[int]*[]int) // 添加多个值到同一个 key 对应的切片中 slice1 := []int{30, 40} mp2[2] = &amp;slice1 // 这里是一个新的实例，在内存中独立存在，与之前的slice1不同 slice2 := []int{30, 40} mp2[3] = &amp;slice2 fmt.Println(*mp2[2]) // 输出: [30 40] fmt.Println(*mp2[3]) // 打印 mp2[2] 和 mp2[3] 指针所指向的地址 fmt.Printf(&quot;Address of mp2[2]: %p\\n&quot;, mp2[2]) fmt.Printf(&quot;Address of mp2[3]: %p\\n&quot;, mp2[3])} 结果： ​​ 总结浅浅地理解map与slice的差异： 对于slice切片，它的核心强调的是容量，能装纳多少数据，但是map强调的是键值对的存储与索引 map底层更加复杂，在扩容上来讲，map扩容的效率要低，slice-&gt;数组；map-&gt;桶；map扩容操作时需要对所有键值对重新计算哈希值和桶的位置。这种操作需要花费更长的时间。 更深层次的理解见底层篇——《核心底层》 总结起来主要了以下几点： 什么是map，map格式与数据类型限制【尤其是key】，map特征，如何创建以及初始化map map的常规操作：访问，赋值，遍历，删除，判断key是否存在，获取长度等 map的嵌套 其实还涉及很多问题，比如map的并发安全问题，syc.map相关，后续想到再巩固","link":"/post/re-learning-go-language-map-z1kmiya.html"},{"title":"Reverse a String","text":"字符串反转翻转含有中文、数字、英文字母​的字符串 如：&quot;子asdf黑g白hjkl小&quot;​ 1234567891011121314151617181920212223242526272829303132package mainimport &quot;fmt&quot;/** @Title main @Description: 1.rune关键字，从golang源码中看出，它是int32的别名（-2^31 ~ 2^31-1），比起byte（-128～127），可表示更多的字符。 2.由于rune可表示的范围更大，所以能处理一切字符，当然也包括中文字符。在平时计算中文字符，可用rune。 3.因此将字符串转为rune的切片，再进行翻转，完美解决。 @Author luommy 2023-09-22 00:19:14**/func main() { src := &quot;子asdf黑g白hjkl小&quot; // int32 is the set of all signed 32-bit integers. Range: -2147483648 through 2147483647. str := reverse([]rune(src)) fmt.Printf(&quot;%v\\n&quot;, string(str))}/** @Title reverse @Description @Author luommy 2023-09-22 00:12:29 @Param s @Return []rune**/func reverse(s []rune) []rune { for i, j := 0, len(s)-1; i &lt; j; i, j = i+1, j-1 { s[i], s[j] = s[j], s[i] } return s} ‍ 运行结果： ​​","link":"/post/string-reverse-1jimhs.html"},{"title":"服务器性能指标相关内容","text":"服务器性能指标相关内容 以前不了解这方面，常听说什么12306、双十一淘宝、天猫的并发支撑的某某性能数据都感觉很酷的样子…..实际情况就是：我知道确实很强，但这些数据性能反应怎么个水平，强到什么水平，完全没有概念，比如可以并发支持用户同时的千万级请求、上亿的同时请求，强到什么概念，凭什么可以这么强，是完全无法想象的…..就比如我先现在八块腹肌、单手五十个俯卧撑，外行感觉很强，但是只有我自己知道付出了多少努力(其实没有多少努力全看天赋[dog])…… 其实打算把这一部分划分到高并发区域，但是这都是一些基础的概念和微不足道的理解，还是划分到架构板块了。 11.21 新的理解：我的评价是高并发不及高可用！可高用是真的太难了[dog] 一些常见名词、概念： 峰值时间：每天80%的访问集中在20%的时间里，这20%时间即峰值时间 并发：一段时间访问的大量用户的请求（并发是最能体现你的程序和机器的性能。） 并行：同一时刻的大量用户的请求(并发和并行很像，但是维度不同) QPS（每秒查询率）：对应fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。 QPS = 并发量 / 平均响应时间 并发量 = QPS * 平均响应时间 峰值估算公式：( 总PV数 * 80% ) / ( 每天秒数 * 20% ) = 峰值时间每秒请求数(QPS) 机器：峰值时间每秒QPS / 单台机器的QPS = 需要的机器数量 阿姆达尔定律：不可并行部分的比率才是决定着是否能成倍增长效率的关键，CPU核心数的提升不一定能够提升QPS QPS：QPS（Queries-per-second）是每秒钟查询率，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准, 即每秒的响应请求数，也即是最大吞吐能力。 对应fetches/sec，即每秒的响应请求数/每秒查询率 QPS即每秒处理事务数，包括了用户请求服务器、服务器自己的内部处理、服务器返回给用户。这三个过程，每秒能够完成N个这三个过程，TPS也就是N； Qps基本类似于Tps，但是不同的是，对于一个页面的一次访问，形成一个Tps； 但一次页面请求，可能产生多次对服务器的请求，服务器对这些请求，就可计入“QPS”之中。理解上来看QPS可能颗粒度更细一点。 TPSTPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。 即每秒处理事务数，每个事务包括了如下3个过程： a.用户请求服务器 b.服务器自己的内部处理（包含应用服务器、数据库服务器等） c.服务器返回给用户 如果每秒能够完成N个这三个过程，tps就是N； 一般的，评价系统性能均以每秒钟完成的技术交易的数量来衡量。系统整体处理能力取决于处理能力最低模块的TPS值。这一点有一点深入的体会，金融交易所涉及的金融交易，一般使用的都是TPS来衡量。比如交易清仓高峰时间、订单峰值之类的可以联想一下。 RT响应时间即RT，处理一次请求所需要的平均处理时间。对于RT，客户端和服务端是大不相同的，因为请求从客户端到服务端，需要经过广域网，所以客户端RT往往远大于服务端RT，同时客户端的RT往往决定着用户的真实体验，服务端RT往往是评估我们系统好坏的一个关键因素。这一点说白了我们能够控制的只有服务器、后端的这一部分链路，涉及到物理、底层基础设施建设的我想很难把控，因为要考虑的因素太多，猜测涉及大部门之间的跨越。 最佳线程数的困扰在开发过程中，我们一定面临过很多的线程数量的配置问题，这种问题往往让人摸不到头脑，往往都是拍脑袋给出一个线程池的数量，但这可能恰恰是不靠谱的，过小的话会导致请求RT极具增加，过大也一样RT也会升高。所以对于最佳线程数的评估往往比较麻烦。 这一部分有一说一确实没有比较好的实践思路….后续可能会了解下，但有固定的思路，但go中又很特殊，是goroutine，猜想应用基准测试来尝试，慢慢试；或者计算，但是计算好像是线程层面的，比如Java中经常喜欢计算求得最佳线程数 QPS和RT的关系单线程场景：假设我们的服务端只有一个线程，那么所有的请求都是串行执行，我们可以很简单的算出系统的QPS，也就是：QPS = 1000ms/RT。假设一个RT过程中CPU计算的时间为49ms，CPU Wait Time 为200ms，那么QPS就为1000/（49+200） = 4.01。 多线程场景我们接下来把服务端的线程数提升到2，那么整个系统的QPS则为：2 *（1000/(49+200)）=8.02。可见QPS随着线程的增加而线性增长，那QPS上不去就加线程呗，听起来很有道理，公式也说得通，但是往往现实并非如此，后面会聊这个问题。 最佳线程数？从上面单线程场景来看，CPU Wait time为200ms,你可以理解为CPU这段时间什么都没做，是空闲的，显然我们没把CPU利用起来，这时候我们需要启多个线程去响应请求，把这部分利用起来，那么启动多少个线程呢？我们可以估算一下 空闲时间200ms，我们要把这部分时间转换为CPU Time,那么就是(200+49)/49 = 5.08个，不考虑上下文切换的话，约等于5个线程。同时还要考虑CPU的核心数和利用率问题，那么我们得到了最佳线程数计算的公式： (（CPU Time + CPU Wait Time）/ CPU Time) * coreSize * cupRatio 最大QPS？得到了最大的线程数和QPS的计算方式： QPS = Thread num * 单线程QPS = （CPU Time + CPU Wait Time）/CPU Time * coreSize * CupRatio * (1000ms/(CPU Time + CPU Wait Time)) = (1000ms/ CPU Time) * coreSize * cpuRatio 所以决定一个系统最大的QPS的因素是CPU Time、CoreSize和CPU利用率。看似增加CPU核数（或者说线程数）可以成倍的增加系统QPS，但实际上增加线程数的同时也增加了很大的系统负荷，更多的上下文切换，QPS和最大的QPS是有偏差的。 CPU Time &amp; CPU Wait Time &amp; CPU 利用率CPU Time：就是一次请求中，实际用到计算资源。CPU Time的消耗是全流程的，涉及到请求到应用服务器，再从应用服务器返回的全过程。实际上这取决于你的计算的复杂度。CPU Wait Time：是一次请求过程中对于IO的操作，CPU这段时间可以理解为空闲的，那么此时要尽量利用这些空闲时间，也就是增加线程数。CPU 利用率：是业务系统利用到CPU的比率，因为往往一个系统上会有一些其他的线程，这些线程会和CPU竞争计算资源，那么此时留给业务的计算资源比例就会下降，典型的像，GC线程的GC过程、锁的竞争过程都是消耗CPU的过程。甚至一些IO的瓶颈，也会导致CPU利用率下降(CPU都在Wait IO，利用率当然不高)。 增加CPU核数是否能对QPS得到提升？首先答案是不一定 从上面的公式我们可以看出，假设CPU Time和CPU 利用率不变，增加CPU的核数能使QPS呈线性增长。但是很遗憾，现实中不是这样的…首先先看一下阿姆达尔定律：阿姆达尔定律给出了任务在固定负载的情况下，随着系统资源的提升，执行速度的理论上限。以计算机科学家Gene Amdahl命名。 ​​​​​​ 其中Slatency: 整个任务的提速比。s: 部分任务得益于系统资源升级带来的提速比。p: 这部分任务执行时间占整个任务执行时间的百分比（系统资源提升前）。 从上可以得到： ​​​​ 以上公式说明了通过资源升级来给任务加速的加速比上限，而且和提速的幅度无关，理论加速比总是受限于不能加速的任务的比例。 阿姆达尔的定律常用于并行计算中，用来估计多处理器情况下的理论加速比。例如，如果有个程序在单核下需要执行20个小时，并且不能被并行处理的部分占1个小时的执行时间，剩余的19个小时(p=0,95)的任务可以并行化，那么不管有多少核心来并行处理这个程序，最小执行时间不可能小于一个小时。由此得到，理论加速比的上限是20倍（1/(1-p) = 20）。因此，并行计算只和少数的核心和极度可并行化的程序相关。 同样，对于1000ms/(CPU Time) * coreSize * cpuRatio我们不断的增加CoreSize或者说线程数的时候。我们的请求变多了，随之而来的就是大量的上下文切换（go中的协程不涉及上下文切换） 、大量的GC、大量的锁征用，这些会增加不可并行部分的总时间，也会大大的增加CPU Time。假设我们的串行部分不变的话，增大核数，CPU不能得到充分的利用，利用率也会降低。所以，对于阿姆达尔定律而言，不可并行部分的比率才是决定着是否能成倍增长效率的关键。也就是说最佳线程数也好，最大QPS也好，增加内核数量不一定能是系统指标有成倍的增长。更关键的是能改变自己的架构，减小串行的比率，让CPU更充分的利用，达到资源的最大利用率。 如何寻求最佳线程数和最大QPS通过上面一些例子，我们发现当线程数增加的时候，线程的上下文切换会增加，GC Time会增加。这也就导致CPU time 增加，QPS减小，RT也会随着增大。这显然不是我们希望的，我们希望的是在核数一定的情况下找到某个点，使系统的QPS最大，RT相对较小。所以我们需要不断的压测，调整线程池，找到这个QPS的峰值，并且使CPU的利用率达到100%,这样才是系统的最大QPS和最佳线程数。 补充上下文切换多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是: 当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。Linux 相比与其他操作系统(包括其他类 Unix 系统)有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。 如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的，CPU 根本没有得到充分利用。 如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率 CPU密集任务与I/O密集任务有一个简单并且适用面比较广的公式: CPU 密集型任务(N+1): 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N (CPU 核心数)+1.比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用CPU 的空闲时间。 I/O 密集型任务(2N): 这种任务应用起来，系统会用大部分的时间来处理 /0 交互，而线程在处理 /O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 如何判断是 CPU 密集任务还是Io 密集任务? CPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及网络读取，文件读取这类都是 I/O 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待I/O操作完成的时间来说很少，大部分时间都花在了等待 I/O 操作完成上. 线程数更严谨的计算的方法应该是: 最佳线程数 = (CPU 核心数)* (1+MT(线程等待时间)/ST (线程计算时间)) 公示也只是参考，具体还是要根据项目实际线上运行情况来动态调整。 PVPV（Page View）：页面访问量，即页面浏览量或点击量，用户每次刷新即被计算一次。可以统计服务一天的访问日志得到。 UVUV（Unique Visitor）：独立访客，统计1天内访问某站点的用户数。可以统计服务一天的访问日志并根据用户的唯一标识去重得到。响应时间（RT）：响应时间是指系统对请求作出响应的时间，一般取平均响应时间。可以通过Nginx、Apache之类的Web Server得到。 DAUDAU(Daily Active User)，日活跃用户数量。常用于反映网站、互联网应用或网络游戏的运营情况。DAU通常统计一日（统计日）之内，登录或使用了某个产品的用户数（去除重复登录的用户），与UV概念相似 MAUMAU(Month Active User)：月活跃用户数量，指网站、app等去重后的月活跃用户数量 系统吞吐量评估我们在做系统设计的时候就需要考虑CPU运算，IO，外部系统响应因素造成的影响以及对系统性能的初步预估。而通常情况下，我们面对需求，我们评估出来的出来QPS，并发数之外，还有另外一个维度：日pv。 通过观察系统的访问日志发现，在用户量很大的情况下，各个时间周期内的同一时间段的访问流量几乎一样。比如工作日的每天早上。只要能拿到日流量图和QPS我们就可以推算日流量。 通常的技术方法： 1、找出系统的最高TPS和日PV，这两个要素有相对比较稳定的关系（除了放假、季节性因素影响之外）。 2、通过压力测试或者经验预估，得出最高TPS，然后跟进1结果的关系，计算出系统最高的日吞吐量。B2B中文和淘宝面对的客户群不一样，这两个客户群的网络行为不应用，他们之间的TPS和PV关系比例也不一样。 软件性能测试的基本概念和计算公式软件做性能测试时需要关注哪些性能呢？ 首先，开发软件的目的是为了让用户使用，我们先站在用户的角度分析一下，用户需要关注哪些性能。 对于用户来说，当点击一个按钮、链接或发出一条指令开始，到系统把结果已用户感知的形式展现出来为止，这个过程所消耗的时间是用户对这个软件性能的直观印 象。也就是我们所说的响应时间，当相应时间较小时，用户体验是很好的，当然用户体验的响应时间包括个人主观因素和客观响应时间，在设计软件时，我们就需要考虑到如何更好地结合这两部分达到用户最佳的体验。如：用户在大数据量查询时，我们可以将先提取出来的数据展示给用户，在用户看的过程中继续进行数据检索，这时用户并不知道我们后台在做什么。 用户关注的是用户操作的相应时间。 其次，我们站在管理员的角度考虑需要关注的性能点。 1、 响应时间2、 服务器资源使用情况是否合理3、 应用服务器和数据库资源使用是否合理4、 系统能否实现扩展5、 系统最多支持多少用户访问、系统最大业务处理量是多少6、 系统性能可能存在的瓶颈在哪里7、 更换那些设备可以提高性能8、 系统能否支持7×24小时的业务访问 再次，站在开发（设计）人员角度去考虑。 1、 架构设计是否合理2、 数据库设计是否合理3、 代码是否存在性能方面的问题4、 系统中是否有不合理的内存使用方式5、 系统中是否存在不合理的线程同步方式6、 系统中是否存在不合理的资源竞争 ‍","link":"/post/server-performance-indicators-and-related-content-shallow-understanding-vlxgk.html"}],"tags":[{"name":"timeline","slug":"timeline","link":"/tags/timeline/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"领域算法","slug":"领域算法","link":"/tags/%E9%A2%86%E5%9F%9F%E7%AE%97%E6%B3%95/"},{"name":"知识体系","slug":"知识体系","link":"/tags/%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"并发编程","slug":"并发编程","link":"/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"goland技巧","slug":"goland技巧","link":"/tags/goland%E6%8A%80%E5%B7%A7/"},{"name":"高并发","slug":"高并发","link":"/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"Myblog","slug":"Myblog","link":"/tags/Myblog/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"nacos","slug":"nacos","link":"/tags/nacos/"}],"categories":[{"name":"京东到家","slug":"京东到家","link":"/categories/%E4%BA%AC%E4%B8%9C%E5%88%B0%E5%AE%B6/"},{"name":"数据结构与算法","slug":"数据结构与算法","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"算法加练","slug":"算法加练","link":"/categories/%E7%AE%97%E6%B3%95%E5%8A%A0%E7%BB%83/"},{"name":"分布式、微服务、架构","slug":"分布式、微服务、架构","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E3%80%81%E6%9E%B6%E6%9E%84/"},{"name":"Golang","slug":"Golang","link":"/categories/Golang/"},{"name":"知识体系","slug":"知识体系","link":"/categories/%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"name":"微服务","slug":"分布式、微服务、架构/微服务","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E3%80%81%E6%9E%B6%E6%9E%84/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"分布式","slug":"分布式、微服务、架构/分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E3%80%81%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"并发编程","slug":"Golang/并发编程","link":"/categories/Golang/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"重学系列","slug":"Golang/重学系列","link":"/categories/Golang/%E9%87%8D%E5%AD%A6%E7%B3%BB%E5%88%97/"},{"name":"核心底层","slug":"Golang/核心底层","link":"/categories/Golang/%E6%A0%B8%E5%BF%83%E5%BA%95%E5%B1%82/"},{"name":"架构","slug":"分布式、微服务、架构/架构","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E3%80%81%E6%9E%B6%E6%9E%84/%E6%9E%B6%E6%9E%84/"}],"pages":[]}