{"posts":[{"title":"2023.9.19 DailyNote","text":"2023-09-19 星期二2023.09.19 Tue 🌞 今年已过了 262 天（第 39 周/共 53 周）,距离 2024 年还有 103 天。距离 2023-11-04 高级-架构师考试还有 46 天。🗓️Weather：🌞🌥☁️⛈🌧🌦🌈🌪🌀⚡❄️🔥🥶🌊🌫🏠Location： 中国·广东省🛌Sleep：1：00 → 7：45 希望早日调整好作息时间 ⏰Must-To-Do 第一次发表… 没有早起 🚀️进展 自律：没有进展，依旧六块腹肌，单手五十俯卧撑 英法：菜够，法语真的难学 遇见：无，又是平凡的一天 其他：参加斗地主比赛 🐣Mini-Habits 月计划与反思 每日Golang 数据结构和算法 回顾昨天 每天早起 🧠今日反省思源功能太强大… 已经玩不明白了… 日历配置这个DailyNote template 配置原理又忘记了… 算了… 毕竟只是工具而已 争取每日后续更新一篇博客 日记推送到timeline​tag下 大概一周一次 看心情决定是否发布 随机复习 距离 2024-07-01​ 还剩 285​ 天，加油，你一定行！ 随机复习是一个好习惯，可惜现在笔记内容提炼不够精简，就不放到博客上展示了….","link":"/post/20230919-tuesday-uuzu4.html"},{"title":"命运齿轮","text":"命运齿轮2023.10.14 Sat 🌞 今年已过了 287 天（第 42 周/共 53 周）,距离 2024 年还有 78 天。距离 2023-11-04 高级-架构师考试还有 21 天。🗓️Weather：🌞🌥☁️⛈🌧🌦🌈🌪🌀⚡❄️🔥🥶🌊🌫🏠Location： 中国·广东省🛌Sleep：5.30 → 12 熬了个大夜-深夜思考人生《痛苦面具》 ⏰Must-To-Do ‍ 🚀️进展 自律：周五晚拉满了，游泳0.5h、力量训练1h、骑车10KM、跑步5kM 25min、核心训练0.5h 英法：IOS 播客很好用 遇见：清华经管-混血修勾？？ 其他：努力永远不会错 🐣Mini-Habits 月计划与反思 每日Golang 数据结构和算法 回顾昨天 每天早起 🧠今日反省 我发现很多技术手段，或者思想在某种程度上都是类似的，到一定阶段必然是融汇贯通的。 很多知识会就是会，不会就是不会，如果一些问题有过自己的思考量，即使知识本身的内容记不住，但是这个逻辑关系，逻辑追溯还是很重要的 最近玩且看狼人杀，记忆力好真的很关键，逻辑梳理更是一种能力，需要培养 ✨随缘记随便写点什么吧…. 缓存的分片有用到哈希算法与一致性哈希算法，同等联想到负载均衡的一种策略，也是一致性哈希算法，瞬间有点悟，好多其实都是相通的… ‍","link":"/post/20231014-saturday-zh8hf1.html"},{"title":"","text":"2023-10-21 星期六2023.10.21 Sat 🌞 今年已过了 294 天（第 43 周/共 53 周）,距离 2024 年还有 71 天。距离 2023-11-04 高级-架构师考试还有 14 天。🗓️Weather：🌞🌥☁️⛈🌧🌦🌈🌪🌀⚡❄️🔥🥶🌊🌫🏠Location： 中国·广东省🛌Sleep：2 → 7.30 ；9.30 → 12 ⏰Must-To-Do 周末学习打卡 🚀️进展 自律：马拉松备赛 ； 俄挺和前水平有点小帅 英法：关注了几个英语博主 遇见：三只羊在沈阳开分公司了？？ 其他： 🐣Mini-Habits 月计划与反思 每日Golang 数据结构和算法 回顾昨天 每天早起 🧠今日反省 越来越信因果效应了… 信息差也是一种能力… 早睡早起肯定算一种优点，但真的好难 快速建立知识体系 ✨随缘记随便写点什么吧…. 如果不知道布隆过滤器，那几乎不可能知道布谷鸟过滤器… 如果知道缓存穿透，那几乎必知道布隆过滤器… 百因必有果… 三缓存问题很常见，不必多说： ​​​​ 今天看过一个系统架构分析的实例： 系统使用过程中，由于同样的数据分别存在于数据库和缓存系统中，必然会造成数据同步或数据不一致性的问题。 法一：应用程序读数据时，首先读缓存，当该数据不在缓存时,再读取数据库；应用程序写数据吋，先写缓存，成功后再写数据库；或者先写数据库，再写缓存。 法二： 读数据操作的基本步骤：1.根据key读缓存；2.读取成功则直接返回；3.若key不在缓存中时，根据key (从数据库中读取数据) ；4.读取成功后， (更新缓存中key值) ；5.成功返回。写数据操作的基本步骤：1.根据key值写 (数据库) ；2.成功后 (更新缓存(key值)) ；3.成功返回。 ‍ ‍","link":"/post/20231021-saturday-z1vmbxc.html"},{"title":"2023.8.30","text":"2023.8.30从今天起，要狠狠滴加练，潶櫹潶櫹！！！ ‍","link":"/post/2023830-zy8cbl.html"},{"title":"几个后端的问题","text":"6.14 以下几个简单但并不简单的问题验证下自己的后端能力水平 业务Rpc服务如何理解 什么场景下应用RPC 是远程调用协议，服务之间只需要维持一个通信协议即可，不需要对编程语言有任何限制，也不用关系底层网络协议。多数是用在微服务、分布式场景下。 分布式下rpc如何调用可以通过注册中心+服务发现的方式实现调用，市面上很多组件比如consul、etcd等等都可以实现。 负载均衡策略有哪些 你们公司用的是哪种包括轮询、随机、加权、一致性哈希及最小连接数等。我们用的是轮询方式。 这个最好是有自己实现过的代码。 手写负载均衡、手写缓存中间件…后续再补充 Rpc协议和HTTP协议如何理解，有什么区别 RPC协议是远程调用协议，HTTP是超文本传输协议； RPC多用在微服务分布式和内部调用，HTTP多用在外部api服务； RPC的调用速度快于HTTP，但HTTP的实现要比RPC简单很多。 RPC有什么优势 为什么有这种优势RPC 不要考虑编程语言，服务只需要维持一个通信协议即可；调用速度非常的快；封装了很多方法，比如负载均衡等；适用于分布式微服务场景，内部扩展型较好。 为什么有这种优势不会答，我估计是要深入到socket层面 月百万级数据以上，总共一亿数据如何管理 优化思路如果是这种场景可以根据时间来划分，按月水平分表，这种思路，可以及时清理掉冷数据，如果在业务评估阶段就估计月表的数据量是百万级，且又可能在业务高峰期上升到千万或者亿界别的数据，那么可以考虑多一个月的数据再次分片，按照大小拆分，建议500w数据量拆分一个，虽然单表的数据量在2000w的时候，MySQL也能维持较好的层数，但是考虑到性能等问题，参考阿里巴巴数据库的设计思路，他们在海量数据的业务场景下经验丰富，比较推荐500w拆分一张 另外一种思路是根据用户id来进行hash分片拆分，多个MySQL部署，举个例子通过userid来计算哈希函数，然后和哈希掩码取模，可以快速定位到用户的数据存储在哪一个实例中，比较推荐使用MyCat这种业内比较成熟的数据库分片组件。 按月分表情况下(几十张表），这种情况下如何查某个用户的所有的订单可以加一层设计，比如用Redis的Hash结构，用户的userid作为key，list作为field，value是所有的订单信息的月份、id等，可以快速定位表位置、索引的信息。 12345678910111213141516{ &quot;user_id_1&quot;: { &quot;order_info&quot;: { &quot;2022-01&quot;: [&quot;order_1&quot;, &quot;order_2&quot;], &quot;2022-02&quot;: [&quot;order_3&quot;], &quot;2022-03&quot;: [&quot;order_4&quot;, &quot;order_5&quot;] } }, &quot;user_id_2&quot;: { &quot;order_info&quot;: { &quot;2022-01&quot;: [&quot;order_6&quot;, &quot;order_7&quot;], &quot;2022-02&quot;: [&quot;order_8&quot;], &quot;2022-03&quot;: [&quot;order_9&quot;, &quot;order_10&quot;] } }} 在这个示例中，每个 userid 都是 Hash 表的 key，而对应的 value 是一个包含了 order_info​ 这个 field。order_info​ 是一个内部的 Hash 结构，它使用年月（例如 “2022-01”）作为 field，而对应的值是一个列表，包含该用户在该月份下的所有订单编号。 通过这种设计，你可以快速定位到指定用户的订单信息，并按照不同月份进行索引。例如： 获取某个用户的所有订单：使用 HGETALL user_id_x 命令获取该用户的完整信息，然后再读取其中的 order_info​ 字段即可得到该用户在不同月份下的订单列表。 获取某个用户指定月份的订单信息：使用 HGET user_id_x order_info:YYYY-MM 命令获取对应月份的订单列表。 需要注意的是，为了保证不同月份的订单信息不会相互冲突，每个月份的 field 最好采用类似 “YYYY-MM” 的格式进行命名。 这种结构可以帮助你更好地组织和索引订单信息，同时也提供了快速定位、查询和统计的能力。 可以实现以下快速定位和索引的优势： 快速访问指定用户的订单信息：通过用户的 userid 作为 key，直接从 Redis 中获取对应的 Hash 结构，从而快速获取该用户的订单信息。 快速定位到指定年月的订单列表：在用户的订单信息中，使用年月作为 field，可以直接访问指定年月的订单列表，而无需遍历整个订单信息。 高效添加和删除订单信息：使用 Redis 的 List 结构存储订单列表，可通过头尾插入和删除操作，快速添加和删除订单，而不会影响其他年月的订单列表。 订单状态怎么查（已支付，待支付，待发货这种）设定字段订单状态status，通过上述的redis方案快速在数据库中查询，这里正好使用了Redis，可以设定缓存，但这就涉及到数据库和缓存更新的一致性问题了，写操作先更新数据库再删除缓存，读如果命中则直接返回，没命中则读取后写入缓存的方案可以避免。 如果用热点数据如何定义热点数据可以用 Redis 的 Zset 来给数据进行评分，这里的热点可以以时间、点击量、购买数等等因素综合考量。 订单状态转换如何更新？如何保持同步这个在上面已经说明了，写是先更新数据库再删除缓存，读是命中则返回，没命中则读取后写入缓存保持同步。这里同样需要保证了两步操作同时执行成功，可以用两种方案来保证，一种是重试机制，将操作发送到消息队列中，执行成功则在消息队列中删除该信息，如果失败则重新读取这条消息，超过一定次数则向上游返回报错，第二种方案是订阅MySQL的binlog日志，由于MySQL执行完之后会将操作记录在binlog里，所以可以使用binlog来找到具体的操作，这里推荐使用阿里巴巴的canal组件，他的思想就是模拟了mysql主从的交互协议实现这个功能。 服务器线上问题有排查过吗，怎么定位问题所在，整体链路讲讲，比如504这种问题。结合实际场景和原因进行分析有排查过，首先思路是并不是问题出现了再去排查问题，我们先要设计一套预防方案，如果是常规的报错问题，可以通过日志、链路追踪查询定位到，如果是服务崩溃等信息，可以通过监控报警，比如grafana，包括在CPU、内存负载压力过高的时候可以提前预警人工介入。 真的是线上出现了bug、崩溃、超时等问题，那么整体流程是，先通过链路追踪定位返回错误信息或者崩溃的服务是哪个，查看服务崩溃的日志的具体原因，精确到代码行，如果无法从代码的逻辑角度排查问题，我们要看CPU、内存的使用情况，通过系统监控确定是哪一个环节出问题，然后查看是否是内存泄漏、SQL慢查询、GC异常等问题。 504 是属于 http请求返回超时的问题，举一个实际场景的例子，比如我做过一个音视频合成的例子，之前我调用合成api接口的超时，然后返回给上游的调度服务错误信息，定位到是我服务返回的超时，模拟了当时的请求信息，通过链路追踪，在jager上查看了收到合成api请求的时间耗时过长，解决方案是，最开始合成方是用http的方式返回合成url，后来我们采用grpc 流式传输的方案可以实时返回，在没接收到结束信号时，不断开链接。 502 和 504 区别 502 是网络无法请求，接收到错误的响应； 504 是网络请求，但是接受响应超时； MySQL慢查询有涉及过吗？如果用了索引还是慢查询该怎么办 ？慢查询可以通过 MySQL 的慢查询日志查到，不过默认 MySQL 的慢查询日志是关闭的。 如果用了索引还是慢查询，可以使用 explain 命令来查看 sql 语句的执行计划，可以查看是否正常使用了索引，排查是否存在索引失效的可能。另外排查是否是数据库的参数，比如缓存、连接等待等问题。 数据量过大该如何优化数据量过大可以考虑分库分表了。分库就是根据实际场景将数据分散到多个库，分表是将数据拆分成多个表，防止单表过大。 如何分库分表 垂直拆分：由于前期表的设计没有抽象，所以这时候要根据关系性较强的几个字段对表进行拆分。一种思路是将长度比较大、不常用的信息，移到扩展表。 水平拆分：将一张大表拆分成数据结构相同的几个表，防止单表过大。 这里举个商城的例子来说明，我们可以拆分成订单信息库、用户信息库、商品详情库，每个库中的表的数据量过于庞大，比如超过500万行就可以考虑水平拆分，如果有几个关系性较强的字段，可以垂直拆分建立一张新表，具体根据自己的业务实际场景来进行扩展。 你常用的索引有哪些 结合业务说明 按照类型分类：B+ 数索引，哈希索引，fulltext索引； 按照存储方式分类：聚簇索引和非聚簇索引； 按照使用的列分类：联合索引和单一索引； 按照使用的字段分类：主键索引、唯一索引、前缀索引和普通索引。 常用的是联合索引和主键索引，比如通过id直接查询一条记录的完整信息，我们可以使用主键索引快速定位，再比如联合性比较强的两个字段，可以建立联合索引，比如要查询年龄为x、成绩是y的成员name，可以用（x，y，name）建立联合索引，利用覆盖索引可以减少回表。 什么是聚簇索引，什么是非聚簇索引，它们有什么区别聚簇索引具有唯一性，比如主键就是聚簇索引，如果没有主键会选择唯一且不为NULL的列作为主键索引，如果没有则会生成一个自增id作为主键索引。聚簇索引存放的信息是完整的数据记录，而非聚簇索引只存放聚簇索引信息。如果查询语句使用的是非聚簇索引，且查询的数据不是主键值，会在叶子节点找到主键值后进行回表，如果是主键值就会进行索引覆盖。 主键索引的底层存储结构 大致实现过程B+ 树。B+ 树是由 B 树改进而来的，所有的索引都存放在叶子节点，并构成有序的链表，其实是双向链表，叶子节点的值存放的是数据页，数据页里包含完整的记录，而非叶子节点只存放索引，非叶子节点会作为叶子节点中索引的最大或者最小节点，比如举个例子，根节点存放的索引是 (1、10、19)，那么第二层就可以是 (1、4、7)，(10、13、16)，(19、22、25)，而第三层如果是叶子节点，就会存放完整的索引和数据。并且支持范围查询，由于 MySQL 的 B+ 树底层节点是双向链表，所以范围查询效率很高；B+ 树的非叶子节点只存放索引，可以存放更多的记录，所以相同数据量下，B+ 树更矮胖，减少了 磁盘 IO；B+ 树有大量的冗余节点，在插入和删除时不会发生过多的树结构变化。 非主键索引、联合索引等的存储结构B+ 树。 这些索引在存储数据时有什么区别主键索引的值是完整的数据记录，其他索引存储的值是主键索引，联合索引的key是多个字段，查询的时候按照最左匹配原则。 主键索引的索引信息存在哪里叶子节点 MySQL的数据存在哪里存在磁盘中，如果是 InnoDB 引擎，则会按照三个文件存放，db.opt 存放的是数据库设置的默认字符集和字符校验规则，.frm 文件是存放表的结构信息，比如列、数据类型、索引等，.ibd 是存放数据的内容。 范围查询时主键索引是如何去做的首先通过二分查找定位到边界，然后通过双向链表，开始遍历即可。 什么是索引覆盖索引覆盖是在联合索引是，查询的内容在联合索引的key上就可以查到，避免了回表。比如联合索引（x，y），现在想通过x条件查询y的内容，就可以使用（x，y）避免再次回表。 MySQL用过什么存储引擎常见的是 Innodb、MyISAM、Memory等。现在默认是 InnoDB。 InnoDB有什么特性 存储：存放在 .frm、.ibd文件； 索引：支持聚簇索引和非聚簇索引； 事务：有undo log 和 redo log，支持事务； 故障恢复：有 redo log，支持故障恢复； 锁：支持表级锁和行级锁； 就针对和 MyISAM 的区别聊 事务有什么特性ACID，原子性、隔离性、一致性、持久性。 可重复读是如何做的，如何实现的通过 MVCC 实现的。 MySQL在事务启动后会为事务生成一个 Read View 快照，Read View 会记录当前事务的id、活跃的事务id列表、活跃事务id的最小值、下一个创建的事务id值，MySQL的行数据也会记录最新修改过这一行的事务id，同样会记录该行的上一版本记录的指针，像一个链表一样。当事务A启动后，事务B启动，事务B的活跃事务id列表就是A和B的事务id，现在事务要对一个行数据进行操作，如果活跃事务id列表的最小值比当前行数据记录的最新修改过这一行的事务id值大，说明最新操作该行数据记录的事务已经提交完成，所以可以对该行进行操作，如果大于下一个创建的事务id值，说明这个最新操作该行数据记录的事务是在当前事务A和B启动之后再启动的事务，那么就不可以对这行数据进行操作。之后要分两种情况讨论，如果在活跃的事务id列表之间，说明有其他事务在操作该行，那么不可以对该行操作，会去找该行记录的上一版本指针，如果不在则说明最新操作该行数据记录的事务已经提交，那么可以对该行进行操作，操作了之后，该行记录会更新最新修改这一样的事务id，同样以链表形式将上一版本记录连接起来。 MVCC 是如何做的上面那一段 如果commit 时，数据版本和快照版本不一致该怎么办回滚 如果加锁的话加的什么锁没懂要问啥，如果是问的可重复读里面的幻读问题，那就是间隙锁，如果带有记录的话是 nextkey lock（间隙锁+记录锁），举个例子： 事务A，执行了 for update 语句 当前读，查询大于等于 5 的记录，这时候事务 B，插入了一条 10 的记录，这样事务 A 如果再次查询的话，前后两次查询就会幻读，所以这时候会引入一个 next key lock，锁住 [5, +∞]的记录，（如果是大于 5 那就是间隙锁）。 Redis常用数据类型String、List、Hash、Set、Sort Set。 可以加上 Hyperloglog、Stream、Bitmap、Geospatial index 。 大 key 问题如何解决如果是 Set 结构则可以修改为 Hash，不要一次性全查。可以将大 key 拆分成多个小 key，读取的时候批量读取拼接即可。而且尽量给大 key 设置较长的过期时间，不让其在缓存中淘汰。删除可以用分批次删除或者异步删除的做法，避免阻塞主线程。 用的 Redis 单机还是集群集群 如何存数据不清楚要问的是底层数据结构还是持久化存储，底层数据结构就将五种基本类型的底层数据结构，持久化就是说 RDB 和 AOF 举个例子，String，如果存储 int 整形，可以用 long 表示，则用 int 来存储，如果是字符串，则小于等于 44 字节用 embstr 存储，其他情况是 raw。 RDB 和 AOF， 就是一个快照，一个追加日志，可能要问一些详细的过程了，还有刷盘策略等等。看具体想问什么。 讲讲主从复制由于 Redis 具有持久化能力（RDB 和 AOF），为了避免单点故障，可以引入主从模式，主机可以进行读写操作，每次的写操作都会同步数据给从机。主从模式主要是为了减轻主机压力以及容灾恢复。接下来大致介绍一下主从复制的流程： 建立好集群及主从关系后，从机会连接主机，发送 SYNC 命令，主机接收到 SYNC 命令后，开始执行 BGSAVE 命令生成 RDB 文件并使用缓冲区记录此后执行的所有写命令，之后会向所有从机发送 RDB ，并在发送期间继续记录被执行的写命令，从机接收到后会载入执行，之后的每一个主机的写命令都会发送到从机执行，如果有断线重连会才用增量复制，补全缓冲区中的命令。 主服务器挂了怎么办哨兵模式会进行监控、选主、通知。首先是如何判断主服务器真的挂了，这里分为主观下线和客观下线，如果哨兵节点接受不到主服务器的信号就会认定为其主观下线，但是哨兵也是集群大的方式部署的，如果超过一半节点认为是主观下线就认定其客观下线。之后哨兵leader会从从机中选取一个新的主节点，进行主从故障转移，让原主下的所有从机都作为新的主机的从机，并且通过发布订阅通知客户端，另外会监控原主机的情况，如果原主机重新上线，那么会作为新主的从。 这里可能还会涉及到哨兵leader、选主策略、主从数据不一致的问题。 主服务器选举是怎么做的首先会找优先级最高的从节点，其次找复制进度最靠前的节点，最后通过id排序。 联想分布式算法原理…… ‍ ‍","link":"/post/614-1z6cwp.html"},{"title":"训练历程","text":"我的数据结构和算法知识体系我的算法体系由hello+代码随想录培养…. 相互补充 K神：力扣（LeetCode）全网阅读量最高博主，发表的《图解算法数据结构》已被订阅 27 万本。 本部分目的是巩固并加强对数据结构的全面理解，大学期间也有多的这门课程，但是内容有限，很难成为体系，算是一个基础的巩固，比起直接手撕LeetCode1000题，我感觉基础巩固更重要，重点记录新的理解。 待办： labuladong Cookbook LeetCode HOT 100 ‍","link":"/post/algorithm-system-1wyqpm.html"},{"title":"多个协程打印相关问题","text":"多个协程打印相关问题​创建三个goroutine，分别输出1 4 7, 2 5 8, 3 6 9, ...... 100， 保证顺序输出1到100​ 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @Author 560463 * @Description //TODO $ 并发：创建三个goroutine，分别输出1 4 7, 2 5 8, 3 6 9, ...... 100， 保证顺序输出1到100, 2种方法 * @Date 2023/10/12 14:36 **/// 实现三个协程并发交替打印数字// 方法一：package mainimport &quot;fmt&quot;var chanDone chan intvar end intvar countA, countB, countC intfunc write(name string, count int, ch, next chan int) { for a := range ch { //fmt.Println(a) count++ fmt.Printf(&quot;协程%s 第%d次打印了%d\\n&quot;, name, count, a) if a &lt; end { next &lt;- a + 1 } else { chanDone &lt;- 0 } }}func main() { ch1, ch2, ch3 := make(chan int, 1), make(chan int, 1), make(chan int, 1) countA, countB, countC = 0, 0, 0 chanDone, end = make(chan int), 100 go write(&quot;A&quot;, countA, ch1, ch2) go write(&quot;B&quot;, countB, ch2, ch3) go write(&quot;C&quot;, countC, ch3, ch1) ch1 &lt;- 1 &lt;-chanDone} ‍ 123456789101112131415161718192021222324252627282930313233343536//方法二：package mainimport &quot;fmt&quot;var wg sync.WaitGroupfunc r(name string, count int, start int, ch1, ch2 chan int) { defer wg.Done() for i := start; i &lt;= 100; { num, ok := &lt;-ch1 if !ok { close(ch2) break } count++ fmt.Printf(&quot;协程%s 第%d次打印了: %d\\n&quot;, name, count, num) i++ ch2 &lt;- i i = i + 2 //如果这样写有个坑：这样写第一次A是打印了1 但是给B的值也是1，虽然后面进行了+3 但是本轮打印时错误的 //ch2 &lt;- i //i = i + 3 }}func main() { chA, chB, chC := make(chan int, 1), make(chan int, 1), make(chan int, 1) countA, countB, countC := 0, 0, 0 wg.Add(3) chA &lt;- 1 go r(&quot;A&quot;, countA, 1, chA, chB) go r(&quot;B&quot;, countB, 2, chB, chC) go r(&quot;C&quot;, countC, 3, chC, chA) wg.Wait()} ‍ 结果： ​​ ​​ ‍ ‍ ​三个协程分别打印100次 cat dog fish​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package mainimport ( &quot;fmt&quot; &quot;sync&quot;)// 三个协程交替打印 cat dog fishvar repeatCount = 100func main() { // wg 用来防止主协程提前先退出 wg := &amp;sync.WaitGroup{} wg.Add(3) chCat := make(chan struct{}, 1) defer close(chCat) chDog := make(chan struct{}, 1) defer close(chDog) chFish := make(chan struct{}, 1) defer close(chFish) go printAnimal(chCat, chDog, &quot;cat&quot;, wg) go printAnimal(chDog, chFish, &quot;dog&quot;, wg) go printAnimal(chFish, chCat, &quot;fish&quot;, wg) chCat &lt;- struct{}{} wg.Wait()}// wg 需要传指针func printAnimal(in, out chan struct{}, s string, wg *sync.WaitGroup) { count := 0 for { &lt;-in count++ fmt.Printf(&quot;第%d次打印%s\\n&quot;, count, s) out &lt;- struct{}{} if count &gt;= repeatCount { wg.Done() return } }} ‍ ​​","link":"/post/concorded-programming-multiple-corporate-printing-zpbnes.html"},{"title":"Docker一文通览","text":"Docker通览 通览篇主要是构建基本的认知，本篇从基础概念、应用、核心底层组成，甚至包括一点点个人理解。 Docker基础与应用 官方文档地址:https://www.docker.com/get-started 中文参考手册:https://docker_practice.gitee.io/zh-cn/ 什么是 Docker官方定义 最新官网首页 1234# 1.官方介绍- We have a complete container solution for you - no matter who you are and where you are on your containerization journey.- 翻译: 我们为你提供了一个完整的容器解决方案,不管你是谁,不管你在哪,你都可以开始容器的的旅程。- 官方定义: docker是一个容器技术。 Docker的起源12345Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护。Docker 项目后来还加入了 Linux 基金会，并成立推动 开放容器联盟（OCI）。Docker 自开源后受到广泛的关注和讨论，至今其 GitHub 项目 已经超过 5 万 7 千个星标和一万多个 fork。甚至由于 Docker 项目的火爆，在 2013 年底，dotCloud 公司决定改名为 Docker。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。 为什么是Docker 在开发的时候，在本机测试环境可以跑，生产环境跑不起来 这里我们拿java Web应用程序举例，我们一个java Web应用程序涉及很多东西，比如jdk、tomcat、mysql等软件环境。当这些其中某一项版本不一致的时候，可能就会导致应用程序跑不起来这种情况。Docker 则将程序以及使用软件环境直接打包在一起，无论在那个机器上保证了环境一致。 优势1: 一致的运行环境,更轻松的迁移 服务器自己的程序挂了，结果发现是别人程序出了问题把内存吃完了，自己程序因为内存不够就挂了 这种也是一种比较常见的情况，如果你的程序重要性不是特别高的话，公司基本上不可能让你的程序独享一台服务器的，这时候你的服务器就会跟公司其他人的程序共享一台服务器，所以不可避免地就会受到其他程序的干扰，导致自己的程序出现问题。Docker就很好解决了环境隔离的问题，别人程序不会影响到自己的程序。 优势2：对进程进行封装隔离,容器与容器之间互不影响,更高效的利用系统资源 公司要弄一个活动，可能会有大量的流量进来，公司需要再多部署几十台服务器 在没有Docker的情况下，要在几天内部署几十台服务器，这对运维来说是一件非常折磨人的事，而且每台服务器的环境还不一定一样，就会出现各种问题，最后部署地头皮发麻。用Docker的话，我只需要将程序打包到镜像，你要多少台服务，我就给力跑多少容器，极大地提高了部署效率。 优势3: 通过镜像复制N多个环境一致容器 Docker和虚拟机区别 关于Docker与虚拟机的区别，我在网上找到的一张图，非常直观形象地展示出来，话不多说，直接上图。 比较上面两张图，我们发现虚拟机是携带操作系统，本身很小的应用程序却因为携带了操作系统而变得非常大，很笨重。Docker是不携带操作系统的，所以Docker的应用就非常的轻巧。另外在调用宿主机的CPU、磁盘等等这些资源的时候，拿内存举例，虚拟机是利用Hypervisor去虚拟化内存，整个调用过程是虚拟内存-&gt;虚拟物理内存-&gt;真正物理内存，但是Docker是利用Docker Engine去调用宿主的的资源，这时候过程是虚拟内存-&gt;真正物理内存。 传统虚拟机 Docker容器 磁盘占用 几个GB到几十个GB左右 几十MB到几百MB左右 CPU内存占用 虚拟操作系统非常占用CPU和内存 Docker引擎占用极低 启动速度 （从开机到运行项目）几分钟 （从开启容器到运行项目）几秒 安装管理 需要专门的运维技术 安装、管理方便 应用部署 每次部署都费时费力 从第二次部署开始轻松简捷 耦合性 多个应用服务安装到一起，容易互相影响 每个应用服务一个容器，达成隔离 系统依赖 无 需求相同或相似的内核，目前推荐是Linux Docker的安装安装docker(centos7.x) 卸载原始docker 12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 安装docker依赖 123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 设置docker的yum源 123$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 安装最新版的docker 1$ sudo yum install docker-ce docker-ce-cli containerd.io 指定版本安装docker 123$ yum list docker-ce --showduplicates | sort -r$ sudo yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io$ sudo yum install docker-ce-18.09.5-3.el7 docker-ce-cli-18.09.5-3.el7 containerd.io 启动docker 12$ sudo systemctl enable docker$ sudo systemctl start docker 关闭docker 1$ sudo systemctl stop docker 测试docker安装 1$ sudo docker run hello-world bash安装(通用所有平台) 在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，CentOS 系统上可以使用这套脚本安装，另外可以通过 --mirror 选项使用国内源进行安装：执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker 的稳定(stable)版本安装在系统中。 12$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 启动docker 12$ sudo systemctl enable docker$ sudo systemctl start docker 创建docker用户组 1$ sudo groupadd docker 将当前用户加入docker组 1$ sudo usermod -aG docker $USER 测试docker安装是否正确 1$ docker run hello-world Docker 的核心架构​​ ​镜像:​ 一个镜像代表一个应用环境,他是一个只读的文件,如 mysql镜像,tomcat镜像,nginx镜像等 ​容器:​ 镜像每次运行之后就是产生一个容器,就是正在运行的镜像,特点就是可读可写 ​仓库:​用来存放镜像的位置,类似于maven仓库,也是镜像下载和上传的位置 ​dockerFile:​docker生成镜像配置文件,用来书写自定义镜像的一些配置 ​tar:​一个对镜像打包的文件,日后可以还原成镜像 ​​ Docker 配置阿里镜像加速服务docker 运行流程 docker配置阿里云镜像加速 访问阿里云登录自己账号查看docker镜像加速服务 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ &quot;registry-mirrors&quot;: [&quot;https://lz2nib3q.mirror.aliyuncs.com&quot;]}EOFsudo systemctl daemon-reloadsudo systemctl restart docker 验证docker的镜像加速是否生效 1234567[root@localhost ~]# docker info .......... 127.0.0.0/8 Registry Mirrors: 'https://lz2nib3q.mirror.aliyuncs.com/' Live Restore Enabled: false Product License: Community Engine Docker的入门应用docker 的第一个程序 docker run hello-world 12345678910111213141516171819202122[root@localhost ~]# docker run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 常用命令启动命令启动docker 1systemctl start docker 关闭docker 1systemctl stop docker 重启docker 1systemctl restart docker docker设置随服务启动而自启动 1systemctl enable docker 辅助命令1234# 1.安装完成辅助命令 docker version -------------------------- 查看docker的信息 docker info -------------------------- 查看更详细的信息 docker --help -------------------------- 帮助命令 Images 镜像命令12345678910111213141516171819202122232425262728293031# 1.查看本机中所有镜像 docker images -------------------------- 列出本地所有镜像 -a 列出所有镜像（包含中间映像层） -q 只显示镜像id# 2.搜索镜像 docker search [options] 镜像名 ------------------- 去dockerhub上查询当前镜像 -s 指定值 列出收藏数不少于指定值的镜像 --no-trunc 显示完整的镜像信息# 3.从仓库下载镜像/拉取仓库镜像到本地 docker pull 镜像名[:TAG|@DIGEST] ----------------- 下载镜像# 4.删除镜像 docker rmi 镜像名 -------------------------- 删除镜像 -f 强制删除 如果删除多个镜像用空格隔开 删除全部镜像 -a 意思为显示全部, -q 意思为只显示ID docker rmi -f $(docker images -aq)# 5.加载本地件tar，恢复为镜像 docker load -i 本地文件 与docker save对应 docker load -i hello.tar# 6.保存镜像为本地文件（打包镜像）** docker save -o 镜像保存位置与名字 镜像名/镜像ID 与docker load对应 docker save -o hello.tar hello-world# 7.上传镜像到docker hub docker push hello:V1 search: ​​ ​docker 安装镜像时如果不指定安装的版本默认最新版本​ Contrainer 容器命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 1.运行容器 docker run 镜像名 -------------------------- 镜像名新建并启动容器 --name 别名为容器起一个名字 -d, --detach 启动守护式容器（在后台启动容器） -p 映射端口号：原始端口号 指定端口号启动 -v, --volume list 给容器挂载数据卷 --name string 指定容器名称 -e, --env list 设置容器环境变量 -i, --interactive 以交互模式运行容器，通常与 -t 同时使用 -m, --memory bytes 容器内存上限 -t, --tty 为容器重新分配一个伪输入终端，通常与 -i 同时使用 -w, --workdir string 指定工作目录 例：docker run -it --name myTomcat -p 8888:8080 tomcat docker run -d --name myTomcat -P tomcat 常用换行的方式： docker run -dit \\ -v $PWD/ql/config:/ql/config \\ -p 5600:5600 \\ --name qinglong \\ --hostname qinglong \\ --restart unless-stopped \\ whyour/qinglong:2.11.3# 2.查看运行的容器 docker ps -------------------------- 列出所有正在运行的容器 -a 正在运行的和历史运行过的容器（已经停止）# 4.删除容器 docker rm -f 容器id和容器名 docker rm -f $(docker ps -aq) -------------------------- 删除所有容器# 5.查看容器内进程 docker top 容器id或者容器名 ------------------ 查看容器内的进程# 6.查看查看容器内部细节 docker inspect 容器id ------------------ 查看容器内部细节# 7.查看容器的运行日志 docker logs [OPTIONS] 容器id或容器名 ------------------ 查看容器日志 -t 加入时间戳 -f 跟随最新的日志打印 --tail 数字 显示最后多少条# 8.进入容器内部 docker exec [options] 容器id 容器内命令 ------------------ 进入容器执行命令 -i 以交互模式运行容器，通常与-t一起使用 -t 分配一个伪终端 shell窗口 bash 常用：docker exec -it redis sh 或者使用docker attach 容器名/容器ID【 不常用 】---没有使用过....# 9.容器和宿主机之间复制文件 docker cp 文件|目录 容器id:容器路径 ----------------- 将宿主机复制到容器内部 docker cp 容器id:容器内资源路径 宿主机目录路径 ----------------- 将容器内资源拷贝到主机上# 10.数据卷(volum)实现与宿主机共享目录 docker run -v 宿主机的路径|任意别名:/容器内的路径 镜像名 注意: 1.如果是宿主机路径必须是绝对路径,宿主机目录会覆盖容器内目录内容 2.如果是别名则会在docker运行容器时自动在宿主机中创建一个目录,并将容器目录文件复制到宿主机中# 11.打包镜像 docker save 镜像名 -o 名称.tar# 12.载入镜像 docker load -i 名称.tar# 13.容器打包成新的镜像 docker commit -m &quot;描述信息&quot; -a &quot;作者信息&quot; （容器id或者名称）打包的镜像名称:标签 补充： 123456789101112# 1.开机自启 docker update --restart=always 容器Id/容器名# 2.更换容器名字 docker rename 容器ID/容器名字 新容器名字# 3.查看docker磁盘 docker system df# 4.查看容器占用内存 docker stats 容器名/容器ID docker stats redis 查看docker磁盘占用 ​​ ​SIZE​（大小）：表示Docker系统中各个部分（镜像、容器、卷、构建缓存）所占用的总磁盘空间大小，以字节为单位。 ​RECLAIMABLE​（可回收大小）：表示可以通过清理或删除不再使用的资源来释放的磁盘空间大小，同样以字节为单位。 docker save 与docker export区别： docker export需要指定容器(container)，不能像docker save那样指定镜像(image)或容器(container)都可以。 docker save保存的是镜像（image），docker export保存的是容器（container）； docker load用来载入镜像包，docker import用来载入容器包，但两者都会恢复为镜像； docker load不能对载入的镜像重命名，而docker import可以为镜像指定新名称。 docker save的应用场景是，如果你的应用是使用docker-compose.yml编排的多个镜像组合，但你要部署的客户服务器并不能连外网。这时，你可以使用docker save将用到的镜像打个包，然后拷贝到客户服务器上使用docker load载入 。—这一点​***深有体会*** docker export的应用场景主要用来制作基础镜像，比如你从一个ubuntu镜像启动一个容器，然后安装一些软件和进行一些设置后，使用docker export保存为一个基础镜像。然后，把这个镜像分发给其他人使用，比如作为基础的开发环境。—这个确实没有应用过 docker的镜像原理镜像是什么？ 镜像是一种轻量级的，可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时所需的库、环境变量和配置文件。 为什么一个镜像会那么大？ 镜像就是花卷 UnionFS（联合文件系统）: Union文件系统是一种分层，轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。Union文件系统是Docker镜像的基础。这种文件系统特性:就是一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 。 Docker镜像原理 docker的镜像实际是由一层一层的文件系统组成。 bootfs（boot file system）主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。在docker镜像的最底层就是bootfs。这一层与Linux/Unix 系统是一样的，包含boot加载器（bootloader）和内核（kernel）。当boot加载完,后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时会卸载bootfs。 rootfs（root file system），在bootfs之上，包含的就是典型的linux系统中的/dev，/proc，/bin，/etc等标准的目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu/CentOS等等。 我们平时安装进虚拟机的centos都有1到几个GB，为什么docker这里才200MB？对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令，工具，和程序库就可以了，因为底层直接使用Host的Kernal，自己只需要提供rootfs就行了。由此可见不同的linux发行版，他们的bootfs是一致的，rootfs会有差别。因此不同的发行版可以共用bootfs。 为什么docker镜像要采用这种分层结构呢? 最大的一个好处就是资源共享 比如：有多个镜像都是从相同的base镜像构建而来的，那么宿主机只需在磁盘中保存一份base镜像。同时内存中也只需要加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。Docker镜像都是只读的。当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称为容器层，容器层之下都叫镜像层。 Docker安装常用服务安装mysql12345678910111213141516171819202122232425262728# 1.拉取mysql镜像到本地 docker pull mysql:tag (tag不加默认最新版本) # 2.运行mysql服务 docker run --name mysql -e MYSQL_ROOT_PASSWORD=root -d mysql:tag --没有暴露外部端口外部不能连接 docker run --name mysql -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -d mysql:tag --没有暴露外部端口# 3.进入mysql容器 docker exec -it 容器名称|容器id bash# 4.外部查看mysql日志 docker logs 容器名称|容器id# 5.使用自定义配置参数 docker run --name mysql -v /root/mysql/conf.d:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=root -d mysql:tag# 6.将容器数据位置与宿主机位置挂载保证数据安全 docker run --name mysql -v /root/mysql/data:/var/lib/mysql -v /root/mysql/conf.d:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -d mysql:tag# 7.通过其他客户端访问 如在window系统|macos系统使用客户端工具访问 # 8.将mysql数据库备份为sql文件 docker exec mysql|容器id sh -c 'exec mysqldump --all-databases -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;' &gt; /root/all-databases.sql --导出全部数据 docker exec mysql sh -c 'exec mysqldump --databases 库表 -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;' &gt; /root/all-databases.sql --导出指定库数据 docker exec mysql sh -c 'exec mysqldump --no-data --databases 库表 -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;' &gt; /root/all-databases.sql --导出指定库数据不要数据# 9.执行sql文件到mysql中 docker exec -i mysql sh -c 'exec mysql -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;' &lt; /root/xxx.sql 安装Redis服务12345678910111213141516171819202122232425262728# 1.在docker hub搜索redis镜像 docker search redis# 2.拉取redis镜像到本地 docker pull redis# 3.启动redis服务运行容器 docker run --name redis -d redis:tag (没有暴露外部端口) docker run --name redis -p 6379:6379 -d redis:tag (暴露外部宿主机端口为6379进行连接) # 4.查看启动日志 docker logs -t -f 容器id|容器名称# 5.进入容器内部查看 docker exec -it 容器id|名称 bash # 6.加载外部自定义配置启动redis容器 默认情况下redis官方镜像中没有redis.conf配置文件 需要去官网下载指定版本的配置文件 1. wget http://download.redis.io/releases/redis-5.0.8.tar.gz 下载官方安装包 2. 将官方安装包中配置文件进行复制到宿主机指定目录中如 /root/redis/redis.conf文件 3. 修改需要自定义的配置 bind 0.0.0.0 开启远程权限 appenonly yes 开启aof持久化 4. 加载配置启动 docker run --name redis -v /root/redis:/usr/local/etc/redis -p 6379:6379 -d redis redis-server /usr/local/etc/redis/redis.conf # 7.将数据目录挂在到本地保证数据安全 docker run --name redis -v /root/redis/data:/data -v /root/redis/redis.conf:/usr/local/etc/redis/redis.conf -p 6379:6379 -d redis redis-server /usr/local/etc/redis/redis.conf 安装Nginx123456789101112131415161718192021222324252627# 1.在docker hub搜索nginx docker search nginx# 2.拉取nginx镜像到本地 [root@localhost ~]# docker pull nginx Using default tag: latest latest: Pulling from library/nginx afb6ec6fdc1c: Pull complete b90c53a0b692: Pull complete 11fa52a0fdc0: Pull complete Digest: sha256:30dfa439718a17baafefadf16c5e7c9d0a1cde97b4fd84f63b69e13513be7097 Status: Downloaded newer image for nginx:latest docker.io/library/nginx:latest# 3.启动nginx容器 docker run -p 80:80 --name nginx01 -d nginx# 4.进入容器 docker exec -it nginx01 /bin/bash 查找目录: whereis nginx 配置文件: /etc/nginx/nginx.conf# 5.复制配置文件到宿主机 docker cp nginx01(容器id|容器名称):/etc/nginx/nginx.conf 宿主机名录# 6.挂在nginx配置以及html到宿主机外部 docker run --name nginx02 -v /root/nginx/nginx.conf:/etc/nginx/nginx.conf -v /root/nginx/html:/usr/share/nginx/html -p 80:80 -d nginx 安装Tomcat123456789101112131415# 1.在docker hub搜索tomcat docker search tomcat# 2.下载tomcat镜像 docker pull tomcat# 3.运行tomcat镜像 docker run -p 8080:8080 -d --name mytomcat tomcat# 4.进入tomcat容器 docker exec -it mytomcat /bin/bash# 5.将webapps目录挂载在外部 docker run -p 8080:8080 -v /root/webapps:/usr/local/tomcat/webapps -d --name mytomcat tomcat 安装MongoDB数据库12345678910111213141516171819# 1.运行mongDB docker run -d -p 27017:27017 --name mymongo mongo ---无须权限 docker logs -f mymongo --查看mongo运行日志# 2.进入mongodb容器 docker exec -it mymongo /bin/bash 直接执行mongo命令进行操作# 3.常见具有权限的容器 docker run --name mymongo -p 27017:27017 -d mongo --auth# 4.进入容器配置用户名密码 mongo use admin 选择admin库 db.createUser({user:&quot;root&quot;,pwd:&quot;root&quot;,roles:[{role:'root',db:'admin'}]}) //创建用户,此用户创建成功,则后续操作都需要用户认证 exit# 5.将mongoDB中数据目录映射到宿主机中 docker run -d -p 27017:27017 -v /root/mongo/data:/data/db --name mymongo mongo 安装ElasticSearch 注意:​调高JVM线程数限制数量 拉取镜像运行elasticsearch123456# 1.dockerhub 拉取镜像 docker pull elasticsearch:6.4.2# 2.查看docker镜像 docker images# 3.运行docker镜像 docker run -p 9200:9200 -p 9300:9300 elasticsearch:6.4.2 启动出现如下错误 预先配置123456789# 1.在centos虚拟机中，修改配置sysctl.conf vim /etc/sysctl.conf# 2.加入如下配置 vm.max_map_count=262144 # 3.启用配置 sysctl -p 注：这一步是为了防止启动容器时，报出如下错误： bootstrap checks failed max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] 启动EleasticSearch容器1234# 0.复制容器中data目录到宿主机中 docker cp 容器id:/usr/share/share/elasticsearch/data /root/es# 1.运行ES容器 指定jvm内存大小并指定ik分词器位置 docker run -d --name es -p 9200:9200 -p 9300:9300 -e ES_JAVA_OPTS=&quot;-Xms128m -Xmx128m&quot; -v /root/es/plugins:/usr/share/elasticsearch/plugins -v /root/es/data:/usr/share/elasticsearch/data elasticsearch:6.4.2 安装IK分词器123456789101112131415161718192021222324252627# 1.下载对应版本的IK分词器 wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.4.2/elasticsearch-analysis-ik-6.4.2.zip# 2.解压到plugins文件夹中 yum install -y unzip unzip -d ik elasticsearch-analysis-ik-6.4.2.zip# 3.添加自定义扩展词和停用词 cd plugins/elasticsearch/config vim IKAnalyzer.cfg.xml &lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;ext_dict.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;ext_stopwords.dic&lt;/entry&gt; &lt;/properties&gt;# 4.在ik分词器目录下config目录中创建ext_dict.dic文件 编码一定要为UTF-8才能生效 vim ext_dict.dic 加入扩展词即可# 5. 在ik分词器目录下config目录中创建ext_stopword.dic文件 vim ext_stopwords.dic 加入停用词即可# 6.重启容器生效 docker restart 容器id# 7.将此容器提交成为一个新的镜像 docker commit -a=&quot;xiaochen&quot; -m=&quot;es with IKAnalyzer&quot; 容器id xiaochen/elasticsearch:6.4.2 安装Kibana12345# 1.下载kibana镜像到本地 docker pull kibana:6.4.2# 2.启动kibana容器 docker run -d --name kibana -e ELASTICSEARCH_URL=http://10.15.0.3:9200 -p 5601:5601 kibana:6.4.2 Docker中出现如下错误解决方案12[root@localhost ~]# docker search mysql 或者 docker pull 这些命令无法使用Error response from daemon: Get https://index.docker.io/v1/search?q=mysql&amp;n=25: x509: certificate has expired or is not yet valid 注意:这个错误的原因在于是系统的时间和docker hub时间不一致,需要做系统时间与网络时间同步 1234567# 1.安装时间同步 sudo yum -y install ntp ntpdate# 2.同步时间 sudo ntpdate cn.pool.ntp.org# 3.查看本机时间 date# 4.从新测试 Dockerfile什么是DockerfileDockerfile可以认为是Docker镜像的描述文件，是由一系列命令和参数构成的脚本。主要作用是用来构建docker镜像的构建文件。 ​​ 通过架构图可以看出通过DockerFile可以直接构建镜像 Dockerfile解析过程 Dockerfile的保留命令官方说明:https://docs.docker.com/engine/reference/builder/ 保留字 作用 FROM 当前镜像是基于哪个镜像的 第一个指令必须是FROM MAINTAINER 镜像维护者的姓名和邮箱地址 RUN 构建镜像时需要运行的指令 EXPOSE 当前容器对外暴露出的端口号 WORKDIR 指定在创建容器后，终端默认登录进来的工作目录，一个落脚点 ENV 用来在构建镜像过程中设置环境变量 ADD 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar包 COPY 类似于ADD，拷贝文件和目录到镜像中​将从构建上下文目录中&lt;原路径&gt;的文件/目录复制到新的一层的镜像内的&lt;目标路径&gt;位置 VOLUME 容器数据卷，用于数据保存和持久化工作 CMD 指定一个容器启动时要运行的命令​Dockerfile中可以有多个CMD指令，但只有最后一个生效，CMD会被docker run之后的参数替换 ENTRYPOINT 指定一个容器启动时要运行的命令​ENTRYPOINT的目的和CMD一样，都是在指定容器启动程序及其参数 FROM 命令 基于那个镜像进行构建新的镜像,在构建时会自动从docker hub拉取base镜像 必须作为Dockerfile的第一个指令出现 语法: 123FROM &lt;image&gt;FROM &lt;image&gt;[:&lt;tag&gt;] 使用版本不写为latestFROM &lt;image&gt;[@&lt;digest&gt;] 使用摘要 MAINTAINER 命令 镜像维护者的姓名和邮箱地址[废弃] 语法: 1MAINTAINER &lt;name&gt; RUN 命令 RUN指令将在当前映像之上的新层中执行任何命令并提交结果。生成的提交映像将用于Dockerfile中的下一步 语法: 12345RUN &lt;command&gt; (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows)RUN echo helloRUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec form)RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;] EXPOSE 命令 用来指定构建的镜像在运行为容器时对外暴露的端口 语法: 12EXPOSE 80/tcp 如果没有显示指定则默认暴露都是tcpEXPOSE 80/udp CMD 命令 用来为启动的容器指定执行的命令,在Dockerfile中只能有一条CMD指令。如果列出多个命令，则只有最后一个命令才会生效。 注意: Dockerfile中只能有一条CMD指令。如果列出多个命令，则只有最后一个命令才会生效。 语法: 123CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec form, this is the preferred form)CMD [&quot;param1&quot;,&quot;param2&quot;] (as default parameters to ENTRYPOINT)CMD command param1 param2 (shell form) WORKDIR 命令 用来为Dockerfile中的任何RUN、CMD、ENTRYPOINT、COPY和ADD指令设置工作目录。如果WORKDIR不存在，即使它没有在任何后续Dockerfile指令中使用，它也将被创建。 语法: 123456WORKDIR /path/to/workdirWORKDIR /aWORKDIR bWORKDIR c`注意:WORKDIR指令可以在Dockerfile中多次使用。如果提供了相对路径，则该路径将与先前WORKDIR指令的路径相对` ENV 命令 用来为构建镜像设置环境变量。这个值将出现在构建阶段中所有后续指令的环境中。 语法： 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key&gt;=&lt;value&gt; ... ADD 命令 用来从context上下文复制新文件、目录或远程文件url，并将它们添加到位于指定路径的映像文件系统中。 语法: 12345ADD hom* /mydir/ 通配符添加多个文件ADD hom?.txt /mydir/ 通配符添加ADD test.txt relativeDir/ 可以指定相对路径ADD test.txt /absoluteDir/ 也可以指定绝对路径ADD url COPY 命令 用来将context目录中指定文件复制到镜像的指定目录中 语法: 12COPY src destCOPY [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] VOLUME 命令 用来定义容器运行时可以挂在到宿主机的目录 语法: 1VOLUME [&quot;/data&quot;] ENTRYPOINT命令 用来指定容器启动时执行命令和CMD类似 语法: 12 [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]ENTRYPOINT command param1 param2 ENTRYPOINT指令，往往用于设置容器启动后的第一个命令，这对一个容器来说往往是固定的。CMD指令，往往用于设置容器启动的第一个命令的默认参数，这对一个容器来说可以是变化的。 ENTRYPOINT命令Dockerfile构建springboot项目部署准备springboot可运行项目 将可运行项目放入linux虚拟机中 编写Dockerfile123456FROM openjdk:8WORKDIR /emsADD ems.jar /emsEXPOSE 8989ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;]CMD [&quot;ems.jar&quot;] 构建镜像1[root@localhost ems]# docker build -t ems . 运行镜像1[root@localhost ems]# docker run -p 8989:8989 ems 访问项目1http://10.15.0.8:8989/ems/login.html 高级网络配置说明当 Docker 启动时，会自动在主机上创建一个 docker0 虚拟网桥，实际上是 Linux 的一个 bridge，可以理解为一个软件交换机。它会在挂载到它的网口之间进行转发。 同时，Docker 随机分配一个本地未占用的私有网段（在 RFC1918 中定义）中的一个地址给 docker0 接口。比如典型的 172.17.42.1，掩码为 255.255.0.0。此后启动的容器内的网口也会自动分配一个同一网段（172.17.0.0/16）的地址。 当创建一个 Docker 容器的时候，同时会创建了一对 veth pair 接口（当数据包发送到一个接口时，另外一个接口也可以收到相同的数据包）。这对接口一端在容器内，即 eth0；另一端在本地并被挂载到 docker0 网桥，名称以 veth 开头（例如 vethAQI2QT）。通过这种方式，主机可以跟容器通信，容器之间也可以相互通信。Docker 就创建了在主机和所有容器之间一个虚拟共享网络。 查看网络信息1# docker network ls 创建一个网桥1# docker network create -d bridge 网桥名称 删除一个网桥1# docker network rm 网桥名称 容器之前使用网络通信12# 1.查询当前网络配置- docker network ls 1234NETWORK ID NAME DRIVER SCOPE8e424e5936b7 bridge bridge local17d974db02da docker_gwbridge bridge locald6c326e433f7 host host local 12# 2.创建桥接网络- docker network create -d bridge info 12345678[root@centos ~]# docker network create -d bridge info6e4aaebff79b1df43a064e0e8fdab08f52d64ce34db78dd5184ce7aaaf550a2f[root@centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPE8e424e5936b7 bridge bridge local17d974db02da docker_gwbridge bridge locald6c326e433f7 host host local6e4aaebff79b info bridge local 1234# 3.启动容器指定使用网桥- docker run -d -p 8890:80 --name nginx001 --network info nginx - docker run -d -p 8891:80 --name nginx002 --network info nginx `注意:一旦指定网桥后--name指定名字就是主机名,多个容器指定在同一个网桥时,可以在任意一个容器中使用主机名与容器进行互通` 12345678910111213141516[root@centos ~]# docker run -d -p 8890:80 --name nginx001 --network info nginx c315bcc94e9ddaa36eb6c6f16ca51592b1ac8bf1ecfe9d8f01d892f3f10825fe[root@centos ~]# docker run -d -p 8891:80 --name nginx002 --network info nginxf8682db35dd7fb4395f90edb38df7cad71bbfaba71b6a4c6e2a3a525cb73c2a5[root@centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf8682db35dd7 nginx &quot;/docker-entrypoint.…&quot; 3 seconds ago Up 2 seconds 0.0.0.0:8891-&gt;80/tcp nginx002c315bcc94e9d nginx &quot;/docker-entrypoint.…&quot; 7 minutes ago Up 7 minutes 0.0.0.0:8890-&gt;80/tcp nginx001b63169d43792 mysql:5.7.19 &quot;docker-entrypoint.s…&quot; 7 minutes ago Up 7 minutes 3306/tcp mysql_mysql.1.s75qe5kkpwwttyf0wrjvd2cda[root@centos ~]# docker exec -it f8682db35dd7 /bin/bashroot@f8682db35dd7:/# curl http://nginx001&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;..... 高级数据卷配置说明数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性： 数据卷 可以在容器之间共享和重用 对 数据卷 的修改会立马生效 对 数据卷 的更新，不会影响镜像 数据卷 默认会一直存在，即使容器被删除 注意：数据卷 的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会复制到数据卷中（仅数据卷为空时会复制）。 创建数据卷12[root@centos ~]# docker volume create my-volmy-vol 查看数据卷123456789101112[root@centos ~]# docker volume inspect my-vol [ { &quot;CreatedAt&quot;: &quot;2020-11-25T11:43:56+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: {}, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;, &quot;Name&quot;: &quot;my-vol&quot;, &quot;Options&quot;: {}, &quot;Scope&quot;: &quot;local&quot; }] 挂载数据卷1234567891011121314[root@centos ~]# docker run -d -P --name web -v my-vol:/usr/share/nginx/html nginx[root@centos ~]# docker inspect web &quot;Mounts&quot;: [ { &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;my-vol&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;, &quot;Destination&quot;: &quot;/usr/share/nginx/html&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;z&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; } ], 删除数据卷1docker volume rm my-vol Docker Compose简介Compose 项目是 Docker 官方的开源项目，负责实现对 Docker 容器集群的快速编排。从功能上看，跟 OpenStack 中的 Heat 十分类似。 其代码目前在 https://github.com/docker/compose 上开源。 Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig。 通过第一部分中的介绍，我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。 Compose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理。 安装与卸载1.linux 在 Linux 上的也安装十分简单，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。例如，在 Linux 64 位系统上直接下载对应的二进制包。 12$ sudo curl -L https://github.com/docker/compose/releases/download/1.25.5/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 2.macos、window Compose 可以通过 Python 的包管理工具 pip 进行安装，也可以直接下载编译好的二进制文件使用，甚至能够直接在 Docker 容器中运行。Docker Desktop for Mac/Windows 自带 docker-compose 二进制文件，安装 Docker 之后可以直接使用。 3.bash命令补全1$ curl -L https://raw.githubusercontent.com/docker/compose/1.25.5/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose 4.卸载 如果是二进制包方式安装的，删除二进制文件即可。 1$ sudo rm /usr/local/bin/docker-compose 5.测试安装成功12$ docker-compose --version docker-compose version 1.25.5, build 4667896b docker compose使用1# 1.相关概念 首先介绍几个术语。 服务 (service)：一个应用容器，实际上可以运行多个相同镜像的实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元。∂一个项目可以由多个服务（容器）关联而成，Compose 面向项目进行管理。 1# 2.场景 最常见的项目是 web 网站，该项目应该包含 web 应用和缓存。 springboot应用 mysql服务 redis服务 elasticsearch服务 ……. 12# 3.docker-compose模板- 参考文档:https://docker_practice.gitee.io/zh-cn/compose/compose_file.html 12345678910111213141516171819202122232425262728293031version: &quot;3.0&quot;services: mysqldb: image: mysql:5.7.19 container_name: mysql ports: - &quot;3306:3306&quot; volumes: - /root/mysql/conf:/etc/mysql/conf.d - /root/mysql/logs:/logs - /root/mysql/data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: root networks: - ems depends_on: - redis redis: image: redis:4.0.14 container_name: redis ports: - &quot;6379:6379&quot; networks: - ems volumes: - /root/redis/data:/data command: redis-server networks: ems: 12# 4.通过docker-compose运行一组容器- 参考文档:https://docker_practice.gitee.io/zh-cn/compose/commands.html 12[root@centos ~]# docker-compose up //前台启动一组服务[root@centos ~]# docker-compose up -d //后台启动一组服务 docker-compose 模板文件模板文件是使用 Compose 的核心，涉及到的指令关键字也比较多。但大家不用担心，这里面大部分指令跟 docker run 相关参数的含义都是类似的。 默认的模板文件名称为 docker-compose.yml，格式为 YAML 格式。 123456789version: &quot;3&quot;services: webapp: image: examples/web ports: - &quot;80:80&quot; volumes: - &quot;/data&quot; 注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。 如果使用 build 指令，在 Dockerfile 中设置的选项(例如：CMD, EXPOSE, VOLUME, ENV 等) 将会自动被获取，无需在 docker-compose.yml 中重复设置。 下面分别介绍各个指令的用法。 build指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。 12345version: '3'services: webapp: build: ./dir 你也可以使用 context 指令指定 Dockerfile 所在文件夹的路径。 使用 dockerfile 指令指定 Dockerfile 文件名。 使用 arg 指令指定构建镜像时的变量。 123456789version: '3'services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 command覆盖容器启动后默认执行的命令。 1command: echo &quot;hello world&quot; container_name指定容器名称。默认将会使用 项目名称_服务名称_序号 这样的格式。 1container_name: docker-web-container 注意: 指定容器名称后，该服务将无法进行扩展（scale），因为 Docker 不允许多个容器具有相同的名称。 depends_on解决容器的依赖、启动先后的问题。以下例子中会先启动 redis db 再启动 web 1234567891011121314version: '3'services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 注意：web 服务不会等待 redis db 「完全启动」之后才启动。 env_file从文件中获取环境变量，可以为单独的文件路径或列表。 如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file 中变量的路径会基于模板文件路径。 如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。 123456env_file: .envenv_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境变量文件中每一行必须符合格式，支持 # 开头的注释行。 12# common.env: Set development environmentPROG_ENV=development environment设置环境变量。你可以使用数组或字典两种格式。 只给定名称的变量会自动获取运行 Compose 主机上对应变量的值，可以用来防止泄露不必要的数据。 1234567environment: RACK_ENV: development SESSION_SECRET:environment: - RACK_ENV=development - SESSION_SECRET 如果变量名称或者值中用到 true|false，yes|no 等表达 布尔 含义的词汇，最好放到引号里，避免 YAML 自动解析某些内容为对应的布尔语义。这些特定词汇，包括 1y|Y|yes|Yes|YES|n|N|no|No|NO|true|True|TRUE|false|False|FALSE|on|On|ON|off|Off|OFF healthcheck通过命令检查容器是否健康运行。 12345healthcheck: test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost&quot;] interval: 1m30s timeout: 10s retries: 3 image指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉取这个镜像。 123image: ubuntuimage: orchardup/postgresqlimage: a4bc65fd networks配置容器连接的网络。 1234567891011version: &quot;3&quot;services: some-service: networks: - some-network - other-networknetworks: some-network: other-network: ports暴露端口信息。 使用宿主端口：容器端口 (HOST:CONTAINER) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。 12345ports: - &quot;3000&quot; - &quot;8000:8000&quot; - &quot;49100:22&quot; - &quot;127.0.0.1:8001:8001&quot; *注意：当使用 ​HOST:CONTAINER*​ * 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 ​YAML*​ * 会自动解析 ​xx:yy​ * 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。 sysctls配置容器内核参数。 1234567sysctls: net.core.somaxconn: 1024 net.ipv4.tcp_syncookies: 0sysctls: - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 ulimits指定容器的 ulimits 限制值。 例如，指定最大进程数为 65535，指定文件句柄数为 20000（软限制，应用可以随时修改，不能超过硬限制） 和 40000（系统硬限制，只能 root 用户提高）。 12345ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 volumes数据卷所挂载路径设置。可以设置为宿主机路径(HOST:CONTAINER)或者数据卷名称(VOLUME:CONTAINER)，并且可以设置访问模式 （HOST:CONTAINER:ro）。 该指令中路径支持相对路径。 1234volumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 如果路径为数据卷名称，必须在文件中配置数据卷。 12345678910version: &quot;3&quot;services: my_src: image: mysql:8.0 volumes: - mysql_data:/var/lib/mysqlvolumes: mysql_data: docker-compose 常用命令1. 命令对象与格式对于 Compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到命令影响。 执行 docker-compose [COMMAND] --help 或者 docker-compose help [COMMAND] 可以查看具体某个命令的使用格式。 docker-compose 命令的基本的使用格式是 1docker-compose [-f=&lt;arg&gt;...] [options] [COMMAND] [ARGS...] 2. 命令选项 -f, --file FILE 指定使用的 Compose 模板文件，默认为 docker-compose.yml，可以多次指定。 -p, --project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名。 --x-networking 使用 Docker 的可拔插网络后端特性 --x-network-driver DRIVER 指定网络后端的驱动，默认为 bridge --verbose 输出更多调试信息。 -v, --version 打印版本并退出。 3.命令使用说明up格式为 docker-compose up [options] [SERVICE...]。 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。 链接的服务都将会被自动启动，除非已经处于运行状态。 可以说，大部分时候都可以直接通过该命令来启动一个项目。 默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。 当通过 Ctrl-C 停止命令时，所有容器将会停止。 如果使用 docker-compose up -d，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。 默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容 down 此命令将会停止 up 命令所启动的容器，并移除网络 exec 进入指定的容器。 ps格式为 docker-compose ps [options] [SERVICE...]。 列出项目中目前的所有容器。 选项： -q 只打印容器的 ID 信息。 restart格式为 docker-compose restart [options] [SERVICE...]。 重启项目中的服务。 选项： -t, --timeout TIMEOUT 指定重启前停止容器的超时（默认为 10 秒）。 rm格式为 docker-compose rm [options] [SERVICE...]。 删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。 选项： -f, --force 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。 -v 删除容器所挂载的数据卷。 start格式为 docker-compose start [SERVICE...]。 启动已经存在的服务容器。 stop格式为 docker-compose stop [options] [SERVICE...]。 停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器。 选项： -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。 top查看各个服务容器内运行的进程。 unpause格式为 docker-compose unpause [SERVICE...]。 恢复处于暂停状态中的服务。 可视化管理工具安装Portainer官方安装说明：https://www.portainer.io/installation/ 123456789[root@ubuntu1804 ~]#docker pull portainer/portainer[root@ubuntu1804 ~]#docker volume create portainer_dataportainer_data[root@ubuntu1804 ~]#docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer20db26b67b791648c2ef6aee444a5226a9c897ebcf0160050e722dbf4a4906e3[root@ubuntu1804 ~]#docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES20db26b67b79 portainer/portainer &quot;/portainer&quot; 5 seconds ago Up 4 seconds 0.0.0.0:8000-&gt;8000/tcp, 0.0.0.0:9000-&gt;9000/tcp portainer 登录和使用Portainer 用浏览器访问：http://localhost:9000 ​ 使用过程… Docker 核心原理从表层来看docker的组成部分是由：仓库、镜像、容器三个部分组成，这是基础层面的体现 犹如MySQL事务的特性：ACID（原子性、一致性、隔离性、持久性）但是维持ACID背后的机制/原理是什么？ 答： 持久性是通过 redo log （重做日志）来保证的； 原子性是通过 undo log（回滚日志） 来保证的； 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的； 一致性则是通过持久性+原子性+隔离性来保证； 那么类推docker的三个组成部分或者三个核心概念背后的三个原理是什么支持的？ 答：Namespace、Cgroups、rootfs。 （联想：既然docker中的技术是参考Linux内核实现的，那么这其中究竟有多少联系？） Namespace，做隔离，让进程只能看到Namespace中的世界； Cgroups，做限制，让这个“世界”围着一个看不见的墙。 rootfs，做文件系统，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。 注意此处 Linux Namespace 不要和 k8s Namespace 概念混淆： Linux Namespace 机制：用于资源和视图隔离，使宿主机看不到容器内的资源，容器也看不到其他容器内的资源，实现不同应用的视图隔离，避免干扰 k8s Namespace 机制：就是用户资源的隔离，为了便于管理 k8s 自身的资源 Namespace本质就是一个障眼法，进入容器后ps看到不同的pid，其实就是namespace的幻化，本质上容器就是一个运行的进程，其它的进程实际是pid为1的子进程，namespace提供了许多种的障眼法。 ​ namespace本质是怎么建立的？ 每一次创建容器的时候本质是Linux系统的fork的调用，在fork调用时会传入一些参数，这个会对Linux内核进行控制生成新的namespace rootfs本质是根文件系统，挂载在容器根目录上，为容器提供隔离后执行环境的文件系统，即容器镜像。 容器的rootfs由三部分组成，1：只读层、2：可读写层、3：init层 只读层:都以增量的方式分别包含了操作系统的一部分。 可读写：就是专门用来存放你修改 rootfs 后产生的增量，无论是增、删、改，都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用 docker commit 和 push 指令，保存这个被修改过的可读写层，并上传到 Docker Hub 上，供其他人使用；而与此同时，原先的只读层里的内容则不会有任何变化。这，就是增量 rootfs 的好处。 Init 层：是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。 Cgroups 虽然容器内的第 1 号进程在“障眼法”的干扰下只能看到容器里的情况，但是宿主机上，它作为第 100 号进程与其他所有进程之间依然是平等的竞争关系。这就意味着，虽然第 100 号进程表面上被隔离了起来，但是它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器）占用的。当然，这个 100 号进程自己也可能把所有资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。 而 Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。 Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。 参考： 容器基础之三大基石","link":"/post/docker-z1rrgi5.html"},{"title":"领域算法知识体系","text":"领域算法 最近一直有一个概念在脑子中循环：程序设计 = 数据结构 + 算法 基于目前的认知我想把算法分为常规算法思想和领域算法：常规算法思想如：分治、动态规划、贪心、二分、搜索等；本篇则是领域算法。 不论哪种技术栈或者组件都和算法离不开，比如gin的路由基数树、负载均衡各种算法、分布式一致性算法、和加密相关的安全算法、分布式ID雪花算法，因此准备抽一些时间好好整理并学习下这部分的内容，这部分我称之为领域算法，形成一个完整的知识体系，不求多深入、多底层，但求个人程度上的理解,快速形成知识体系。 ‍ ​​ ‍ 重点关注：负载均衡、雪花、一致性以及一点点大数据相关的，希望形成自己的理解","link":"/post/field-algorithm-1sbsuf.html"},{"title":"全局理解：微服务架构演进","text":"全局理解：微服务架构演进 本文参考并引用楼仔的文章作为基础并结合自己的一些理解，做一下微服务演进的链路，后续还会结合补充其它一些好的见解，希望更好地从全局角度理解微服务，但不做技术细节深究。 思考点：单体、单体到微服务的转变、微服务的瓶颈； 微服务涉及到的技术手段、工具组件以及为什么用这些工具 提升点：培养微服务的宏观意识，不想当架构师的开发者不是一个好的自律人。 以电商为例的演进前言本文将介绍微服务架构和相关的组件，介绍他们是什么，以及为什么要使用微服务架构和这些组件。 从单体应用到微服务并不是一蹴而就的，这是一个逐渐演变的过程。本文将以一个网上超市应用为例来说明这一过程。 创业思维-&gt;产品卖点-&gt;产生功能需求和业务-&gt;技术驱动-&gt;技术迭代-&gt;产品迭代升级 ​​ 最初的需求几年前，小明和小皮一起创业做网上超市。小明负责程序开发，小皮负责其他事宜。当时互联网还不发达，网上超市还是蓝海。只要功能实现了就能随便赚钱。所以他们的需求很简单，只需要一个网站挂在公网，用户能够在这个网站上浏览商品、购买商品；另外还需一个管理后台，可以管理商品、用户、以及订单数据 。 我们整理一下功能清单： 1）网站 用户注册、登录功能 商品展示 下单 2）管理后台 ​用户管理 商品管理 订单管理 由于需求简单，小明左手右手一个慢动作，网站就做好了。管理后台出于安全考虑，不和网站做在一起，小明右手左手慢动作重播，管理网站也做好了。总体架构图如下： ​​ 小明挥一挥手，找了家云服务部署上去，网站就上线了。上线后好评如潮，深受各类肥宅喜爱。小明小皮美滋滋地开始躺着收钱。 业务发展：单体瓶颈出现好景不长，没过几天，各类网上超市紧跟着拔地而起，对小明小皮造成了强烈的冲击。 在竞争的压力下，小明小皮决定开展一些营销手段： ​​开展促销活动。比如元旦全场打折，春节买二送一，情人节狗粮优惠券等等。 拓展渠道，新增移动端营销。除了网站外，还需要开发移动端APP，微信小程序等。 精准营销。利用历史数据对用户进行分析，提供个性化服务。 …… 这些​​活动都需要程序开发的支持。小明拉了同学小红加入团队。小红负责数据分析以及移动端相关开发。小明负责促销活动相关功能的开发。 因为开发任务比较紧迫，小明小红没有好好规划整个系统的架构，随便拍了拍脑袋，决定把促销管理和数据分析放在管理后台里，微信和移动端APP另外搭建。通宵了几天后，新功能和新应用基本完工。这时的架构图如下： ​​ 这一阶段存在很多不合理的地方： 网站和移动端应用有很多相同***业务逻辑的重复代码***。 数据有时候通过数据库共享，有时候通过接口调用传输。接口调用关系杂乱。 单个应用为了给其他应用提供接口，渐渐地越改越大，包含了很多本来就不属于它的逻辑。应用边界模糊，功能归属混乱。 管理后台在一开始的设计中保障级别较低。加入数据分析和促销管理相关功能后出现性能瓶颈，影响了其他应用。 数据库**表结构被多个应用依赖**，无法重构和优化。 单库瓶颈： 所有应用都在一个数据库上操作，数据库出现性能瓶颈。特别是数据分析跑起来的时候，数据库性能急剧下降。 开发、测试、部署、维护愈发困难。即使只改动一个小功能，也需要整个应用一起发布。有时候发布会不小心带上了一些未经测试的代码，或者修改了一个功能后，另一个意想不到的地方出错了。为了减轻发布可能产生的问题的影响和线上业务停顿的影响，所有应用都要在凌晨三四点执行发布。发布后为了验证应用正常运行，还得盯到第二天白天的用户高峰期……**需要CICD拯救** 团队出现推诿扯皮现象。关于一些公用的功能应该建设在哪个应用上的问题常常要争论很久，最后要么干脆各做各的，或者随便放个地方但是都不维护。需要职责单一 尽管有着诸多问题，但也不能否认这一阶段的成果：快速地根据业务变化建设了系统。不过紧迫且繁重的任务容易使人陷入局部、短浅的思维方式，从而做出妥协式的决策。在这种架构中，每个人都只关注在自己的一亩三分地，缺乏全局的、长远的设计。长此以往，系统建设将会越来越困难，甚至陷入不断推翻、重建的循环。 拆分幸好小明和小红是有追求有理想的好青年。意识到问题后，小明和小红从琐碎的业务需求中腾出了一部分精力，开始梳理整体架构，针对问题准备着手改造。 要做改造，首先你需要有足够的精力和资源。如果你的需求方（业务人员、项目经理、上司等）很强势地一心追求需求进度，以至于你无法挪出额外的精力和资源的话，那么你可能无法做任何事…… 在编程的世界中，最重要的便是抽象能力。微服务改造的过程实际上也是个抽象的过程。小明和小红整理了网上超市的业务逻辑，抽象出公用的业务能力，做成几个公共服务： 用户服务 商品服务 促销服务 订单服务 数据分析服务 各个应用后台只需从这些服务获取所需的数据，从而删去了大量冗余的代码，就剩个轻薄的控制层和前端。这一阶段的架构如下： ​​ 这个阶段只是将服务分开了，数据库依然是共用的，所以一些烟囱式系统的缺点仍然存在： 数据库成为性能瓶颈，并且有单点故障的风险。 数据管理趋向混乱。即使一开始有良好的模块化设计，随着时间推移，总会有一个服务直接从数据库取另一个服务的数据的现象。 数据库表结构可能被多个服务依赖，牵一发而动全身，很难调整。 如果一直保持共用数据库的模式，则整个架构会越来越僵化，失去了微服务架构的意义。因此小明和小红一鼓作气，把数据库也拆分了。所有持久化层相互隔离，由各个服务自己负责。另外，为了提高系统的实时性，加入了消息队列机制。架构如下： ​​ 完全拆分后各个服务可以采用异构的技术。比如数据分析服务可以使用数据仓库作为持久化层，以便于高效地做一些统计计算；商品服务和促销服务访问频率比较大，因此加入了缓存机制等。 还有一种抽象出公共逻辑的方法是把这些公共逻辑做成公共的框架库。这种方法可以减少服务调用的性能损耗。但是这种方法的管理成本非常高昂，很难保证所有应用版本的一致性。 数据库拆分也有一些问题和挑战：比如说跨库级联的需求，通过服务查询数据颗粒度的粗细问题等。但是这些问题可以通过合理的设计来解决。总体来说，数据库拆分是一个利大于弊的。 微服务架构还有一个技术外的好处，它使整个系统的分工更加明确，责任更加清晰，每个人专心负责为其他人提供更好的服务。在单体应用的时代，公共的业务功能经常没有明确的归属。最后要么各做各的，每个人都重新实现了一遍；要么是随机一个人（一般是能力比较强或者比较热心的人）做到他负责的应用里面。 在后者的情况下，这个人在负责自己应用之外，还要额外负责给别人提供这些公共的功能——而这个功能本来是无人负责的，仅仅因为他能力较强/比较热心，就莫名地背锅（这种情况还被美其名曰能者多劳）。结果最后大家都不愿意提供公共的功能。长此以往，团队里的人渐渐变得各自为政，不再关心全局的架构设计。 微服务架构本质从管理上也是一种手段，而不止步于技术层面 改造完成后，小明和小红分清楚各自的锅。两人十分满意，一切就像是麦克斯韦方程组一样漂亮完美。 然而…… 没有银弹“没有银弹”是一个常用的成语，意思是指没有一种简单而完美的解决方法或办法。这个成语通常用于形容解决复杂问题时的困难和复杂性。我这里更想说高可用没有银弹，强如阿里，双十一语雀崩溃、阿里全系崩溃、BillBill技术团队刚发布高可用落地实践就崩了还泄漏源码······ 春天来了，万物复苏，又到了一年一度的购物狂欢节。眼看着日订单数量蹭蹭地上涨，小皮小明小红喜笑颜开。可惜好景不长，乐极生悲，突然嘣的一下，系统挂了。 以往单体应用，排查问题通常是看一下日志，研究错误信息和调用堆栈。而微服务架构整个应用分散成多个服务，***定位故障点非常困难**。小明一台机器一台机器地查看日志*，一个服务一个服务的手工调用。经过十几分钟的查找，小明终于定位到故障点：促销服务由于接收的请求量太大而停止响应了。其他服务都直接或间接地会调用促销服务，于是也跟着宕机了。 在微服务架构中，一个服务故障可能会产生雪崩效用，导致整个系统故障。 其实在节前，小明和小红是有做过请求量评估的。按照预计，服务器资源是足以支持节日的请求量的，所以肯定是哪里出了问题。不过形势紧急，随着每一分每一秒流逝的都是白花花的银子，因此小明也没时间排查问题，当机立断在云上新建了几台虚拟机，然后一台一台地部署新的促销服务节点。几分钟的操作后，系统总算是勉强恢复正常了。整个故障时间内估计损失了几十万的销售额，三人的心在滴血…… 引入日志系统，才能方便排查故障 事后，小明简单写了个日志分析工具（量太大了，文本编辑器几乎打不开，打开了肉眼也看不过来），统计了促销服务的访问日志，发现在故障期间，商品服务由于代码问题，在某些场景下会对促销服务发起大量请求。这个问题并不复杂，小明手指抖一抖，修复了这个价值几十万的Bug。 问题是解决了，但谁也无法保证不会再发生类似的其他问题。微服务架构虽然逻辑设计上看是完美的，但就像积木搭建的华丽宫殿一样，经不起风吹草动。微服务架构虽然解决了旧问题，也引入了新的问题： 微服务架构整个应用分散成多个服务，定位故障点非常困难。 稳定性下降。服务数量变多导致其中一个服务出现故障的概率增大，并且一个服务故障可能导致整个系统挂掉。事实上，在大访问量的生产场景下，故障总是会出现的。 服务数量非常多，部署、管理的工作量很大。 开发方面：如何保证各个服务在持续开发的情况下仍然保持协同合作。 测试方面：服务拆分后，几乎所有功能都会涉及多个服务。原本单个程序的测试变为服务间调用的测试。测试变得更加复杂。 小明小红痛定思痛，决心好好解决这些问题。对故障的处理一般从两方面入手，一方面尽量减少故障发生的概率，另一方面降低故障造成的影响。 ​​ 故障预测 - 监控系统我个人更喜欢将建立监控系统的核心目的是为了提前做故障的预测。公司内部的实际案例：也是电商的部门，后台服务（具体服务不清楚，电商同事没说）的磁盘满了，第二天人就不在了，大致过程是磁盘监控中占用80%的时候没有扩建或者类似转移的操作，下一周突然就崩溃了，造成了一些损失（电商部门真金白银） 在高并发分布式的场景下，故障经常是突然间就雪崩式爆发。所以必须建立完善的监控体系，尽可能发现故障的征兆。 微服务架构中组件繁多，各个组件所需要监控的指标不同。比如Redis缓存一般监控占用内存值、网络流量，数据库监控连接数、磁盘空间，业务服务监控并发数、响应延迟、错误率等。因此如果做一个大而全的监控系统来监控各个组件是不大现实的，而且扩展性会很差。一般的做法是让各个组件提供报告自己当前状态的接口（metrics接口），这个接口输出的数据格式应该是一致的。然后部署一个指标采集器组件，定时从这些接口获取并保持组件状态，同时提供查询服务。最后还需要一个UI，从指标采集器查询各项指标，绘制监控界面或者根据阈值发出告警。 大部分组件都不需要自己动手开发，网络上有开源组件。小明下载了RedisExporter和MySQLExporter，这两个组件分别提供了Redis缓存和MySQL数据库的指标接口。微服务则根据各个服务的业务逻辑实现自定义的指标接口。然后小明采用Prometheus作为指标采集器，Grafana配置监控界面和邮件告警。这样一套微服务监控系统就搭建起来了： ​​ 定位问题 - 链路跟踪在微服务架构下，一个用户的请求往往涉及多个内部服务调用。为了方便定位问题，需要能够记录每个用户请求时，微服务内部产生了多少服务调用，及其调用关系。这个叫做链路跟踪。 我们用一个Istio文档里的链路跟踪例子来看看效果： ​​ 图片来自Istio文档 从图中可以看到，这是一个用户访问productpage页面的请求。在请求过程中，productpage服务顺序调用了details和reviews服务的接口。而reviews服务在响应过程中又调用了ratings的接口。整个链路跟踪的记录是一棵树： ​​ 要实现链路跟踪，每次服务调用会在HTTP的HEADERS中记录至少记录四项数据： traceId：traceId标识一个用户请求的调用链路。具有相同traceId的调用属于同一条链路。 spanId：标识一次服务调用的ID，即链路跟踪的节点ID。 parentId：父节点的spanId。 requestTime &amp; responseTime：请求时间和响应时间。 另外，还需要调用日志收集与存储的组件，以及展示链路调用的UI组件。 ​​ 以上只是一个极简的说明，关于链路跟踪的理论依据可详见Google的Dapper 了解了理论基础后，小明选用了Dapper的一个开源实现Zipkin。然后手指一抖，写了个HTTP请求的拦截器，在每次HTTP请求时生成这些数据注入到HEADERS，同时异步发送调用日志到Zipkin的日志收集器中。这里额外提一下，HTTP请求的拦截器，可以在微服务的代码中实现，也可以使用一个网络代理组件来实现（不过这样子每个微服务都需要加一层代理）。 链路跟踪只能定位到哪个服务出现问题，不能提供具体的错误信息。查找具体的错误信息的能力则需要由日志分析组件来提供。 日志系统 ** - 日志分析**日志分析组件应该在微服务兴起之前就被广泛使用了。即使单体应用架构，当访问数变大、或服务器规模增多时，日志文件的大小会膨胀到难以用文本编辑器进行访问，更糟的是它们分散在多台服务器上面。排查一个问题，需要登录到各台服务器去获取日志文件，一个一个地查找（而且打开、查找都很慢）想要的日志信息。 因此，在应用规模变大时，我们需要一个日志的“搜索引擎”。以便于能准确地找到想要的日志。另外，数据源一侧还需要收集日志的组件和展示结果的UI组件： ​​ 小明调查了一下，使用了大名鼎鼎的ELK日志分析组件。ELK是Elasticsearch、Logstash和Kibana三个组件的缩写。 Elasticsearch：搜索引擎，同时也是日志的存储。 Logstash：日志采集器，它接收日志输入，对日志进行一些预处理，然后输出到Elasticsearch。 Kibana：UI组件，通过Elasticsearch的API查找数据并展示给用户。 最后还有一个小问题是如何将日志发送到Logstash。一种方案是在日志输出的时候直接调用Logstash接口将日志发送过去。这样一来又（咦，为啥要用“又”）要修改代码……于是小明选用了另一种方案：日志仍然输出到文件，每个服务里再部署个Agent扫描日志文件然后输出给Logstash。 网关 - 权限控制，服务治理拆分成微服务后，出现大量的服务，大量的接口，使得整个调用关系乱糟糟的。经常在开发过程中，写着写着，忽然想不起某个数据应该调用哪个服务。或者写歪了，调用了不该调用的服务，本来一个只读的功能结果修改了数据…… 为了应对这些情况，微服务的调用需要一个把关的东西，也就是网关。在调用者和被调用者中间加一层网关，每次调用时进行权限校验。另外，网关也可以作为一个提供服务接口文档的平台。 使用网关有一个问题就是要决定在多大粒度上使用：最粗粒度的方案是整个微服务一个网关，微服务外部通过网关访问微服务，微服务内部则直接调用；最细粒度则是所有调用，不管是微服务内部调用或者来自外部的调用，都必须通过网关。折中的方案是按照业务领域将微服务分成几个区，区内直接调用，区间通过网关调用。 由于整个网上超市的服务数量还不算特别多，小明采用的最粗粒度的方案： ​​ 服务注册与发现 - 动态扩容前面的组件，都是旨在降低故障发生的可能性。然而故障总是会发生的，所以另一个需要研究的是如何降低故障产生的影响。 最粗暴的（也是最常用的）故障处理策略就是冗余。一般来说，一个服务都会部署多个实例，这样一来能够分担压力提高性能，二来即使一个实例挂了其他实例还能响应。 冗余的一个问题是使用几个冗余？这个问题在时间轴上并没有一个切确的答案。根据服务功能、时间段的不同，需要不同数量的实例。比如在平日里，可能4个实例已经够用；而在促销活动时，流量大增，可能需要40个实例。因此冗余数量并不是一个固定的值，而是根据需要实时调整的。 一般来说新增实例的操作为： 部署新实例 将新实例注册到负载均衡或DNS上 操作只有两步，但如果注册到负载均衡或DNS的操作为人工操作的话，那事情就不简单了。想想新增40个实例后，要手工输入40个IP的感觉…… 解决这个问题的方案是服务自动注册与发现。首先，需要部署一个服务发现服务，它提供所有已注册服务的地址信息的服务。DNS也算是一种服务发现服务。然后各个应用服务在启动时自动将自己注册到服务发现服务上。并且应用服务启动后会实时（定期）从服务发现服务同步各个应用服务的地址列表到本地。服务发现服务也会定期检查应用服务的健康状态，去掉不健康的实例地址。这样新增实例时只需要部署新实例，实例下线时直接关停服务即可，服务发现会自动检查服务实例的增减。 ​​ 服务发现还会跟客户端负载均衡配合使用。由于应用服务已经同步服务地址列表在本地了，所以访问微服务时，可以自己决定负载策略。甚至可以在服务注册时加入一些元数据（服务版本等信息），客户端负载则根据这些元数据进行流量控制，实现A/B测试、蓝绿发布等功能。 服务发现有很多组件可以选择，比如说Zookeeper 、Eureka、Consul、Etcd等。不过小明觉得自己水平不错，想炫技，于是基于Redis自己写了一个…… 熔断、服务降级、限流熔断 当一个服务因为各种原因停止响应时，调用方通常会等待一段时间，然后超时或者收到错误返回。如果调用链路比较长，可能会导致请求堆积，整条链路占用大量资源一直在等待下游响应。所以当多次访问一个服务失败时，应熔断，标记该服务已停止工作，直接返回错误。直至该服务恢复正常后再重新建立连接。 ​​ 图片来自《微服务设计》 服务降级 当下游服务停止工作后，如果该服务并非核心业务，则上游服务应该降级，以保证核心业务不中断。比如网上超市下单界面有一个推荐商品凑单的功能，当推荐模块挂了后，下单功能不能一起挂掉，只需要暂时关闭推荐功能即可。 限流 一个服务挂掉后，上游服务或者用户一般会习惯性地重试访问。这导致一旦服务恢复正常，很可能因为瞬间网络流量过大又立刻挂掉，在棺材里重复着仰卧起坐。因此服务需要能够自我保护——限流。限流策略有很多，最简单的比如当单位时间内请求数过多时，丢弃多余的请求。另外，也可以考虑分区限流。仅拒绝来自产生大量请求的服务的请求。例如商品服务和订单服务都需要访问促销服务，商品服务由于代码问题发起了大量请求，促销服务则只限制来自商品服务的请求，来自订单服务的请求则正常响应。 ​​ 微服务测试​微服务架构下，测试分为三个层次： 端到端测试：覆盖整个系统，一般在用户界面机型测试。 服务测试：针对服务接口进行测试。 单元测试：针对代码单元进行测试。 三种测试从上到下实施的容易程度递增，但是测试效果递减。端到端测试最费时费力，但是通过测试后我们对系统最有信心。单元测试最容易实施，效率也最高，但是测试后不能保证整个系统没有问题。 ​​ 由于端到端测试实施难度较大，一般只对核心功能做端到端测试。一旦端到端测试失败，则需要将其分解到单元测试：则分析失败原因，然后编写单元测试来重现这个问题，这样未来我们便可以更快地捕获同样的错误。 服务测试的难度在于服务会经常依赖一些其他服务。这个问题可以通过Mock Server解决： ​​ 单元测试大家都很熟悉了。我们一般会编写大量的单元测试（包括回归测试）尽量覆盖所有代码。 微服务框架指标接口、链路跟踪注入、日志引流、服务注册发现、路由规则等组件以及熔断、限流等功能都需要在应用服务上添加一些对接代码。如果让每个应用服务自己实现是非常耗时耗力的。基于DRY的原则，小明开发了一套微服务框架，将与各个组件对接的代码和另外一些公共代码抽离到框架中，所有的应用服务都统一使用这套框架进行开发。 使用微服务框架可以实现很多自定义的功能。甚至可以将程序调用堆栈信息注入到链路跟踪，实现代码级别的链路跟踪。或者输出线程池、连接池的状态信息，实时监控服务底层状态。 使用统一的微服务框架有一个比较严重的问题：框架更新成本很高。每次框架升级，都需要所有应用服务配合升级。当然，一般会使用兼容方案，留出一段并行时间等待所有应用服务升级。但是如果应用服务非常多时，升级时间可能会非常漫长。并且有一些很稳定几乎不更新的应用服务，其负责人可能会拒绝升级……因此，使用统一微服务框架需要完善的版本管理方法和开发管理规范。 另一条路 - Service Mesh另一种抽象公共代码的方法是直接将这些代码抽象到一个反向代理组件。每个服务都额外部署这个代理组件，所有出站入站的流量都通过该组件进行处理和转发。这个组件被称为Sidecar。 Sidecar不会产生额外网络成本。Sidecar会和微服务节点部署在同一台主机上并且共用相同的虚拟网卡。所以sidecar和微服务节点的通信实际上都只是通过内存拷贝实现的。 ​​ 图片来自：Pattern: Service Mesh Sidecar只负责网络通信。还需要有个组件来统一管理所有sidecar的配置。在Service Mesh中，负责网络通信的部分叫数据平面（data plane），负责配置管理的部分叫控制平面（control plane）。数据平面和控制平面构成了Service Mesh的基本架构。 ​​ 图片来自：Pattern: Service Mesh Sevice Mesh相比于微服务框架的优点在于它不侵入代码，升级和维护更方便。它经常被诟病的则是性能问题。即使回环网络不会产生实际的网络请求，但仍然有内存拷贝的额外成本。另外有一些集中式的流量处理也会影响性能。 结束、也是开始微服务不是架构演变的终点。往细走还有Serverless、FaaS等方向。另一方面也有人在唱合久必分分久必合，重新发现单体架构…… 最后，参考楼仔的座右铭：我从清晨走过，也拥抱夜晚的星辰，人生没有捷径，你我皆平凡，你好，陌生人，一起共勉。 ‍","link":"/post/global-understanding-the-evolution-of-microservices-architecture-eww7f.html"},{"title":"Go并发编程 | 总纲","text":"Go并发编程 | 总纲 本专题参考以晁岳攀老师为主，以培养全面、系统的Go并发编程学习体系。 期间可能也会掺杂一些个人理解或者其他的参考资料进行补充和校正。 持续更新中······ 预计2024年初更新完 ​​ 基本介绍基本并发原语：Mutex、RWMutex、Waitgroup、Cond、Pool、Context 等标准库中的并发原语，这些都是传统的并发原语，在其它语言中可能也会有。一通百通 原子操作：原子操作是其它并发原语的基础，学会了你就可以自己创造新的并发原语。 Channel：Channel 类型是 Go 语言独特的类型，因为比较新，较难以掌握，掌握它的基本用法、处理场景和应用模式，避免踩坑。 扩展并发原语：目前来看，Go 开发组不准备在标准库中扩充并发原语了，但是还有一些并发原语应用广泛，比如信号量、SingleFlight、循环栅栏、ErrGroup 等。掌握了它们，就可以在处理一些并发问题时，取得事半功倍的效果。 分布式并发原语：分布式并发原语是应对大规模的应用程序中并发问题的并发类型。我主要会介绍使用 etcd 实现的一些分布式并发原语，比如 Leader 选举、分布式互斥锁、分布式读写锁、分布式队列等，在处理分布式场景的并发问题时，特别有用。这一部分结合了分布式的知识，将并发和分布式结合可能会受益匪浅。 目标学习主线层面，主要是四大步骤，包括​基础用法、实现原理、易错场景、知名项目中的 Bug。每一个模块，我都会带着你按照这四个步骤来学习，目的就是带你熟知每一种并发原语的实现机制和适用场景 实际上，针对同一种场景，也许存在很多并发原语都适用的情况，但是一定是有最合适的那一个。所以，你必须非常清楚每种并发原语的实现机制和适用场景，千万不要被网上的一些文章误导，万事皆用 Channel 奔着精通的方向： 深入学习下 Go 并发原语的源代码。你会发现很多独到的设计，比如Mutex 为了公平性考量的设计、sync.Map 为提升性能做的设计，以及很多并发原语的异常状况的处理方式。尤其是这些异常状况，常常是并发编程中程序 panic 的原因。 没有做过大型并发项目：可能不理解并发原语的重要性。——&gt;参看知名项目中犯过的错，站在巨人的肩膀上分析总结 怎么算并发编程学的好，精通了？ 晁岳攀老师给出的建议是：自己创造并发原语库和实现机制以及适用场景。 ‍","link":"/post/go-parallel-programming-chief-outline-znxfpt.html"},{"title":"Goland 快捷键、模板与一些规范","text":"Goland 快捷键、模板与一些规范题记： 快捷键的使用可以大大提高效率，良好的注释对项目后续的开发维护工作也是十分必要。本文档旨在明确项目开发过程中go代码的注释规范，并提供基于goland的注释模板设置指导。便于开发人员快速配置环境，高效、合规开展工作。 规范部分随缘整理….. 我的配置 Go文件采用goland自带的go文件和代码模板 123456789101112/** * @Description //TODO * @Author luommy * @Date ${DATE} ${TIME} **/package ${GO_PACKAGE_NAME}// 我不习惯加这个func main() {} ​​​​ ‍ 方法、结构体、接口注释采用插件实现 ‍ 自定义快捷键与注释模板​自定义快捷键：CTRL+J​ ​​ 添加新建类的注释模板路径：File -&gt; Settings -&gt; File and Code Templates ​​ 添加如下信息： 123456789package ${GO_PACKAGE_NAME}/** * @Description //TODO * @Author luommy * @Date ${DATE} ${TIME} **/func main() {} 添加方法注释模板‍ Live Templates ​ go templates 内置函数‍ 在模板变量中使用的预定义函数有很多都用不到的，感觉并不好用，没啥能用到的场景 Item Description ​lowercaseAndDash(String)​​ 将驼峰字符串转换为小写，并插入连接符作为分隔符。例如,lowercaseAndDash(MyExampleName)​​返回my-example-name​​. ​snakeCase(String)​​ 将驼峰字符串转化为蛇形字符串，例如,snakeCase(fooBar)​​返回foo_bar​​. ​spaceSeparated(String)​​ 将字符串转化为小写并插入空格作为分隔符，例如,spaceSeparated(fooBar)​​returnsfoo bar​​. ​underscoresToCamelCase(String)​​ 将蛇形字符串转化为驼峰字符串. 例如,underscoresToCamelCase(foo_bar)​​返回fooBar​​. ​underscoresToSpaces(sParameterWithSpaces)​​ 将字符串中的下划线替换为空格. 例如,underscoresToSpaces(foo_bar)​​返回foo bar​​. ​camelCase(String)​​ 将字符串转化为驼峰法. 例如,camelCase(my-text-file)​​,camelCase(my text file)​​, 和camelCase(my_text_file)​​均返回myTextFile​​. ​capitalize(String)​​ 将参数的第一个字母大写。 ​capitalizeAndUnderscore(sCamelCaseName)​​ 根据驼峰法分割参数，将各个部分转化为大写，并插入下划线。例如,capitalizeAndUnderscore(FooBar)​​返回FOO_BAR​​. ​classNameComplete()​​ 这个表达式代替了在变量位置完成类名。 ​clipboard()​​ 返回系统剪贴板的内容。 ​complete()​​ 引用代码完成。 ​completeSmart()​​ 调用变量位置的智能类型完成。 ​concat(expressions...)​​ 返回传递给函数的所有字符串作为参数的级联。 ​date(sDate)​​ 以指定的格式返回当前系统日期。如果没有参数，则以默认的系统格式返回当前日期。 ​decapitalize(sName)​​ 用相应的小写字母替换参数的第一个字母。 ​enum(sCompletionString1,sCompletionString2,...)​​ 返回在模板扩展时建议完成的逗号分隔字符串的列表。 ​escapeString(sEscapeString)​​ Escapes the string specified as the parameter. ​expectedType()​​ Returns the expected type of the expression into which the template expands. Makes sense if the template expands in the right part of an assignment, afterreturn​​, etc. ​fileName()​​ 返回当前文件（包括扩展名） ​fileNameWithoutExtension()​​ 返回当前文件（不包括扩展名） ​firstWord(sFirstWord)​​ 返回作为参数传递的字符串的第一个单词。 ​lineNumber()​​ 返回当前行数。 ​substringBefore(String,Delimiter)​​ 删除指定分隔符之后的扩展名，只返回文件名。这对测试文件名是有帮助的，例如,substringBefore($FileName$,&quot;.&quot;)​​returnscomponent-test​​incomponent-test.js​​). ​time(sSystemTime)​​ 以指定的格式返回当前系统时间。 ​timestamp()​​ 返回当前系统时间戳 ​user()​​ 返回当前用户 ‍ ‍ 中英互译插件目前觉得挺好用的 支持自定义快捷键：setting-keymap-plugins 习惯设置Alt+T 一键互译 ​​ 注释插件：Goanno插件市场安装，通过tools进行配置——Goanno Setting ​​​​ 方法、接口、结构体注释模板配置​​ 配置内容如下： 123456789101112131415161718192021222324Normal Method 配置内容：/** @Title ${function_name} @Description ${todo} @Author luommy ${date} @Param ${params} @Return ${return_types} **/interface配置内容// ${interface_name} Interface Method 配置内容// @Title ${function_name} // @Description ${todo} // @Author luommy ${date} // @Param ${params} // @Return ${return_types} Struct配置内容// ${struct_name} Struct Field 不做配置 配置完成点击submit 验证注释 在方法、结构体、接口上 win使用快捷键: ctrl +alt +/​ mac使用快捷键:control + commond + /​ 可自动生成注释 如下截图: ​ AI 插件 CodeGeeX官网 几个重点的功能： 多语言代码之间翻译，无缝转换 注释生成代码/自动补全代码 AI问答","link":"/post/goland-shortcut-key-template-and-some-specifications-z2131hx.html"},{"title":"","text":"Golang 📄 Go典型使用·实例 📑 重学系列 📄 Go经验小记 📄 练习两年半释疑 📄 Go新版本特性 📄 最常用的 Go CLI 命令 📄 重学Go语言 | 如何在Go中使用Context 📑 Go并发编程 📄 Go语言实现的可读性更高的并发神库 🥗 Golang门面担当 📄 GMP模型的知识体系 📄 Go 语言五大日志库 📄 Go语言——延迟函数defer的使用 📄 Go语言——Map的底层介绍及扩缩容机制 📄 Go语言——切片(Slice)的坑 📄 Go语言——内存管理 📑 专项 📄 Gorm框架的思考 📄 Go与操作系统的线程（进程）间通信 📄 Go单机锁与同步原语 📄 深入解析go channel各状态下的操作结果 📄 解密Go协程的栈内存管理 📄 Golang实现延迟队列（DelayQueue） 📄 go channel的应用案例 📑 Gin深入理解 📄 Gin框架流程原理以及上下文、内存的思考 📄 我给 gin 提交了一行代码 - 墨天轮 📄 由Gin路由原理引发的一系列思考 📄 Gin路由设计及源码分析：httprouter路由实现原理 📄 Gin生命周期 📑 Google书签整理 📄 Go Modules 依赖管理，这篇总结的挺全 📄 最全Go select底层原理，一文学透高频用法 📄 剑指 Offer 38. 字符串的排列 📄 阅读破10万的学Go建议，不管初学还是进阶Go都值得一看！ 📍 日常小结 📄 【Golang】来几道题以加强切片知识 📄 goland-IDEOlog 插件 📄 Rob Pike谈Google Go 📄 Go的数组与切片傻傻分不清楚 📄 Go获取IP地址 📄 json字段控制 📄 Go 语言 iota 的神奇力量 📄 日志框架 📄 Go语言的%d,%p,%v等占位符的使用 📄 Go Web开发的五大利器 📄 Go中常见的IO模式 📄 Go常见错误第16篇：any的常见错误和最佳实践 - 掘金 📄 GORM框架 📄 标准库-time包 📄 高阶函数编程Go语言中的函数一等公民 📄 【Golang】怎样优雅的清空切片 📄 Go web项目布局 📄 死锁、活锁、饥饿、自旋锁（Go 语言描述） 📄 2023.3.8小结 📄 《100 Go Mistakes and How to Avoid Them》 📑 进阶篇 📄 Context包源码解读 📄 nethttp库源码解读 📄 Golang 单机锁实现原理 📄 进阶篇之函数篇 📄 优秀后端都应该具备的开发好习惯 📄 进阶篇之结构体篇 📄 Go Channel底层原理 📄 go语言反射的使用 📄 time包 📄 json字段控制 📄 深入浅出Go调度器中的GMP模型 📄 有无缓冲管道 📄 一文让你理解go语言的Context 📄 GO语言实现设计模式【上】 📄 GO语言实现设计模式【下】 📑 框架 📑 Go-zero 📄 玩转 Go 链路追踪 📄 go-zero 是如何追踪你的请求链路 📄 GoFrame学习之路 📑 RPC与GRPC 📄 RPC 📄 grpcurl的使用 📄 grpcui安装使用 ‍","link":"/post/golang-2sensm.html"},{"title":"Golang测试","text":"Golang测试相关引言Go 语言的单元测试默认采用官方自带的测试框架，通过引入 testing 包，通过执行 go test​ 命令来实现单元测试功能。 在源代码包目录内，所有以 _test.go​ 为后缀名的源文件会被 go test​ 认定为单元测试的文件，这些单元测试的文件不会包含在 go build​ 的源代码构建中，而是单独通过 go test 来编译并执行。 不写测试的开发不是好程序员。TDD（Test Driven Development）在国内其实都不太关注测试这一部分。 这篇文章主要介绍下在Golang中如何做单元测试和基准测试，同时自己测试时也发现了一些问题。 单元测试Go 单元测试的基本规范Go 单元测试的基本规范如下： 每个测试函数都必须导入 testing 包。测试函数的命名类似func TestName(t *testing.T)​，入参必须是 *testing.T​类型 测试函数的函数名必须以大写的 Test 开头，后面紧跟函数名，要么是大写，要么就是下划线，比如 func TestName(t *testing.T)​ 或者 func Test_name(t *testing.T)​ 都是可以的， 但是 func Testname(t *testing.T)​这不会被检测到，一定要注意 通常情况下，需要将测试文件和源代码放在同一个包内。一般测试文件的命名，都是 {source_filename}_test.go​，比如源代码文件是ai.go ，那么就会在 ai.go 的相同目录下，再建立一个 ai_test.go 的单元测试文件去测试 ai.go 文件里的相关方法。 当运行 go test 命令时，go test 会遍历当前目录下所有的 *_test.go​ 中符合上述命名规则的函数，然后生成一个临时的 main 包用于调用相应的测试函数，然后构建并运行、报告测试结果，最后清理测试中生成的临时文件。 常用命令单元测试： ​go test ​ ​go test -v​ ​go test -v -run=&quot;方法名关键字&quot;​ 覆盖率测试： ​go test -cover -coverprofile=输出文件名字​ ​go tool cover -html=输出文件名字​ -v参数: 是在测试结果中补充被测函数名称和运行时间 -run参数: 它对应一个正则表达式，只有函数名匹配上的测试函数才会被go test​命令执行。 测试示例这里参考七米的示例： 定义一个split​的包，包中定义了一个Split​函数，函数功能是为了从一个s字符串中去除sep，具体实现如下： 123456789101112131415161718192021// split/split.gopackage splitimport &quot;strings&quot;// split package with a single split function.// Split slices s into all substrings separated by sep and// returns a slice of the substrings between those separators.func Split(s, sep string) (result []string) { i := strings.Index(s, sep) for i &gt; -1 { result = append(result, s[:i]) s = s[i+1:] //这里有bug，没有考虑到sep为多个字符的情况：s = s[i+len(sep):] 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return} 然后 ***在相同的目录下 *** 写测试函数（可以写多个测试函数，运行时可加run参数正则匹配对应的被测函数，如果不加参数默认测试全部的）： 1234567891011121314151617// split/split_test.gopackage splitimport ( &quot;reflect&quot; &quot;testing&quot;)// 这里注意函数名规范：Test+首字母大写的被测函数名func TestSplit(t *testing.T) { // 测试函数名必须以Test开头，必须接收一个*testing.T类型参数 got := Split(&quot;a:b:c&quot;, &quot;:&quot;) // 程序输出的结果 want := []string{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;} // 期望的结果 if !reflect.DeepEqual(want, got) { // 因为slice不能比较直接，借助反射包中的方法比较 t.Errorf(&quot;expected:%v, got:%v&quot;, want, got) // 测试失败输出错误提示 }} 此时的目录结构： 1234split $ ls -ltotal 16-rw-r--r-- 1 liwenzhou staff 408 4 29 15:50 split.go-rw-r--r-- 1 liwenzhou staff 466 4 29 16:04 split_test.go 然后在此目录下执行go test​， 结果如下： 123split $ go testPASSok github.com/Q1mi/studygo/code_demo/test_demo/split 0.005s PASS标志测试成功 一般都习惯加下 -v 参数 再写一个测试函数TestMoreSplit，具体内容已省略，失败的情况： 123456789split $ go test -v=&lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; RUN TestSplit--- PASS: TestSplit (0.00s)&lt;/span&gt;= RUN TestMoreSplit--- FAIL: TestMoreSplit (0.00s) split_test.go:21: expected:[a d], got:[a cd]FAILexit status 1FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.005s 测试用例组这实际环境下通常要考虑一些更全面的东西，比如中文切割。 参考七米的例子：有多个测试用例组 12345678910111213141516171819202122func TestSplit(t *testing.T) { // 定义一个测试用例类型 type test struct { input string sep string want []string } // 定义一个存储测试用例的切片 tests := []test{ {input: &quot;a:b:c&quot;, sep: &quot;:&quot;, want: []string{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}}, {input: &quot;a:b:c&quot;, sep: &quot;,&quot;, want: []string{&quot;a:b:c&quot;}}, {input: &quot;abcd&quot;, sep: &quot;bc&quot;, want: []string{&quot;a&quot;, &quot;d&quot;}}, {input: &quot;沙河有沙又有河&quot;, sep: &quot;沙&quot;, want: []string{&quot;河有&quot;, &quot;又有河&quot;}}, } // 遍历切片，逐一执行测试用例 for _, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(&quot;expected:%v, got:%v&quot;, tc.want, got) } }} go test 执行后，发现并不能直观的看出 1234567split $ go test -v=&lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; RUN TestSplit--- FAIL: TestSplit (0.00s) split_test.go:42: expected:[河有 又有河], got:[ 河有 又有河]FAILexit status 1FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s 这里有一个小技巧：使用​%#v​的格式化方式 t.Errorf(“expected:%v, got:%#v”, tc.want, got) 然后就很直观了： 1234567split $ go test -v&lt;/span&gt;= RUN TestSplit--- FAIL: TestSplit (0.00s) split_test.go:42: expected:[]string{&quot;河有&quot;, &quot;又有河&quot;}, got:[]string{&quot;&quot;, &quot;河有&quot;, &quot;又有河&quot;}FAILexit status 1FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s 然后就再优化下被测函数或者干脆直接把测试函数的期望改了 改测试函数期望：want: []string{&quot;&quot;,&quot;河有&quot;, &quot;又有河&quot;}​ 改被测函数：比如在result = append(result, s)​之前判断下len(s)是否大于0,或者考虑下strings.Index的合理性… 子测试如果在测试中有大量的测试用例，不好区分哪个测试用例失效了，这种情况下要用到子测试，其实也可以用map的方式 map： 12345678910111213141516171819func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 &quot;simple&quot;: {input: &quot;a:b:c&quot;, sep: &quot;:&quot;, want: []string{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}}, &quot;wrong sep&quot;: {input: &quot;a:b:c&quot;, sep: &quot;,&quot;, want: []string{&quot;a:b:c&quot;}}, &quot;more sep&quot;: {input: &quot;abcd&quot;, sep: &quot;bc&quot;, want: []string{&quot;a&quot;, &quot;d&quot;}}, &quot;leading sep&quot;: {input: &quot;沙河有沙又有河&quot;, sep: &quot;沙&quot;, want: []string{&quot;河有&quot;, &quot;又有河&quot;}}, } for name, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(&quot;name:%s expected:%#v, got:%#v&quot;, name, tc.want, got) // 将测试用例的name格式化输出 } }} 子测试：t.Run() 123456789101112131415161718192021func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 &quot;simple&quot;: {input: &quot;a:b:c&quot;, sep: &quot;:&quot;, want: []string{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;}}, &quot;wrong sep&quot;: {input: &quot;a:b:c&quot;, sep: &quot;,&quot;, want: []string{&quot;a:b:c&quot;}}, &quot;more sep&quot;: {input: &quot;abcd&quot;, sep: &quot;bc&quot;, want: []string{&quot;a&quot;, &quot;d&quot;}}, &quot;leading sep&quot;: {input: &quot;沙河有沙又有河&quot;, sep: &quot;沙&quot;, want: []string{&quot;河有&quot;, &quot;又有河&quot;}}, } for name, tc := range tests { t.Run(name, func(t *testing.T) { // 使用t.Run()执行子测试 got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(&quot;expected:%#v, got:%#v&quot;, tc.want, got) } }) }} 再go test： 123456789101112131415split $ go test -v=&lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; RUN TestSplit&lt;/span&gt;= RUN TestSplit/leading_sep=&lt;span style=&quot;font-weight: bold;&quot; class=&quot;mark&quot;&gt; RUN TestSplit/simple&lt;/span&gt;= RUN TestSplit/wrong_sep=== RUN TestSplit/more_sep--- FAIL: TestSplit (0.00s) --- FAIL: TestSplit/leading_sep (0.00s) split_test.go:83: expected:[]string{&quot;河有&quot;, &quot;又有河&quot;}, got:[]string{&quot;&quot;, &quot;河有&quot;, &quot;又有河&quot;} --- PASS: TestSplit/simple (0.00s) --- PASS: TestSplit/wrong_sep (0.00s) --- PASS: TestSplit/more_sep (0.00s)FAILexit status 1FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s 补充： 可以通过-run=RegExp​来指定运行的测试用例，还可以通过/​来指定要运行的子测试用例，例如：go test -v -run=Split/simple​只会运行simple​对应的子测试用例。 意料之外的情况将被测函数NewLRU与测试函数NewLRU_test的目录下有其他的文件，且这些有文件在main包中，这种情况下，main会被检测到，导致其它不相干的函数也被检测到了。 如果目录像我这种：我有一个LRU算法函数和一个测试LRU的测试函数 ​​ 直接go test会报错： ​​ 解决方式： 执行文件：go test .\\NewLRU.go .\\NewLRU_test.go -v​ ,可这也太麻烦了 移到的新包下，比如新建个test目录，将NewLRU.go​与NewLRU_test.go​一进去 通常业界也观察到，大部分测试文件和被测文件是放在同一个目录下的 覆盖率测试测试覆盖率是你的代码被测试套件覆盖的百分比。通常我们使用的都是语句的覆盖率，也就是在测试中至少被运行一次的代码占总代码的比例。 Go提供内置功能来检查你的代码覆盖率。我们可以使用go test -cover​来查看测试覆盖率。例如： 123PASScoverage: 100.0% of statementsok main/test 0.818s o还提供了一个额外的-coverprofile​参数，用来将覆盖率相关的记录信息输出到一个文件。例如： ​go test -cover -coverprofile=c​ 执行go tool cover -html=c​，使用cover​工具来处理生成的记录信息，该命令会打开本地的浏览器窗口生成一个HTML报告 ​​ 每个用绿色标记的语句块表示被覆盖了，而红色的表示没有被覆盖。 基准测试 我一般理解为性能测试，在一定工作负载下检测程序性能的一种方法。 后续补充…..","link":"/post/golang-test-related-mbnma.html"},{"title":"高并发设计思考体系","text":"高并发设计思考体系 读了苏三的博客，进一步对高并发设计整体的思考多了许多 。 在原文章上加入一些自己了理解和新的认识。 “ 高并发不会是区别大厂、小厂工程师的标准，却是 检验技术实力的一道关。” 如何设计一个​高并发系统 ？ 瞬间联想秒杀系统？50W QPS？数据库、缓存、消息队列、分布式服务如何演进的？ 这个问题真的可以无限无限难，这个问题也是激发我学习的动力，后续会花一部分时间专研这部分，本文章目的是扩展下高并发设计的解决思路，即总纲。 对于高并发的设计理念有了比较系统的理解与认识，尤其是对于某些架构方面的认识，越来越觉得高并发、微服务、分布式这些获取比较庞大知识体系其实是相互共存的，懂了一方面理解另一方面也会更加容器，也更容易形成知识体系。 本部分与 Go并发编程 直接联系，当然了并发编程不一定局限于某种语言，思想和实践方式都有比较类似之处。 ‍ 关联文章： 并发编程的业务场景 参考文章： 苏三博客 并发设计解决宏图​​ 从前端——后端——运维 每一个步骤和环节都有着处理的方式，眼界瞬间就提升了，不是因为你是后端，局限于后端的部分，或者是前端。 解决思路1 页面静态化对于高并发系统的页面功能，我们必须要做静态化​设计。 如果并发访问系统的用户非常多，每次用户访问页面的时候，都通过服务器动态渲染，会导致服务端承受过大的压力，而导致页面无法正常加载的情况发生。 使用Freemarker​或Velocity​模板引擎，实现页面静态化功能。 以商城官网首页为例，我们可以在Job​中，每隔一段时间，查询出所有需要在首页展示的数据，汇总到一起，使用模板引擎生成到html文件当中。 然后将该html​文件，通过shell​脚本，自动同步到前端页面相关的服务器上。 有一说一这部分是 前端 的部分工作，并不了解，问了几个前端的朋友也没有了解过的，可能大厂的前端才可能用? 不得而知。 2 CDN加速虽说页面静态化可以提升网站网页的访问速度，但还不够，因为用户分布在全国各地，有些人在北京，有些人在成都，有些人在深圳，地域相差很远，他们访问网站的网速各不相同。 如何才能让用户最快访问到活动页面呢？ 这就需要使用CDN，它的全称是Content Delivery Network，即内容分发网络。 ​ 使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。 CDN加速的基本原理是：将网站的静态内容（如图片、CSS、JavaScript文件等）复制并存储到分布在全球各地的服务器节点上。 ——当用户请求访问网站时，CDN系统会根据用户的地理位置，自动将内容分发给离用户最近的服务器，从而实现快速访问。 国内常见的CDN提供商有阿里云CDN、腾讯云CDN、百度云加速等，它们提供了全球分布的节点服务器，为全球范围内的网站加速服务。 这个东西在大学期间就有所耳闻，其实也很好理解。 3 缓存在高并发的系统中，缓存​可以说是必不可少的技术之一。 目前缓存有两种： 基于应用服务器的内存缓存，也就是我们说的二级缓存。 使用缓存中间件，比如：Redis、Memcached等，这种是分布式缓存。 这两种缓存各有优缺点。 二级缓存的性能更好，但因为是基于应用服务器内存的缓存，如果系统部署到了多个服务器节点，可能会存在数据不一致的情况。 而Redis或Memcached虽说性能上比不上二级缓存，但它们是分布式缓存，避免多个服务器节点数据不一致的问题。 缓存的用法一般是这样的：使用缓存之后，可以减轻访问数据库的压力，显著的提升系统的性能。 有些业务场景，甚至会分布式缓存和二级缓存一起使用。 比如获取商品分类数据，流程如下：​ 不过引入缓存，虽说给我们的系统性能带来了提升，但同时也给我们带来了一些新的问题，比如：《数据库和缓存双向数据库一致性问题》、《缓存穿透、击穿和雪崩问题》等。要结合实际业务场景，切记不要为了缓存而缓存。 4 异步有时候，我们在高并发系统当中，某些接口的业务逻辑，没必要都同步执行。 比如有个用户请求接口中，需要做业务操作，发站内通知，和记录操作日志。为了实现起来比较方便，通常我们会将这些逻辑放在接口中同步执行，势必会对接口性能造成一定的影响。 接口内部流程图如下： 这个接口表面上看起来没有问题，但如果你仔细梳理一下业务逻辑，会发现只有业务操作才是核心逻辑，其他的功能都是非核心逻辑。 在这里有个原则就是：核心逻辑可以同步执行，同步写库。非核心逻辑，可以异步执行，异步写库。 上面这个例子中，发站内通知和用户操作日志功能，对实时性要求不高，即使晚点写库，用户无非是晚点收到站内通知，或者运营晚点看到用户操作日志，对业务影响不大，所以完全可以异步处理。 通常异步主要有两种：多线程 和 mq。 这部分内容在项目中经常能用到，比如一个OA系统在同步业务处理的时候需要把业务内容同步给邮箱让用户知晓，但这个同步的过程不是核心业务，也就是晚点通知、早点通知都是无所谓的，不影响系统的功能。 4.1 线程池使用线程池改造之后，接口逻辑如下： 发站内通知和用户操作日志功能，被提交到了两个单独的线程池中。 这样接口中重点关注的是业务操作，把其他的逻辑交给线程异步执行，这样改造之后，让接口性能瞬间提升了。 但使用线程池有个小问题就是：如果服务器重启了，或者是需要被执行的功能出现异常了，无法重试，会丢数据。 那么这个问题该怎么办呢？ =&gt;这个问题：我联想到了幂等性、MQ 、持久化等方面。比较肤浅的思路就是将需要被执行的任务持久化到磁盘或者数据库中，当任务执行失败或服务器重启之后，可以重新读取任务，并进行重试。也许后续会更新…. 4.2 mq使用mq改造之后，接口逻辑如下： 对于发站内通知和用户操作日志功能，在接口中并没真正实现，它只发送了mq消息到mq服务器。然后由mq消费者消费消息时，才真正的执行这两个功能。 这样改造之后，接口性能同样提升了，因为发送mq消息速度是很快的，我们只需关注业务操作的代码即可。 思路非常合理 5 多线程处理在高并发系统当中，用户的请求量很大。 假如我们现在用mq处理业务逻辑。 一下子有大量的用户请求，产生了大量的mq消息，保存到了mq服务器。 而mq的消费者，消费速度很慢。 =》可能会导致大量的消息积压问题。 从而严重影响数据的实时性。 我们需要对消息的消费者做优化。 最快的方式是使用多线程​消费消息，比如：改成线程池消费消息。 当然核心线程数、最大线程数、队列大小 和 线程回收时间，一定要做成配置的，后面可以根据实际情况动态调整。 这样改造之后，我们可以快速解决消息积压问题。 除此之外，在很多数据导入场景，用多线程导入数据，可以提升效率。 温馨提醒一下：使用多线程消费消息，可能会出现消息的顺序问题。如果你的业务场景中，需要保证消息的顺序，则要用其他的方式解决问题。 ‍ 这部分其实就是多线程处理业务了，不但业务开了线程池，同时的消息消费的处理也开线程池来处理 ‍ 6 分库分表有时候，高并发系统的吞吐量受限的不是别的，而是数据库。 当系统发展到一定的阶段，用户并发量大，会有大量的数据库请求，需要占用大量的数据库连接，同时会带来磁盘IO的性能瓶颈问题。 此外，随着用户数量越来越多，产生的数据也越来越多，一张表有可能存不下。由于数据量太大，sql语句查询数据时，即使走了索引也会非常耗时。 这时该怎么办呢？ 答：需要做分库分表​。 如下图所示： 图中将用户库拆分成了三个库，每个库都包含了四张用户表。 如果有用户请求过来的时候，先根据用户id路由到其中一个用户库，然后再定位到某张表。 路由的算法挺多的： 根据id取模，比如：id=7，有4张表，则7%4=3，模为3，路由到用户表3。 给id指定一个区间范围，比如：id的值是0-10万，则数据存在用户表0，id的值是10-20万，则数据存在用户表1。 一致性hash算法 分库分表主要有两个方向：垂直​和水平​。 说实话垂直方向（即业务方向）更简单。 在水平方向（即数据方向）上，分库和分表的作用，其实是有区别的，不能混为一谈。 分库：是为了解决数据库连接资源不足问题，和磁盘IO的性能瓶颈问题。 分表：是为了解决单表数据量太大，sql语句查询数据时，即使走了索引也非常耗时问题。此外还可以解决消耗cpu资源问题。 分库分表：可以解决 数据库连接资源不足、磁盘IO的性能瓶颈、检索数据耗时 和 消耗cpu资源等问题。 如果在有些业务场景中，用户并发量很大，但是需要保存的数据量很少，这时可以只分库，不分表。 如果在有些业务场景中，用户并发量不大，但是需要保存的数量很多，这时可以只分表，不分库。 如果在有些业务场景中，用户并发量大，并且需要保存的数量也很多时，可以分库分表。 关于分库分表更详细的内容，苏三里面讲的更深入《阿里二面：为什么分库分表？》 同时也关联我之前分库分表的思考：微服务下分库分表的思考 7 池化技术其实不光是高并发系统，为了性能考虑，有些低并发的系统，也在使用池化技术​，比如：数据库连接池、线程池等。 池化技术是多例设计模式​的一个体现。 我们都知道创建​和销毁​数据库连接是非常耗时耗资源的操作。 如果每次用户请求，都需要创建一个新的数据库连接，势必会影响程序的性能。 为了提升性能，我们可以创建一批数据库连接，保存到内存中的某个集合中，缓存起来。 这样的话，如果下次有需要用数据库连接的时候，就能直接从集合中获取，不用再额外创建数据库连接，这样处理将会给我们提升系统性能。当然用完之后，需要及时归还。 目前常用的数据库连接池有：Druid、C3P0、hikari和DBCP等。 8 读写分离不知道你有没有听说过二八原则​，在一个系统当中可能有80%是读数据请求，另外20%是写数据请求。 不过这个比例也不是绝对的。 我想告诉大家的是，一般的系统读数据请求会远远大于写数据请求。 如果读数据请求和写数据请求，都访问同一个数据库，可能会相互抢占数据库连接，相互影响。 我们都知道，一个数据库的数据库连接数量是有限，是非常宝贵的资源，不能因为读数据请求，影响到写数据请求吧？ 这就需要对数据库做读写分离​了。 于是，就出现了主从读写分离架构：考虑刚开始用户量还没那么大，选择的是一主一从的架构，也就是常说的一个master​，一个slave​。 所有的写数据请求，都指向主库。一旦主库写完数据之后，立马异步同步给从库。这样所有的读数据请求，就能及时从从库中获取到数据了（除非网络有延迟）。 但这里有个问题就是：如果用户量确实有些大，如果master挂了，升级slave为master，将所有读写请求都指向新master。 但此时，如果这个新master根本扛不住所有的读写请求，该怎么办？ 这就需要一主多从的架构了：上图中我列的是一主两从，如果master挂了，可以选择从库1或从库2中的一个，升级为新master。假如我们在这里升级从库1为新master，则原来的从库2就变成了新master的的slave了。 调整之后的架构图如下：这样就能解决上面的问题了。 除此之外，如果查询请求量再增大，我们还可以将架构升级为一主三从、一主四从…一主N从等。 9 索引在高并发的系统当中，用户经常需要查询数据，对数据库增加索引​，是必不可少的一个环节。 尤其是表中数据非常多时，加了索引，跟没加索引，执行同一条sql语句，查询相同的数据，耗时可能会相差N个数量级。 虽说索引能够提升SQL语句的查询速度，但索引也不是越多越好。 在insert数据时，需要给索引分配额外的资源，对insert的性能有一定的损耗。 我们要根据实际业务场景来决定创建哪些索引，索引少了，影响查询速度，索引多了，影响写入速度。 很多时候，我们需要经常对索引做优化。 可以将多个单个索引，改成一个联合索引。 删除不要索引。 使用explain关键字，查询SQL语句的执行计划，看看哪些走了索引，哪些没有走索引。 要注意索引失效的一些场景。 必要时可以使用force index来强制查询sql走某个索引。 如果你想进一步了解explain的详细用法，可以看看我的另一篇文章《explain | 索引优化的这把绝世好剑，你真的会用吗？》。 如果你想进一步了解哪些情况下索引会失效，可以看看我的另一篇文章《聊聊索引失效的10种场景，太坑了》。 这一部分涉及到了： 索引失效 索引优化 10 批处理有时候，我们需要从指定的用户集合中，查询出有哪些是在数据库中已经存在的。 实现代码可以这样写： 123456789public List&lt;User&gt; queryUser(List&lt;User&gt; searchList) { if (CollectionUtils.isEmpty(searchList)) { return Collections.emptyList(); } List&lt;User&gt; result = Lists.newArrayList(); searchList.forEach(user -&gt; result.add(userMapper.getUserById(user.getId()))); return result;} 这里如果有50个用户，则需要循环50次，去查询数据库。我们都知道，每查询一次数据库，就是一次远程调用。 如果查询50次数据库，就有50次远程调用，这是非常耗时的操作。 那么，我们如何优化呢？ 答：批处理​。 具体代码如下： 1234567public List&lt;User&gt; queryUser(List&lt;User&gt; searchList) { if (CollectionUtils.isEmpty(searchList)) { return Collections.emptyList(); } List&lt;Long&gt; ids = searchList.stream().map(User::getId).collect(Collectors.toList()); return userMapper.getUserByIds(ids);} 提供一个根据用户id集合批量查询​用户的接口，只远程调用一次，就能查询出所有的数据。 这里有个需要注意的地方是：id集合的大小要做限制，最好一次不要请求太多的数据。要根据实际情况而定，建议控制每次请求的记录条数在500以内。 12345678910111213func queryUsers(searchList []User) []User { if len(searchList) == 0 { return []User{} } var ids []int for _, user := range searchList { ids = append(ids, user.ID) } return getUserByIDList(ids)} 这种批处理的方式还是看业务应用场景比较好理解 11 集群系统部署的服务器节点，可能会down机，比如：服务器的磁盘坏了，或者操作系统出现内存不足问题。 为了保证系统的高可用，我们需要部署多个节点，构成一个集群​，防止因为部分服务器节点挂了，导致系统的整个服务不可用的情况发生。 集群有很多种： 应用服务器集群 数据库集群 中间件集群 文件服务器集群 我们以中间件Redis​为例。 在高并发系统中，用户的数据量非常庞大时，比如用户的缓存数据总共大小有40G，一个服务器节点只有16G的内存。 这样需要部署3台服务器节点。 该业务场景，使用普通的master/slave模式，或者使用哨兵模式都行不通。 40G的数据，不能只保存到一台服务器节点，需要均分到3个master服务器节点上，一个master服务器节点保存13.3G的数据。 当有用户请求过来的时候，先经过路由，根据用户的id或者ip，每次都访问指定的服务器节点。这用就构成了一个集群。 但这样有风险，为了防止其中一个master服务器节点挂掉，导致部分用户的缓存访问不了，还需要对数据做备份。 这样每一个master，都需要有一个slave，做数据备份。 如果master挂了，可以将slave升级为新的master，而不影响用户的正常使用。 本质还是结合了主从的思想，这里有个问题，一个节点就代表一个服务器吗？ 12 负载均衡如果我们的系统部署到了多台服务器节点。那么哪些用户的请求，访问节点a，哪些用户的请求，访问节点b，哪些用户的请求，访问节点c？ 我们需要某种机制，将用户的请求，转发到具体的服务器节点上。 这就需要使用负载均衡​机制了。 在linux下有Nginx​、LVS​、Haproxy​等服务可以提供负载均衡服务。 在SpringCloud微服务架构中，大部分使用的负载均衡组件就是Ribbon​、OpenFegin​或SpringCloud Loadbalancer​。 硬件方面，可以使用F5​实现负载均衡。它可以基于交换机实现负载均衡，性能更好，但是价格更贵一些。 常用的负载均衡策略有： ​轮询​：每个请求按时间顺序逐一分配到不同的服务器节点，如果服务器节点down掉，能自动剔除。 ​weight权重​：weight代表权重默认为1，权重越高，服务器节点被分配到的概率越大。weight和访问比率成正比，用于服务器节点性能不均的情况。 ​ip hash​：每个请求按访问ip的hash结果分配, 这样每个访客固定访问同一个服务器节点，它是解诀Session共享的问题的解决方案之一。 ​最少连接数​：把请求转发给连接数较少的服务器节点。轮询算法是把请求平均的转发给各个服务器节点，使它们的负载大致相同；但有些请求占用的时间很长，会导致其所在的服务器节点负载较高。这时least_conn方式就可以达到更好的负载均衡效果。 ​最短响应时间​：按服务器节点的响应时间来分配请求，响应时间短的服务器节点优先被分配。 从划分大类上还可以分为动态负载均衡、静态负载均衡；动态负载均衡考虑服务器当前状态，静态不考虑 13 限流对于高并发系统，为了保证系统的稳定性，需要对用户的请求量做限流​。 特别是秒杀系统中，如果不做任何限制，绝大部分商品可能是被机器抢到，而非正常的用户，有点不太公平。 所以，我们有必要识别这些非法请求，做一些限制。那么，我们该如何现在这些非法请求呢？ 目前有两种常用的限流方式： 基于nginx限流 基于redis限流 13.1 对同一用户限流为了防止某个用户，请求接口次数过于频繁，可以只针对该用户做限制。限制同一个用户id，比如每分钟只能请求5次接口。 13.2 对同一ip限流有时候只对某个用户限流是不够的，有些高手可以模拟多个用户请求，这种nginx就没法识别了。 这时需要加同一ip限流功能。限制同一个ip，比如每分钟只能请求5次接口。 但这种限流方式可能会有误杀的情况，比如同一个公司或网吧的出口ip是相同的，如果里面有多个正常用户同时发起请求，有些用户可能会被限制住。 13.3 对接口限流别以为限制了用户和ip就万事大吉，有些高手甚至可以使用代理，每次都请求都换一个ip。 这时可以限制请求的接口总次数。在高并发场景下，这种限制对于系统的稳定性是非常有必要的。但可能由于有些非法请求次数太多，达到了该接口的请求上限，而影响其他的正常用户访问该接口。看起来有点得不偿失。 13.4 加验证码相对于上面三种方式，加验证码的方式可能更精准一些，同样能限制用户的访问频次，但好处是不会存在误杀的情况。 ​通常情况下，用户在请求之前，需要先输入验证码。用户发起请求之后，服务端会去校验该验证码是否正确。只有正确才允许进行下一步操作，否则直接返回，并且提示验证码错误。 此外，验证码一般是一次性的，同一个验证码只允许使用一次，不允许重复使用。 普通验证码，由于生成的数字或者图案比较简单，可能会被破解。优点是生成速度比较快，缺点是有安全隐患。 还有一个验证码叫做：移动滑块​​，它生成速度比较慢，但比较安全，是目前各大互联网公司的首选。 限流应用很常见 14 服务降级前面已经说过，对于高并发系统，为了保证系统的稳定性，需要做限流。 但光做限流还不够。 我们需要合理利用服务器资源，保留核心的功能，将部分非核心的功能，我们可以选择屏蔽或者下线掉。 我们需要做服务降级​。 我们在设计高并发系统时，可以预留一些服务降级的开关。 比如在秒杀系统中，核心的功能是商品的秒杀，对于商品的评论功能，可以暂时屏蔽掉。 在服务端的分布式配置中心，比如：apollo中，可以增加一个开关，配置是否展示评论功能，默认是true。 前端页面通过服务器的接口，获取到该配置参数。 如果需要暂时屏蔽商品评论功能，可以将apollo中的参数设置成false。 此外，我们在设计高并发系统时，还可以预留一些兜底方案。 比如某个分类查询接口，要从redis中获取分类数据，返回给用户。但如果那一条redis挂了，则查询数据失败。 这时候，我们可以增加一个兜底方案。 如果从redis中获取不到数据，则从apollo中获取一份默认的分类数据。 目前使用较多的熔断降级中间件是：Hystrix​ 和 Sentinel​。 Hystrix是Netflix开源的熔断降级组件。 Sentinel是阿里中间件团队开源的一款不光具有熔断降级功能，同时还支持系统负载保护的组件。 二者的区别如下图所示： 降级确实了解的不多 15 故障转移在高并发的系统当中，同一时间有大量的用户访问系统。 如果某一个应用服务器节点处于假死状态，比如CPU使用率100%了，用户的请求没办法及时处理，导致大量用户出现请求超时的情况。 如果这种情况下，不做任何处理，可能会影响系统中部分用户的正常使用。 这时我们需要建立故障转移​机制。 当检测到经常接口超时，或者CPU打满，或者内存溢出的情况，能够自动重启那台服务器节点上的应用。 在SpringCloud微服务当中，可以使用Ribbon​做负载均衡器。 Ribbon是Spring Cloud中的一个负载均衡器组件，它可以检测服务的可用性，并根据一定规则将请求分发至不同的服务节点。在使用Ribbon时，需要注意以下几个方面： 设置请求超时时间，当请求超时时，Ribbon会自动将请求转发到其他可用的服务上。 设置服务的健康检查，Ribbon会自动检测服务的可用性，并将请求转发至可用的服务上。 此外，还需要使用Hystrix​做熔断处理。 Hystrix是SpringCloud中的一个熔断器组件，它可以自动地监测所有通过它调用的服务，并在服务出现故障时自动切换到备用服务。在使用Hystrix时，需要注意以下几个方面： 设置断路器的阈值，当故障率超过一定阈值后，断路器会自动切换到备用服务上。 设置服务的超时时间，如果服务在指定的时间内无法返回结果，断路器会自动切换到备用服务上。到其他的能够正常使用的服务器节点上。 16 异地多活有些高并发系统，为了保证系统的稳定性，不只部署在一个机房当中。 为了防止机房断电，或者某些不可逆的因素，比如：发生地震，导致机房挂了。 需要把系统部署到多个机房。 我们之前的游戏登录系统，就部署到了深圳、天津和成都，这三个机房。 这三个机房都有用户的流量，其中深圳机房占了40%，天津机房占了30%，成都机房占了30%。 如果其中的某个机房突然挂了，流量会被自动分配到另外两个机房当中，不会影响用户的正常使用。 这就需要使用异地多活​​架构了。 用户请求先经过第三方的DNS服务器解析，然后该用户请求到达路由服务器，部署在云服务器上。 路由服务器，根据一定的算法，会将该用户请求分配到具体的机房。 问题也来了：异地多活的难度是多个机房需要做数据同步，如何保证数据的一致性？ 这个好像是多数据中心的问题，是否可以参考多集群下的数据同步，缓存同步同理。比如，通常一个mysql集群有一主多从构成。用户的数据都是写入主库Master，Master将数据写入到本地二进制日志binary log中。从库Slave启动一个IO线程(I/O Thread)从主从同步binlog，写入到本地的relay log中，同时slave还会启动一个SQL Thread，读取本地的relay log，写入到本地，从而实现数据同步。 17 压测高并发系统，在上线之前，必须要做的一件事是做压力测试​。 我们先要预估一下生产环境的请求量，然后对系统做压力测试，之后评估系统需要部署多少个服务器节点。 比如预估有10000的qps，一个服务器节点最大支持1000pqs，这样我们需要部署10个服务器节点。 但假如只部署10个服务器节点，万一突增了一些新的用户请求，服务器可能会扛不住压力。 因此，部署的服务器节点，需要把预估用户请求量的多一些，比如：按3倍的用户请求量来计算。 这样我们需要部署30个服务器节点。 压力测试的结果跟环境有关，在dev环境或者test环境，只能压测一个大概的趋势。 想要更真实的数据，我们需要在pre环境，或者跟生产环境相同配置的专门的压测环境中，进行压力测试。 目前市面上做压力测试的工具有很多，比如开源的有：Jemter、LoaderRunnder、Locust等等。 收费的有：阿里自研的云压测工具PTS。 18 监控监控系统！ 为了出现系统或者SQL问题时，能够让我们及时发现，我们需要对系统做监控。 目前业界使用比较多的开源监控系统是：Prometheus​。 它提供了 监控​ 和 预警​ 的功能。 架构图如下：​ 我们可以用它监控如下信息： 接口响应时间 调用第三方服务耗时 慢查询sql耗时 cpu使用情况 内存使用情况 磁盘使用情况 数据库使用情况 等等······ 它的界面大概长这样子： 可以看到mysql当前qps，活跃线程数，连接数，缓存池的大小等信息。 如果发现数据量连接池占用太多，对接口的性能肯定会有影响。 这时可能是代码中开启了连接忘了关，或者并发量太大了导致的，需要做进一步排查和系统优化。 截图中只是它一小部分功能，如果你想了解更多功能，可以访问 Prometheus的官网 其实，高并发的系统中，还需要考虑安全问题，比如： 遇到用户不断变化ip刷接口怎办？ 遇到用户大量访问缓存中不存在的数据，导致缓存雪崩怎么办？ 如果用户发起ddos攻击怎么办？ 用户并发量突增，导致服务器扛不住了，如何动态扩容？ promethues+grafana 搭建起来确实可以监测很多方面的数据，服务器运行、数据库运行等等均可以很直观的展现，很利于微服务下的项目管控。 ‍","link":"/post/high-combined-design-thinking-system-11vul.html"},{"title":"My Blog ’s Plan","text":"My Blog ’s Plan 早在之前就有过建立自己的博客，但没有坚持下来。现重操旧业，坚持每天输出一篇Blog 整体思路，标签用来关键词联想与提示，或者自成体系的一套内容，分类为专题系列。 ‍ timeline：时间线，结合思源dailynote记录 标签：主要是一些主题性相关的，可能会很杂 例如：十大排序算法Ten-Sorts​​、杂记​​、英语​​、前沿技术​​ 分类： Golang 重学Go（偏基础重新巩固） Golang门面担当（常用的一些底层或者核心） 并发编程（并发相关） 数据结构与算法（偏入门，但是成体系） 架构设计师（面向考试与架构理解） MySQL Redis MQ ELK Docker 分布式 微服务 计算机底层（操作系统、网络、等一些综合性的理解） 🏳️‍🌈 秉持原则: ​ 拒绝无脑CV，深恶痛绝 CSDN 低劣文章，没头没尾，浪费时间 对于容易搜索的到内容有三点：一是要站在巨人的肩膀上总结和理解；二是确实很重点的部分才值得重复做；三自己的新学到内容，可能很粗浅，没有深刻领会，领会后可删除 将精力放在核心上，而不是排版、工具等无意义的点上 详情： A detailed list ‍ ‍","link":"/post/my-blog-content-planning-and-current-plan-z1tjjzq.html"},{"title":"分布式晋级之路","text":"我的分布式目标 理论 理解分布式的产生意义 分布式CAP、BASE理论 分布式算法 Paxos Raft 一致性Hash ZAB 分布式事务与锁 分布式事务背景与理论 分布式事务解决方案 分布式锁应用 实现分布式锁 分布式服务 API网关 服务注册与发现 分布式监控 负载均衡 容器化和服务 微服务技术栈 Service Mesh 分布式存储 分库分表 读写分离 NoSQL应用 ES应用 分布式消息队列 消息队列应用场景 消费顺序、消息重复消费、消费模式 Kafka高性能 RocketMQ 分布式缓存 ….. 分布式高可用 常见技术手段 限流策略 服务性能指标 监控系统 日志系统 ….. ‍","link":"/post/my-distributed-goals-ktyod.html"},{"title":"Nacos相关记录","text":"Nacos相关记录 Nacos1.X与2.X有差异，目前基本使用2.X版本，也是推荐的版本 Nacos初次尝试…. ‍ ‍ 权限认证🔒开启权限认证： 注意 Nacos是一个内部微服务组件，需要在可信的内部网络中运行，不可暴露在公网环境，防止带来安全风险。 Nacos提供简单的鉴权实现，为防止业务错用的弱鉴权体系，不是防止恶意攻击的强鉴权体系。 如果运行在不可信的网络环境或者有强鉴权诉求，请参考官方简单实现做进行自定义插件开发。 修改nacos配置文件 ——这个时候再访问nacos控制台页面，则会直接报错。 因此，还需要再设置两个属性（数值可随便填） 12nacos.core.auth.server.identity.key=authKeynacos.core.auth.server.identity.value=nacosSecurty 这两个属性是auth的白名单，用于标识来自其他服务器的请求。 添加好这两个属性时页面就能正常访问了。 还需要再其他服务的配置文件中加上如下配置，这也就是服务注册的权限 （修改代码方式）注意：密码不要有特殊符号不然会报错 12spring.cloud.nacos.username=nacospring.cloud.nacos.password=nacos 🤡 此外还需要配置： NACOS_AUTH_TOKEN token 默认:SecretKey012345678901234567890123456789012345678901234567890123456789 ‍","link":"/post/nacos-configuration-and-use-zm1qbo.html"},{"title":"微服务下分库分表的思考","text":"微服务下分库分表的思考 分库分表的这个名词再常见不过了，一开始的理解不够，随着对mysql理解的加深。 关于这个问题目前我的整体前置思路是：Mysql的瓶颈在哪里——InnoDB存储引擎——MySQL单表存储的瓶颈以及瓶颈推演与测试——分库分表，内容不一定绝对，但是思路是完整、有迹可循的。 关联文章：Mysql单表存储数据量瓶颈推演（2000W左右） 目录 什么是分库分表？ 为什么需要分库分表？ 如何分库分表？ 什么时候开始考虑分库分表？ 分库分表会导致哪些问题？如何解决？ 分库分表中间件？ PS: 这里有个小插曲，关于分区的问题，这一点我是在架构师考试备考（数据库）中遇到的问题，mysql为什么好像从来没有听说分区的相关内容，经查资料才了解到数据库按理说是支持分区的，所谓的是将一个表按照一定规则水平划分成多个子表，每个子表存储一部分数据。分区是针对单个表的，个人理解上是物理上可能跨磁盘、跨系统，但本质上还是一张逻辑表，主要的目的是提高查询效率和管理大型表的数据，减少索引长度和IO操作等问题。MySQL不支持分区，但是可以通过其他方式来实现分区的功能。比如，可以通过应用程序来实现分区的功能，或者使用其他数据库管理系统，比如Oracle、SQL Server等，它们都支持分区。MySQL不支持分区是由于历史原因、成本问题和性能问题所致。追根揭底还是技术栈范围扩大后，很多概念是宏观的，很难保证一致性。比如说只聊数据库，会聊关系模式、关系代数、-（前面这些都是关系数据库讲的，我只用Redis的话谈什么这些）数据库设计、聊数据库优化技术，这些东西太过宏观，像是“基础”可我本质上还是觉得完全就不是一类东西，只能算是时代的眼泪，曾经的思想参考。 1. 什么是分库分表分库：就是一个数据库分成多个数据库，部署到不同机器。单体架构下就没有分库分表，比较单一，SOA到微服务的发展中演进的 ​​ 分表：就是一个数据库表分成多个表。 ​​ 2.为什么需要分库分表呢？如果业务量剧增，数据库可能会出现性能瓶颈，这时候我们就需要考虑拆分数据库。从这几方面来看： 磁盘存储（很容易想到，这边便硬件层面的因素） 业务量剧增，MySQL单机磁盘容量会撑爆，拆成多个数据库，磁盘使用率大大降低。 并发连接支撑（这个是mysql本身的瓶颈） 我们知道数据库连接是有限的。在高并发的场景下，大量请求访问数据库，MySQL单机是扛不住的！当前非常火的微服务架构出现，就是为了应对高并发。它把订单、用户、商品等不同模块，拆分成多个应用，并且把单个数据库也拆分成多个不同功能模块的数据库（订单库、用户库、商品库），以分担读写压力。 为什么需要分表？ 数据量太大的话，SQL的查询就会变慢。如果一个查询SQL没命中索引，千百万数据量的表可能会拖垮这个数据库。 即使SQL命中了索引，如果表的数据量超过一千万的话，查询也是会明显变慢的。这是因为索引一般是B+树结构，数据千万级别的话，B+树的高度会增高，查询就变慢啦。 Mysql单表存储数据量瓶颈推演（2000W左右） MySQL的B+树的高度怎么计算的呢？ InnoDB存储引擎最小储存单元是页，一页大小就是16k。B+树叶子存的是数据，内部节点存的是键值+指针。索引组织表通过非叶子节点的二分查找法以及指针确定数据在哪个页中，进而再去数据页中找到需要的数据，B+树结构图如下： ​​ 假设B+树的高度为2的话，即有一个根结点和若干个叶子结点。这棵B+树的存放总记录数为=根结点指针数*单个叶子节点记录行数。 如果一行记录的数据大小为1k，那么单个叶子节点可以存的记录数 =16k/1k =16​ 非叶子节点内存放多少指针呢？我们假设主键ID为bigint类型，长度为8字节(面试官问你int类型，一个int就是32位，4字节)，而指针大小在InnoDB源码中设置为6字节，所以就是 8+6=14 ​​字节，16k/14B =16*1024B/14B = 1170​ 因此，一棵高度为2的B+树，能存放1170 * 16=18720​条这样的数据记录。 同理一棵高度为3​的B+树，能存放1170 *1170 *16 =21902400​，大概可以存放两千万左右的记录。B+树高度一般为1-3层，如果B+到了4层，查询的时候会多查磁盘的次数，SQL就会变慢。 因此单表数据量超过千万-&gt;就需要考虑分表啦。 这个地方从两种角度分析：颗粒度不一样，层面也不一样 1.页的细节角度 16K的页内结构 ​​ ​​ 这种角度： 非叶子节点内指向其他页的数量为 x 叶子节点内能容纳的数据行数为 y B+ 数的层数为 z 页的结构，索引也也不例外，都会有 File Header (38 byte)、Page Header (56 Byte)、Infimum + Supermum（26 byte）、File Trailer（8byte）, 再加上页目录，大概 1k 左右。 我们就当做它就是 1K, 那整个页的大小是 16K, 剩下 15k 用于存数据，在索引页中主要记录的是主键与页号，主键我们假设是 Bigint (8 byte), 而页号也是固定的（4Byte）, 那么索引页中的一条数据的大小=8+4=12byte。15K=15*1024B 所以 非叶子节点内指向其他页的数量x ​=15*1024/12≈1280 行。 叶子节点和非叶子节点的结构是一样的，同理，能放数据的空间也是 15k。 但是叶子节点中存放的是真正的行数据，这个影响的因素就会多很多，比如，字段的类型，字段的数量。每行数据占用空间越大，页中所放的行数量就会越少。 这边我们暂时按一条行数据 1k 来算，那一页就能存下 15 条，Y = 15*1024/1000 ≈15。 算到这边了，是不是心里已经有谱了啊。 Total 总数据行数 根据上述的公式，Total =x^(z-1) *y，已知 x=1280，y=15： 假设 B+ 树是两层，那就是 z = 2， Total = （1280 ^1 ）*15 = 19200 假设 B+ 树是三层，那就是 z = 3， Total = （1280 ^2） *15 = 24576000 （约 2.45kw） ​​ 2.磁盘块角度（上述即是）： ​​ ‍ 3. 如何分库分表 水平即行，垂直即列，一横一竖，横即拆行，竖即拆列 3.1 垂直拆分​​ 3.1.1 垂直分库在业务发展初期，业务功能模块比较少，为了快速上线和迭代，往往采用单个数据库来保存数据。数据库架构如下： ​​ 但是随着业务蒸蒸日上，系统功能逐渐完善。这时候，可以按照系统中的不同业务进行拆分，比如拆分成用户库、订单库、积分库、商品库，把它们部署在不同的数据库服务器，这就是垂直分库。 垂直分库，将原来一个单数据库的压力分担到不同的数据库，可以很好应对高并发场景。数据库垂直拆分后的架构如下： ​​ 3.1.2 垂直分表如果一个单表包含了几十列甚至上百列，管理起来很混乱，每次都select *​的话，还占用IO资源。这时候，我们可以将一些不常用的、数据较大或者长度较长的列拆分到另外一张表。 比如一张用户表，它包含user_id、user_name、mobile_no、age、email、nickname、address、user_desc​，如果email、address、user_desc​等字段不常用，我们可以把它拆分到另外一张表，命名为用户详细信息表。这就是垂直分表： ​​ 3.2 水平拆分3.2.1 水平分库水平分库是指，将表的数据量切分到不同的数据库服务器上，每个服务器具有相同的库和表，只是表中的数据集合不一样。它可以有效的缓解单机单库的性能瓶颈和压力。 用户库的水平拆分架构如下： ​​ 3.2.2 水平分表如果一个表的数据量太大，可以按照某种规则（如hash取模、range​），把数据切分到多张表去。 一张订单表，按时间range​拆分如下： ​​ 3.3. 水平分库分表策略分库分表策略一般有几种，使用与不同的场景： range范围 hash取模 range+hash取模混合 3.3.1 range范围range，即范围策略划分表。比如我们可以将表的主键，按照从0~1000万​的划分为一个表，1000~2000万​划分到另外一个表，以此类推。如下图： ​​ 当然，有时候我们也可以按​时间范围来划分，如不同年月的订单放到不同的表，它也是一种range的划分策略。 这种方案的优点： 这种方案有利于扩容，不需要数据迁移。假设数据量增加到5千万，我们只需要水平增加一张表就好啦，之前0~4000万​的数据，不需要迁移。 缺点： 这种方案会有热点问题，因为订单id是一直在增大的，也就是说最近一段时间都是汇聚在一张表里面的。比如最近一个月的订单都在1000万~2000​万之间，平时用户一般都查最近一个月的订单比较多，请求都打到order_1​表啦，这就导致表的数据热点问题。 3.3.2 hash取模hash取模策略：指定的路由key（一般是user_id、订单id作为key）对分表总数进行取模，把数据分散到各个表中。 比如原始订单表信息，我们把它分成4张分表： ​​ 比如id=1，对4取模，就会得到1，就把它放到第1张表，即t_order_0​; id=3，对4取模，就会得到3，就把它放到第3张表，即t_order_2​; 这种方案的优点： hash取模的方式，不会存在明显的热点集中问题。 缺点： 如果一开始按照hash取模分成4个表了，未来某个时候，表数据量又到瓶颈了，需要扩容，这就比较棘手了。比如你从4张表，又扩容成8​张表，那之前id=5​的数据是在（5%4=1​，即第一张表），现在应该放到（5%8=5​，即第5​张表），也就是说历史数据要做迁移了。 3.3.3 range+hash取模混合既然range存在热点数据问题，hash取模扩容迁移数据比较困难，我们可以综合两种方案一起嘛，取之之长，弃之之短。 比较简单的做法就是，在拆分库的时候，我们可以先用range范围方案，比如订单id在04000万的区间，划分为订单库1，id在4000万8000万的数据，划分到订单库2,将来要扩容时，id在8000万~1.2亿的数据，划分到订单库3。然后订单库内，再用hash取模的策略，把不同订单划分到不同的表。 ​​ 4. 什么时候考虑分库分表？前提：能不能切分就不要分，分了会极大的导致系统的复杂性。避免”过度设计”和”过早优化”。 在分库分表之前，不要为分而分，先尽力去做力所能及的事情，例如：升级硬件、升级网络、读写分离、索引优化等等。当数据量达到单表的瓶颈时候，再考虑分库分表。 4.1 什么时候分表？ 如果你的系统处于快速发展时期，如果每天的订单流水都新增几十万，并且，订单表的查询效率明变慢时，就需要规划分库分表了。一般B+树索引高度是2~3层最佳，如果数据量千万级别，可能高度就变4层了，数据量就会明显变慢了。不过业界流传，一般500万数据就要考虑分表了，属于提前考虑，留好空间。 4.2 什么时候分库 这一点我的理解是看业务量，微服务架构下，微服务特别多，或者很多重要的业务要维护高并发高可用的要求，就要分库，比如说重点的业务单体抽取出来单独分库。 业务发展很快，还是多个服务共享一个单体数据库，数据库成为了性能瓶颈，就需要考虑分库了。比如订单、用户等，都可以抽取出来，新搞个应用（其实就是微服务思想），并且拆分数据库（订单库、用户库）。 综合来讲，考虑分库分表的无非以下方面： ① 数据量快速增长，当业务中数据量急速增长时 ② 维护困难时：比如，备份时较为困难，单表太大，备份时需要大量的磁盘IO和网络IO；对一个很大的表进行DDL修改时，MySQL会锁住全表，这个时间会很长，这段时间业务不能访问此表，影响很大。如果使用pt-online-schema-change，使用过程中会创建触发器和影子表，也需要很长的时间。在此操作过程中，都算为风险时间。将数据表拆分，总量减少，有助于降低这个风险；经常访问与更新，就更有可能出现锁等待，将数据切分，用空间换时间，变相降低访问压力。 ③ 业务需求：需要对某些字段垂直拆分，本质上还是因用户增多导致的业务特点发生了变化 举个例子，假如项目一开始设计的用户表如下： 12345id bigint #用户的IDname varchar #用户的名字last_login_time datetime #最近登录时间personal_info text #私人信息..... #其他信息字段 在项目初始阶段，这种设计是满足简单的业务需求的，也方便快速迭代开发。而当业务快速发展时，用户量从10w激增到10亿，用户非常的活跃，每次登录会更新 last_login_name 字段，使得 user 表被不断update，压力很大。 而其他字段：id, name, personal_info 是不变的或很少更新的，此时在业务角度，就要将 last_login_time 拆分出去，新建一个 user_time 表。 personal_info 属性是更新和查询频率较低的，并且text字段占据了太多的空间。这时候，就要对此垂直拆分出 user_ext 表了 ④ 安全性角度：鸡蛋不要放在一个篮子里。在业务层面上垂直切分，将不相关的业务的数据库分隔，因为每个业务的数据量、访问量都不同，不能因为一个业务把数据库搞挂而牵连到其他业务。利用水平切分，当一个数据库出现问题时，不会影响到100%的用户，每个库只承担业务的一部分数据，这样整体的可用性就能提高。 5. 分库分表会导致哪些问题分库分表之后，也会存在一些问题： 事务问题 跨库关联 排序问题 分页问题 分布式ID 数据迁移、扩容问题 5.1 事务问题 分库分表后，假设两个表在不同的数据库，那么本地事务已经无效啦，需要使用分布式事务了。 5.2 跨库关联 跨节点Join的问题：解决这一问题可以分两次查询实现； 冗余处理，反规范化设计： 增加冗余列(复制某一列的数据) 这一点是真的好用 增加派生列(计算总和，平均值..) 表合并(把部分来自不同表的常用列合并成新表) 表分割(把数据拆分为常用和不常用，行拆分是比如订单信息，列拆分比如账户信息&lt;额外的住址之类的并不常用，减少查询压力&gt;) 题中要求是商品信息冗余，所以应该采用&lt;增加冗余列&gt;的方法 5.3 排序问题 跨节点的count,order by,group by以及聚合函数等问题：可以分别在各个节点上得到结果后在应用程序端进行合并。 5.4 分页问题 方案1：在个节点查到对应结果后，在代码端汇聚再分页。 方案2：把分页交给前端，前端传来pageSize和pageNo，在各个数据库节点都执行分页，然后汇聚总数量前端。这样缺点就是会造成空查，如果分页需要排序，也不好搞。 5.5 分布式ID 据库被切分后，不能再依赖数据库自身的主键生成机制啦，最简单可以考虑UUID，或者使用雪花算法生成分布式ID。 UUID: UUID标准形式包含32个16进制数字，分为5段，形式为8-4-4-4-12的32个字符，例如：550e8400-e29b-41d4-a716-446655440000 UUID是主键是最简单的方案，本地生成，性能高，没有网络耗时。但缺点也很明显，由于UUID非常长，会占用大量的存储空间；另外，作为主键建立索引和基于索引进行查询时都会存在性能问题，在InnoDB下，UUID的无序性会引起数据位置频繁变动，导致分页。 雪花算法： Twitter的snowflake算法解决了分布式系统生成全局ID的需求，生成64位的Long型数字，组成部分： 第一位未使用 接下来41位是毫秒级时间戳，41位的长度可以表示69年的时间 5位datacenterId，5位workerId。10位的长度最多支持部署1024个节点 最后12位是毫秒内的计数，12位的计数顺序号支持每个节点每毫秒产生4096个ID序列 ​​ 好处：毫秒数在高位，生成的ID整体上按时间趋势递增；不依赖第三方系统，稳定性和效率较高，理论上QPS约为409.6w/s（1000*2^12），并且整个分布式系统内不会产生ID碰撞；可根据自身业务灵活分配bit位。 不足：强依赖机器时钟，如果时钟回拨，则可能导致生成ID重复。 综上，结合数据库和snowflake的唯一ID方案，可以参考业界较为成熟的解法：Leaf——美团点评分布式ID生成系统，并考虑到了高可用、容灾、分布式下时钟等问题。 数据迁移、扩容问题： 当业务高速发展，面临性能和存储的瓶颈时，才会考虑分片设计，此时就不可避免的需要考虑历史数据迁移的问题。一般做法是先读出历史数据，然后按指定的分片规则再将数据写入到各个分片节点中。 此外还需要根据当前的数据量和QPS，以及业务发展的速度，进行容量规划，推算出大概需要多少分片（一般建议单个分片上的单表数据量不超过1000W）。 如果采用数值范围分片，只需要添加节点就可以进行扩容了，不需要对分片数据迁移。如果采用的是数值取模分片，则考虑后期的扩容问题就相对比较麻烦。 6. 分库分表中间件目前流行的分库分表中间件比较多： cobar Mycat Sharding-JDBC（当当） Atlas TDDL（淘宝） vitess（谷歌开发的数据库中间件） ​ ​ 参考资料：如何分库分表！ ‍","link":"/post/my-thinking-about-the-mysql-database-table-zmjl3t.html"},{"title":"重学Go语言 | 如何在Go中使用Context","text":"重学Go语言 | 如何在Go中使用Context我们知道在开发Go应用程序时，尤其是网络应用程序，需要启动大量的Goroutine​来处理请求： ​​ 不过Goroutine​被创建之后，除非执行后正常退出或者触发panic​退出，Go并没有提供在一个Goroutine​中关闭另一个Goroutine​的机制。 有没有一种方法，可以从一个Goroutine​中通知另一个Goroutine​退出执行呢？这时候就该Context​登场了！ 什么是Context？​Context​，中文叫做上下文​，Go语言在1.7​版本中新增的context​包中定义了Context​，Context​本质是一个接口，这个接口一共定义了四个方法： 123456type Context interface { Deadline() (deadline time.Time, ok bool) Done() &lt;-chan struct{} Err() error Value(key any) any} ​Dateline()​：获取定时关闭的时间。 ​Done()​：从一个channel​获取关闭的信号 ​Err()​：获取错误信息。 ​Value()​：根据key​从Context​取值 Context的作用为什么要使用Context​？或者说设计Context​的目的是什么？ 想像一下这样的场景，当用户开启浏览器访问我们的Web​服务时，我们可能会开启多个Goroutine​来处理用户的请求，这些Goroutine​需要读取不同的资源，最终返回给用户，但如果用户在我们的Web服务还没处理完成就关闭浏览器，断开连接，而此时Web不知道用户已经关闭请求，仍然在处理并返回最终并不会被接收的数据。 ​Context​设计的目的就是可以从上游的Goroutine​发送信息给下游的Goroutine​，回到处理用户请求的场景，当处理请求的Goroutine​发现用户断开连接时，通过Context​发送停止执行的信息，而下游的Goroutine​得到停止信号时就返回，避免资源的浪费。 概括起来，Context​的作用主要体现在两个方面： 在Goroutine​之间传递关闭信息，定时关闭，超时关闭，手动关闭。 在Goroutine​之间传递数据。 Context的使用下面我们来讲讲Context​的基本使用。 创建Context任何上下文都是从一个空白的Context​开始的，创建一个空白的Context​有两种方式： 使用context.Background()​： 1ctx := context.Backgroud() 使用contenxt.TODO()​: 1ctx := context.TODO() 当然大部分时候，我们不需要自己创建一个空白的Context​，比如在处理HTTP​请求时： 123http.HandleFunc(&quot;/hello&quot;, func(w http.ResponseWriter, r *http.Request) { ctx := r.Context()}) 上面的代码中，可以从http.Request​中获取Context​实例，而http.Request​的实际上也是调用context.Backgroud()​: 1234567//net/http/request.gofunc (r *Request) Context() context.Context { if r.ctx != nil { return r.ctx } return context.Background()} 实例讲解为了讲解Context​是如何在Goroutine​之间传递信号与数据，我们通过下面的案例进行说明： 这段代码是一个读取文件内容的函数ReadFile，它接受一个上下文对象ctx、文件名fileName和一个通道result用于返回读取到的文件内容。首先，通过os.Open函数打开文件，并在出现错误时返回。然后，创建一个空的totalResult切片，用于存储整个文件的内容。接下来，进入一个无限循环，在循环中使用select语句监听上下文的完成事件。如果上下文被取消或超时，将空切片[]byte{}发送到result通道，并返回函数。如果上下文没有完成，则继续执行循环体。在每次循环中，使用file.Read函数读取1024字节的数据到切片b中，并检查是否遇到了文件结束（EOF）错误。如果是，则将切片b的内容追加到totalResult切片中，并跳出循环。如果没有遇到文件结束错误，将切片b的内容追加到totalResult切片中，然后继续循环。当循环结束后，将完整的totalResult切片发送到result通道中，函数执行完毕。 1234567891011121314151617181920212223242526func ReadFile(ctx context.Context, fileName string, result chan&lt;- []byte) { file, err := os.Open(fileName) if err != nil { return } totalResult := make([]byte, 0) for { select { case &lt;-ctx.Done(): result &lt;- []byte{} return // default分支为空，这是为了确保在没有收到上下文完成事件时，循环不会阻塞在select语句上 default: } b := make([]byte, 1024) //每次循环读取1024字节的数据到切片b中 _, err := file.Read(b) if err == io.EOF { totalResult = append(totalResult, b...) break } totalResult = append(totalResult, b...) } result &lt;- totalResult} 在上面的程序中，我们调用Context​的Done()​方法，该方法会返回一个Channel​，而使用select​语句则可以让我们在处理业务的同时，监听上游Goroutine​是否有传递取消执行的信息。 手动取消：WithCancel设置Context传递停止信号空白的Context​并不能发挥什么作用，要达到手动取消执行的目的，需要调用context​包下的WithCancel​函数进行封装，封装返回一个新的context​以及一个取消的句柄cancel​函数： 12ctx := context.Background()ctx, cancel := context.WithCancel(ctx) 下面是完整的使用方法：将context对象传入到协程函数中，手动调用cancel进行取消 1234567891011121314151617181920package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;io&quot; &quot;os&quot; &quot;time&quot;)func main() { ctx := context.Background() ctx, cancel := context.WithCancel(ctx) result := make(chan []byte) // 100ms后此协程被手动cancel取消 go ReadFile(ctx, &quot;./test.tar&quot;, result) time.Sleep(100 * time.Millisecond) cancel() fmt.Println(&lt;-result)} 在上面的程序中，我们创建一个Context​之后，传给了ReadFile​函数，并且在暂停100毫秒后调用cancel()​函数，达到手动取消另一个Goroutine​的目的。 截止时间：WithDeadline给Context设置一个截止时间除了手动取消，也可以调用context​包下的WithDeadline()​函数给Context​加一个截止时间，这样在某个时间点，Context​会自动发出取消信号： 12afterTime := time.Now().Add(30 * time.Millisecond)ctx, cancel := context.WithDeadline(context.Background(), afterTime) 下面是完整的示例： 123456789101112131415161718package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;io&quot; &quot;os&quot; &quot;time&quot;)func main() { afterTime := time.Now().Add(30 * time.Millisecond) ctx, cancel := context.WithDeadline(context.Background(), afterTime) defer cancel() result := make(chan []byte) go ReadFile(ctx, &quot;./test.tar&quot;, result) fmt.Println(&lt;-result)} 超时取消：WithTimeout给Context一个超时时间调用context​包下的WithTimeout()​函数可以为Context​加一个超时限制，这对于我们编写超时控制程序非常有帮助： 1ctx, cancel := context.WithTimeout(context.Background(), 10*time.Millisecond) 下面是完整的调用程序： 1234567891011121314151617package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;io&quot; &quot;os&quot; &quot;time&quot;)func main() { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Millisecond) defer cancel() result := make(chan []byte) go ReadFile(ctx, &quot;./test.tar&quot;, result) fmt.Println(&lt;-result)} 传递数据：使用Context传递数据调用context​包下的WithValue()​函数可以生成一个携带数据的Context​，这个机制方便我们跟踪一个处理流程中的Goroutine​： 1 ctx, cancel := context.WithValue(context.Background(), &quot;testKey&quot;,&quot;testValue&quot;) 下游的Goroutine​就可以通过Context​的Value()​函数来获取上游传递下来的值了。 123func MyGoroutine(ctx context.Context){ ctx.Value(&quot;testKey&quot;)} 思考：这里联想下业务场景 使用Context的几点建议 ​Context​不要放在结构体中，需要以参数方式传递 ​Context​作为函数参数时，一般放在第一位，作为函数的第一个参数 使用 context.Background​函数生成根节点的Context​ ​Context ​要传值必要的值，不要什么都传 ​Context​ 是多协程安全的，可以在多个协程中使用 小结至此，应该对Context​有所了解了吧，总的来说，通过Context​可以做到： WithCancel手动控制下游Goroutine​取消执行。 WithDeadline定时控制下游Goroutine​取消执行。 WithTimeout超时控制下游Goroutine​取消执行。 WithValue传递数据给下游的Goroutine​。","link":"/post/re-learning-go-language-how-to-use-context-in-go-ckuaa.html"},{"title":"重学Go语言 | Map","text":"重学Go语言 | Map Go重学系列不追求底层和深度，只求重温下没有使用过或者使用过的一些操作，要注意的细节，以及彻底掌握的一种前提思路。后续讲重学与底层部分练习在一起进行双链操作。 重学篇在自己重温Go的同时，也希望努力做最好的初学教程 本篇思路：什么是map、map的格式与数据类型限制、map的特征、如何创建与初始化等 Map简述123Go语言中的map(映射、字典、哈希表)是一种内置的数据结构，它是一个无序的key-value对的集合，比如以身份证号作为唯一键来标识一个人的信息。Go语言中并没有提供一个set类型，但是map中的key也是不相同的，可以用map实现类似set的功能。 从表面上看map大致是这样的： ​​ ​​ 从底层看： ​map​是一个无序的键值对(key-value​)集合，其底层数据结构是一个哈希表，通过哈希函数，将key​转换为对哈希表中的索引，将value​存储到索引对应的位置，在map​中查找、删除、查找value的时间复杂度O(1)​。 ​​ map格式与数据类型限制关键点：key数据类型限制 map的value可以是Go支持的任意数据类型，而key则所有限制： key的数据类型必须是可以使用​ = ​和​ != ​进行比较 所以，key不能是函数、切片、map，因此这些数据类型不能进行比较，另外，而数组和结构体则可以作为map的key，不过，如果数组的元素包含函数、切片、map，则数组不能作为map的key，结构体的字段如果有以上三者，也同样不能作为map的key。 12345678910111213141516type Test struct { ID string Name string}//正确m := map[Test]int{}type Test struct { ID string Name string scores []int}//报错m := map[Test]int{} key的值必须是唯一，同一个map中不能相同的两个key 12345m := map[string]string{ &quot;name&quot;:&quot;小明&quot;, &quot;age&quot;:&quot;24&quot;, &quot;name&quot;:&quot;小墨&quot;//报错，不能有相同的key} 123//在Go语言中，用 map[KeyType]ValueType 表示一个map，//其中，KeyType 表示 key 的数据类型，ValueType 表示 value 的数据类型：map[keyType]valueType 12345// ※ 重点 ※ // 在一个 map 里所有的键都是唯一的，而且必须是支持 == 和 != 操作符的类型，// 切片、函数以及包含切片的结构类型这些类型由于具有引用语义，不能作为映射的键，// 使用这些类型会造成编译错误：dict := map[ []string ]int{} //err, invalid map key type []string —✩ ✰ ✪ ✫— map值可以是任意类型，没有限制。map里所有键的数据类型必须是相同的，值也必须如此，但键和值的数据类型可以不相同。 注意：​map是无序的，我们无法决定它的返回顺序，所以，每次打印结果的顺序有可能不同。 –&gt;这个地方就有深度了，为什么map遍历是无序的？ Java中的set也是如此吗？为什么？原理是否一样？ map的特征推导总结： map是无序的 map的key是唯一的 map是引用数据类型，因此在使用前必须初始化 函数，切片，map等数据类型不能作为map的key。key必须是可比较的类型，可以是结构体和数组，不能是切片、函数、map，且结构体和数组中也不能包含以上三者中的任何 创建及初始化Map很多教程都是直接告诉你如何初始化，可能只教一种实现方式，不教所以然，这样很片面 未经初始化由于map​是引用数据类型，其底层引用一个哈希表，因此未经初始化(即未分配到内存空间)的map​无法直接使用： 1234var m map[string]intm[&quot;a&quot;] = 1 //未初始化，报错 初始化方式初始化map有两种方式： 使用make函数 字面量初始化 刘丹冰：New与Make 理论上New也是可行的，但是十分不推荐 make函数内置函数make可以为map​类型的变量分配内存： 123m := make(map[string]string)m[&quot;name&quot;] = &quot;test&quot;m[&quot;age&quot;] = &quot;12&quot; 此外还可以指定容量初始化： 这种方式有好处也有劣势，好处是省去了频繁地扩容，坏处是如果一次性建立很大的容量但实际上并不需要会造成资源浪费 12m := make(map[int]string, 10) //第2个参数指定容量“10”fmt.Println(m) //map[] 这种方法指定了map的初始创建容量。与slice类似，后期在使用过程中，map可以自动扩容。 只不过map更方便一些，不用借助类似append的函数，直接赋值即可。如，m[17] = “Nami”。赋值过程中，key如果与已有map中key重复，会将原有map中key对应的value覆盖。但是！对于map而言，可以使用len()函数，但不能使用 cap() 函数。 map是一种可以动态增长的数据结构，但由于其底层实现的复杂性，能直接获取其容量，只能使用len()​函数来获取其元素数量。 字面量初始化 初始化同时赋值 通过字面量初始化map​时，可以给map​中的key​和value​赋初始值： 1234567891011// 这样直接指定初值，要保证key不重复。// 注意！这是常用的方式，使用“：”自动推导的方式m := map[string]string{ &quot;name&quot;:&quot;test&quot;, &quot;age&quot;:&quot;12&quot;,}// 当然了，还有一种不常用的方式：// 定义的同时完成初始化，但有一说一这种方式太不简洁了var m1 map[int]string = map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;} fmt.Println(m1) //map[1:Luffy 2:Sanji] 只初始化，不赋值 常用： 如果不想给map初始化数据，也可以声明一个空的map类型，自动分配了内存： 12// 这里的`m`是一个字符串键和字符串值类型的map，但你也可以根据需要更改其键和值类型。m := map[string]string{} 注意，空的map已经初始化好了，只是没有存入值而已，而直接声明一个map变量时，该map变量为nil，这两者不一样 不常用： 声明一个未初始化的map的方式如下： 1var m map[string]string 这里的m​是一个字符串键和字符串值类型的map变量，但它没有被初始化，因此不能直接使用，如果你尝试在其上执行读或写操作，会触发panic错误。要使用未初始化的map，还需要使用make函数对其进行初始化，如下所示： 12// var这种方式也不常用，因为还要再写这一行完成内存分配才能使用m = make(map[string]string) 这样，你的map就可以安全地使用了。 非要用new初始化123456789101112131415161718192021package mainimport &quot;fmt&quot;func main() { // 使用 new 创建 map myMap := new(map[string]int) // 初始化 map *myMap = make(map[string]int) // 向 map 中添加键值对 (*myMap)[&quot;apple&quot;] = 1 (*myMap)[&quot;banana&quot;] = 2 (*myMap)[&quot;orange&quot;] = 3 // 遍历 map 并打印键值对 for key, value := range *myMap { fmt.Println(key, &quot;:&quot;, value) }} ‍ 常用操作赋值与访问赋值赋值上来说：有直接初始化的同时直接赋值，也有先初始化，再赋值这两种 12345678910m1 := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;}m1[1] = &quot;Nami&quot; //修改m1[3] = &quot;Zoro&quot; //追加， go底层会自动为map分配空间fmt.Println(m1) //map[1:Nami 2:Sanji 3:Zoro]m2 := make(map[int]string, 10) //创建map，指定容量m2[0] = &quot;aaa&quot;m2[1] = &quot;bbb&quot;fmt.Println(m2) //map[0:aaa 1:bbb]fmt.Println(m2[0], m2[1]) //aaa bbb 访问通过key，可以访问map变量中的value： 1234567rank : = map[string]int{ &quot;PHP&quot;:90, &quot;Go&quot;:99 &quot;Java&quot;:95}fmt.Println(rank[&quot;PHP&quot;]) 如果对应的key不存在，则会返回对应value数据类型的空值，比如value为string，则返回空字符串： 123456789//因为value为intfmt.Println(rank[&quot;Python&quot;]) // 输出：0m := map[string]string{&quot;name&quot;:&quot;小张&quot;}//由于value为string类型fmt.Println(m[&quot;age&quot;]) //输出空字符串 判断key是否存在如果我们想在通过key访问map之前就确定对应的key是否存在，有另外一种写法： 1v, ok := m[k] 上面的表达式中，有第二个返回值ok​，该值为boolean类型，当key存在时，ok​的值为true，v​为对应的value​；否则为ok​为false​,v​为空值。 12345678910// 方法一v,ok := rank[k]if ok { fmt.Println(v)}// 方法二，这种更常见if v,ok := rank[k];ok{ fmt.Println(v)} 有时候可能需要知道对应的元素是否真的是在map之中。可以使用下标语法判断某个key是否存在。map的下标语法将产生两个值，其中第二个是一个布尔值，用于报告元素是否真的存在。如果key存在，第一个返回值返回value的值。第二个返回值为 true。 12345678910// 方法一：v,ok := rank[k]if ok { fmt.Println(v)}// 方法二：这种可能更常见if v,ok := rank[k];ok{ fmt.Println(v)} 遍历使用for-range​语句可以遍历map​，获得map​的key​和value​： 1234567m := map[string]string{ &quot;name&quot;:&quot;xiaoming&quot;, &quot;age&quot;:&quot;18岁&quot;}for k,v := range m{ fmt.Println(k,v)} 注意： Map的迭代顺序是不确定的，并且不同的[哈希函数]实现可能导致不同的遍历顺序。在实践中，遍历的顺序是随机的，每一次遍历的顺序都不相同。这是故意的，每次都使用随机的遍历顺序可以强制要求程序不会依赖具体的哈希函数实现。 1234567891011121314m1 := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;}//遍历1，第一个返回值是key，第二个返回值是valuefor k, v := range m1 { fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, v)//1 ----&gt; Luffy//2 ----&gt; yoyo }//遍历2，第一个返回值是key，第二个返回值是value（可省略）for k := range m1 { fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, m1[k])//1 ----&gt; Luffy//2 ----&gt; Sanji } 删除要删除map的value，可以使用Go内置的delete​函数，该函数格式如下： 1func delete(m map[KeyType]ValueType, key Type) 该函数的第一个参数是我们要操作的map类型的变量，第二个参数表示要删除哪个key： 12345678m := map[string]int{ &quot;a&quot;:1, &quot;b&quot;:2}fmt.Println(m)delete(m,&quot;a&quot;)fmt.Println(m) 使用delete()函数，指定key值可以方便的删除map中的k-v映射。 12345678910111213141516m1 := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;, 3: &quot;Zoro&quot;}for k, v := range m1 { //遍历，第一个返回值是key，第二个返回值是value fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, v) }//1 ----&gt; Sanji//2 ----&gt; Sanji//3 ----&gt; Zorodelete(m1, 2) //删除key值为2的mapfor k, v := range m1 { fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, v) }//1 ----&gt; Luffy//3 ----&gt; Zoro 123456789// 使用delete删除一个不存在的keydelete(m1, 5) //删除key值为5的mapfor k, v := range m1 { fmt.Printf(&quot;%d ----&gt; %s\\n&quot;, k, v) }//1 ----&gt; Luffy//3 ----&gt; Zoro Map输出结果依然是原来的样子，且不会有任何错误提示。 delete()操作是安全的，即使元素不在map中也没有关系；如果查找删除失败将返回value类型对应的零值。 获取长度map中无法获取容量，也就是无法使用cap函数，它的容量是动态变化的，内部是通过哈希表实现的，底层实现与数组、切片不同；同时，他们的扩容机制【slice与map】有何不同？ 要获取map的长度，同样是用内置的len​函数： 1234567var user = map[string]string{ &quot;id&quot;: &quot;0001&quot;, &quot;name&quot;: &quot;小张&quot;, &quot;age&quot;: &quot;18岁&quot;,}fmt.Println(len(user)) //输出：3 Map嵌套由于map​的value​并没有数据类型的限制，所以value​也可以是另一个map​类型： 理论上map嵌套map可以一直嵌套下去，但一般不会这么做的。很丑！ 123456mm := map[string]map[int]string{ &quot;a&quot;: {1: &quot;test1&quot;}, &quot;b&quot;: {2: &quot;test2&quot;}, &quot;c&quot;: {2: &quot;test3&quot;},}fmt.Println(mm) Map做函数参数与slice 相似，在函数间传递映射并不会制造出该映射的一个副本，不是[值传递]，而是引用传递 12345678910111213141516171819func DeleteMap(m map[int]string, key int) { delete(m, key) //删除key值为2的map for k, v := range m { fmt.Printf(&quot;len(m)=%d, %d ----&gt; %s\\n&quot;, len(m), k, v)}//len(m)=2, 1 ----&gt; Luffy//len(m)=2, 3 ----&gt; Zoro}func main() { m := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;, 3: &quot;Zoro&quot;} DeleteMap(m, 2) //删除key值为2的map for k, v := range m { fmt.Printf(&quot;len(m)=%d, %d ----&gt; %s\\n&quot;, len(m), k, v)}//len(m)=2, 1 ----&gt; Luffy//len(m)=2, 3 ----&gt; Zoro} Map做函数返回值返回的依然是引用 123456789101112131415func test() map[int]string {// m1 := map[int]string{1: &quot;Luffy&quot;, 2: &quot;Sanji&quot;, 3: &quot;Zoro&quot;} m1 := make(map[int]string, 1) // 创建一个初创容量为1的map m1[1] = &quot;Luffy&quot; m1[2] = &quot;Sanji&quot; // 自动扩容 m1[67] = &quot;Zoro&quot; m1[2] = &quot;Nami&quot; // 覆盖 key值为2 的map fmt.Println(&quot;m1 = &quot;, m1) return m1}func main() { m2 := test() // 返回值 —— 传引用 fmt.Println(&quot;m2 = &quot;, m2)} 输出： 12m1 = map[1:Luffy 2:Nami 67:Zoro]m2 = map[2:Nami 67:Zoro 1:Luffy] Map的排序​map​是无序的，所以每次遍历map​输出的顺序都不一定相同 1234567891011var user = map[string]string{ &quot;id&quot;: &quot;0001&quot;, &quot;name&quot;: &quot;小张&quot;, &quot;age&quot;: &quot;18岁&quot;,}for k, v := range user { fmt.Println(k, &quot;:&quot;, v)}for k, v := range user { fmt.Println(k, &quot;:&quot;,v)} 要有序地遍历一个map类型的变量，可以这么做： 利用切片将map中的key固定排序，这样就稳定了顺序 1234567891011121314order := []string{}for k, _ := range user { order = append(order, k)}// 对user中的key进行升序排序// sort.Strings(order)for _, v := range order { fmt.Println(user[v])}for _, v := range order { fmt.Println(user[v])} Map之间无法比较，只能与nil比较​map​类型变量之间不能进行比较，map​只能与nil​进行比较： 12345678910111213var m map[int]string//判断是否等于nilif m == nil{ fmt.Println(&quot;m hasn't been initialized&quot;)}m1 := map[string]string{&quot;name&quot;: &quot;小明&quot;}m2 := map[string]string{&quot;name&quot;: &quot;小明&quot;}//报错if m1 == m2 { fmt.Println(&quot;相等&quot;)} 这个地方我看到有的教程说：“nil map 和空 map 是相等的，只是 nil map 不能添加元素。”这个map之间不能被比较何来相等一说？对此有疑问 不能对Map的value进行取址操作取址操作作用： 当对数组和切片进行取址操作时，我们可以实现参数传递、修改元素和实现数据结构等功能。以下是一些示例： 参数传递： 12345678910111213package mainimport &quot;fmt&quot;func modifyElement(arr *[3]int) { (*arr)[0] = 100}func main() { array := [3]int{1, 2, 3} modifyElement(&amp;array) fmt.Println(array) // 输出 [100 2 3]} 在这个例子中，通过对数组元素取址，将数组作为指针传递给函数modifyElement​，在函数内部可以直接修改数组元素的值。 修改元素： 1234567891011package mainimport &quot;fmt&quot;func main() { slice := []int{1, 2, 3} fmt.Println(slice) // 输出 [1 2 3] slicePtr := &amp;slice (*slicePtr)[0] = 100 fmt.Println(slice) // 输出 [100 2 3]} 在这个例子中，通过对切片元素取址，可以直接修改切片中的元素值。 实现数据结构： 1234567891011121314151617package mainimport &quot;fmt&quot;type Node struct { Value int Next *Node}func main() { node1 := Node{Value: 1} node2 := Node{Value: 2} node1.Next = &amp;node2 fmt.Println(node1.Value) // 输出 1 fmt.Println(node1.Next.Value) // 输出 2} 在这个例子中，我们通过对结构体中的指针字段进行取址操作，实现了链表数据结构。 这些示例展示了如何使用数组和切片的取址操作来实现参数传递、修改元素和实现数据结构，从而充分展示了取址操作的用处。 Go的数组和切片允许对元素进行取址操作，但不允许对map的元素进行取址操作： 12345678910111213//对数组元素取址a := [3]int{1, 2, 3}fmt.Println(&amp;a[1])//对切片元素取址s := []int{1, 2, 3}fmt.Println(&amp;s[1])//对map元素取址，错误m := map[string]string{ &quot;test&quot;:&quot;test&quot;,}fmt.Println(&amp;m[&quot;test&quot;]) 为什么Go允许数组、切片进行取址操作，但要限制对map的元素取址呢？ 因为Go可以在添加新的键值对时更改键值对的内存位置。Go将在后台执行此操作，以将检索键值对的复杂性保持在恒定水平。因此，地址可能会变得无效，Go宁愿禁止访问一个可能无效的地址。 对map不允许直接进行取址操作的主要原因是为了避免潜在的数据竞争和安全问题。 在Go语言中，map是通过哈希表实现的，它的内部结构相对复杂。当我们对map进行取址操作时，实际上是获取了一个指向底层哈希表的指针。这样一来，如果在获取指针后对map进行了添加、删除或重新分配内存等操作，会导致指针指向的地址发生变化，从而可能使之前获取到的指针失效或指向无效的内存区域。这就会造成潜在的数据不一致性和安全隐患。 为了避免这种情况，Go语言禁止对map进行取址操作，并且在对map进行增删改等操作时，可能会触发内部的扩容和重新散列等操作，从而导致map的底层数据结构发生变化。如果允许对map进行取址操作，那么在并发访问的情况下，就可能引发竞态条件，导致数据不一致或安全问题。 相反，对数组和切片进行取址操作是允许的。这是因为数组和切片的内部结构相对简单，取址操作不会引发底层数据结构的变化。同时，数组和切片的元素是可以被直接修改的，因此可以对它们进行取址操作来实现参数传递、修改元素和实现数据结构等功能。在使用数组和切片的取址操作时，仍然需要注意并发安全和正确性问题，但相对于map，由于其内部结构的简单性，风险较小。 总而言之，在Go语言中，不允许对map进行取址操作是为了避免数据竞争和安全问题，而数组和切片则具备相对简单的内部结构和更明确的使用方式，因此允许进行取址操作。 补充 本部分有待商榷 什么时候需要用指针封装map在 Go 中，大部分情况下，不需要对 map 进行指针封装。因为 map 本身是引用类型，在函数传递时传递的是指向底层数据结构的指针。这意味着在函数间传递 map 时，传递的是其引用而不是值的拷贝，因此可以直接对 map 进行操作而无需额外封装为指针。 然而，有些情况下可能需要对 map 进行指针封装： 1. 需要在多个函数中修改同一个 map如果需要在多个函数中修改同一个 map 并且希望这些修改对所有函数都生效，可以使用指针封装。因为 map 是引用类型，传递指向 map 的指针可以确保在不同函数间共享相同的 map 实例，对 map 的修改可以在不同函数中体现出来。 2. 减少 map 的拷贝开销在某些情况下，当 map 很大并且需要在函数之间传递时，将 map 传递为指针可以减少拷贝的开销。大的 map 传递值拷贝可能会消耗较多的内存和时间，而传递指针则只需传递地址。 3. 需要避免 map 自身的nil指针问题在某些场景下，需要避免 map 本身可能出现 nil 指针的问题。通过使用指针封装 map，可以在 nil map 的情况下返回一个非 nil 的 map，避免出现空 map 操作时的错误。 当需要在多个函数中修改同一个 map 并确保修改对所有函数生效时，使用指针封装可以很有用。以下是示例： 1234567891011121314151617181920212223242526package mainimport &quot;fmt&quot;type MapStruct struct { Data map[string]int}func NewMapStruct() *MapStruct { return &amp;MapStruct{ Data: make(map[string]int), }}func AddData(m *MapStruct, key string, value int) { m.Data[key] = value}func main() { mapInstance := NewMapStruct() AddData(mapInstance, &quot;Key1&quot;, 10) AddData(mapInstance, &quot;Key2&quot;, 20) fmt.Println(mapInstance.Data) // 输出：map[Key1:10 Key2:20]} 上述代码中，AddData​ 函数接收一个 MapStruct​ 的指针作为参数，并向其 map 字段中添加键值对。在 main​ 函数中调用 AddData​ 两次来修改同一个 MapStruct​ 实例的 map，并最终输出修改后的 map 数据。 对于第二种情况，如果需要在多个函数之间传递大型的 map，通过指针封装可以避免大的 map 被复制多次，节省内存和提高效率。以下是一个例子： 1234567891011121314151617package mainimport &quot;fmt&quot;func ProcessMap(m *map[string]int) { // 对传入的 map 进行处理 (*m)[&quot;Key1&quot;] = 100 (*m)[&quot;Key2&quot;] = 200}func main() { myMap := map[string]int{} ProcessMap(&amp;myMap) fmt.Println(myMap) // 输出：map[Key1:100 Key2:200]} 在这个示例中，ProcessMap​ 函数接收一个 map[string]int​ 类型的指针，并对传入的 map 进行操作。在 main​ 函数中调用 ProcessMap​ 函数，并传递了 myMap​ 的地址。通过指针传递，避免了对 myMap​ 进行复制，直接在原始的 map 上进行修改。 第三种情况，当需要避免 map 本身可能出现 nil 指针问题时，可以通过指针封装来确保 map 不会为 nil。以下是一个例子： 1234567891011121314151617181920212223242526package mainimport &quot;fmt&quot;type MapWrapper struct { Map *map[string]string}func NewMapWrapper() *MapWrapper { m := make(map[string]string) return &amp;MapWrapper{Map: &amp;m}}func main() { mapWrapper := NewMapWrapper() // 检查 map 是否为 nil，如果是则初始化一个空的 map if *(mapWrapper.Map) == nil { *mapWrapper.Map = make(map[string]string) } (*mapWrapper.Map)[&quot;Key1&quot;] = &quot;Value1&quot; (*mapWrapper.Map)[&quot;Key2&quot;] = &quot;Value2&quot; fmt.Println(*mapWrapper.Map) // 输出：map[Key1:Value1 Key2:Value2]} 在这个例子中，MapWrapper​ 结构体包含一个指向 map[string]string​ 类型的指针。NewMapWrapper​ 函数返回一个初始化过的 MapWrapper​ 实例，其中的 map 指针初始化为一个空的 map。然后在 main​ 函数中对 map 进行操作，并确保了 map 不会是 nil。 关于map中的key可以是任意类型为了说明值可以是任意类型的，这里给出了一个使用 func() int 作为值的 map——mf： 123456789101112package mainimport &quot;fmt&quot;func main() { mf := map[int]func() int{ 1: func() int { return 10 }, 2: func() int { return 20 }, 5: func() int { return 50 }, } fmt.Println(mf)} 输出结果为：map[1:0x53b9a0 2:0x53b9c0 5:0x53b9e0] 整形key的value值都被映射到了对应的函数地址 用slice作为map的值既然一个 key 只能对应一个 value，而 value 又是一个原始类型，那么如果一个 key 要对应多个值怎么办？例如，当我们要处理unix机器上的所有进程，以父进程（pid 为整形）作为 key，所有的子进程（以所有子进程的 pid 组成的切片）作为value。通过将 value 定义为 []int 类型或者其他类型的切片，就可以优雅的解决这个问题。 这里有一些定义这种 map 的例子： 12mp1 := make(map[int][]int)mp2 := make(map[int]*[]int) 示例中的 mp1​ 和 mp2​ 都是用来处理一个 key 对应多个值的情况。 ​mp1 := make(map[int][]int)​：这创建了一个 map，其中每个 key 是一个整数，而每个 value 是一个整数切片。这样，对于每个 key，你可以将其值设置为包含多个整数的切片，实现了一个 key 对应多个值的存储。 ​mp2 := make(map[int]*[]int)​：这个 map 与上述的 mp1​ 稍有不同，它的 value 是指向整数切片的指针。这意味着每个 key 对应一个指向整数切片的指针，这样可以通过指针修改对应 key 的整数切片。 举例说明： 1234567891011121314151617181920212223package mainimport &quot;fmt&quot;func main() { // 定义一个 map，每个 key 对应一个整数切片 mp1 := make(map[int][]int) // 添加多个值到同一个 key 对应的切片中 mp1[1] = append(mp1[1], 10) mp1[1] = append(mp1[1], 20) fmt.Println(mp1) // 输出: map[1:[10 20]] // 定义一个 map，每个 key 对应一个整数切片的指针 mp2 := make(map[int]*[]int) // 添加多个值到同一个 key 对应的切片中 slice := []int{30, 40} mp2[2] = &amp;slice fmt.Println(*mp2[2]) // 输出: [30 40]} 这些示例展示了如何使用这两种方式来实现一个 key 对应多个值的存储，并在需要时向切片中添加多个值。 在使用上，map[int][]int​ 和 map[int]*[]int​ 这两种方式有一些不同点： 数据所有权： ​map[int][]int​：在这种情况下，切片是直接存储在 map 的值中，而不是指针。这意味着每个 key 对应的值都是独立的，修改一个 key 对应的切片不会影响其他 key。 ​map[int]*[]int​：这种情况下，map 中的值是指向切片的指针。多个 key 可能共享相同的切片，因为它们都指向同一个切片。修改一个 key 对应的切片可能会影响其他 key。 内存管理和性能： ​map[int][]int​：由于切片是直接存储在 map 中的值，这可能会增加内存使用量，尤其当值的数量较多时。但是，访问和修改数据的性能可能更好，因为它们是直接存储在 map 中的。 ​map[int]*[]int​：这种情况下，指针存储在 map 中的值，实际的切片数据存储在堆上。它可能需要更多的内存，但在某些情况下，它可以减少复制和传递数据的开销，因为多个 key 可以共享相同的切片数据。 安全性： ​map[int][]int​：由于切片是直接存储在 map 中的值，对一个 key 对应的切片的修改不会影响其他 key。这可以提供更好的安全性，因为每个 key 拥有独立的数据。 ​map[int]*[]int​：当多个 key 共享同一个切片时，一个 key 的修改可能会影响其他 key，可能导致意外的数据修改。在多个地方使用同一个切片时，需要更小心地管理数据的修改。 因此，在使用时，需要根据具体的场景和需求来选择适当的方式。如果需要独立的数据副本或更好的数据安全性，map[int][]int​ 可能更适合；如果需要共享数据、降低内存占用或减少复制开销，map[int]*[]int​ 可能更适合。 这个差异的优缺点有待商榷 当使用 map[int]*[]int​ 的方式时，多个 key 可能共享相同的切片。因此，对一个 key 对应的切片的修改可能会影响其他 key。 举个例子： 12345678910111213141516171819202122package mainimport ( &quot;fmt&quot;)func main() { myMap := make(map[int]*[]int) // 创建一个切片并将其地址存储在 map[1] 中 slice := []int{10, 20} myMap[1] = &amp;slice // 将 map[2] 指向 map[1] 所指向的切片 myMap[2] = myMap[1] // 修改 map[1] 中的切片 *myMap[1] = append(*myMap[1], 30) // 输出 map[2]，查看是否受到影响 fmt.Println(*myMap[2]) // 输出：[10 20 30]} 在这个示例中，我们创建了一个 map[int]*[]int​ 类型的 map，将一个切片的地址存储在 map[1] 中。接着，我们将 map[2] 的值指向了 map[1] 所指向的同一个切片。 然后，我们修改了 map[1] 中的切片，通过在 map[1] 所指向的切片中添加一个新元素。接着，我们打印了 map[2] 的值，发现其值也发生了变化，因为 map[2] 实际上指向了 map[1] 所指向的同一个切片。 这就说明了当使用 map[int]*[]int​ 时，一个 key 的切片修改会影响到其他 key，因为它们指向的是相同的切片。 我的尝试： 指针所指向的地址相同的情况： 12345678910111213141516171819202122232425262728293031323334package mainimport &quot;fmt&quot;func main() { // 定义一个 map，每个 key 对应一个整数切片 mp1 := make(map[int][]int) // 添加多个值到同一个 key 对应的切片中 mp1[1] = append(mp1[1], 10) mp1[1] = append(mp1[1], 20) fmt.Println(mp1) // 输出: map[1:[10 20]] // 定义一个 map，每个 key 对应一个整数切片的指针 mp2 := make(map[int]*[]int) // 添加多个值到同一个 key 对应的切片中 slice1 := []int{30, 40} mp2[2] = &amp;slice1 mp2[3] = mp2[2] fmt.Println(*mp2[2]) fmt.Println(*mp2[3]) *mp2[2] = append(*mp2[2], 50) fmt.Println(*mp2[2]) // 输出: [30 40] fmt.Println(*mp2[3]) // 打印 mp2[2] 和 mp2[3] 指针所指向的地址 fmt.Printf(&quot;Address of mp2[2]: %p\\n&quot;, mp2[2]) fmt.Printf(&quot;Address of mp2[3]: %p\\n&quot;, mp2[3])} 结果: ​​ ‍ 指针所指向的地址不相同的情况： 1234567891011121314151617181920212223242526272829303132package mainimport &quot;fmt&quot;func main() { // 定义一个 map，每个 key 对应一个整数切片 mp1 := make(map[int][]int) // 添加多个值到同一个 key 对应的切片中 mp1[1] = append(mp1[1], 10) mp1[1] = append(mp1[1], 20) fmt.Println(mp1) // 输出: map[1:[10 20]] // 定义一个 map，每个 key 对应一个整数切片的指针 mp2 := make(map[int]*[]int) // 添加多个值到同一个 key 对应的切片中 slice1 := []int{30, 40} mp2[2] = &amp;slice1 // 这里是一个新的实例，在内存中独立存在，与之前的slice1不同 slice2 := []int{30, 40} mp2[3] = &amp;slice2 fmt.Println(*mp2[2]) // 输出: [30 40] fmt.Println(*mp2[3]) // 打印 mp2[2] 和 mp2[3] 指针所指向的地址 fmt.Printf(&quot;Address of mp2[2]: %p\\n&quot;, mp2[2]) fmt.Printf(&quot;Address of mp2[3]: %p\\n&quot;, mp2[3])} 结果： ​​ 总结浅浅地理解map与slice的差异： 对于slice切片，它的核心强调的是容量，能装纳多少数据，但是map强调的是键值对的存储与索引 map底层更加复杂，在扩容上来讲，map扩容的效率要低，slice-&gt;数组；map-&gt;桶；map扩容操作时需要对所有键值对重新计算哈希值和桶的位置。这种操作需要花费更长的时间。 更深层次的理解见底层篇——《核心底层》 总结起来主要了以下几点： 什么是map，map格式与数据类型限制【尤其是key】，map特征，如何创建以及初始化map map的常规操作：访问，赋值，遍历，删除，判断key是否存在，获取长度等 map的嵌套 其实还涉及很多问题，比如map的并发安全问题，syc.map相关，后续想到再巩固","link":"/post/re-learning-go-language-map-z1kmiya.html"},{"title":"服务器性能指标相关内容","text":"服务器性能指标相关内容 以前不了解这方面，常听说什么12306、双十一淘宝、天猫的并发支撑的某某性能数据都感觉很酷的样子…..实际情况就是：我知道确实很强，但这些数据性能反应怎么个水平，强到什么水平，完全没有概念，比如可以并发支持用户同时的千万级请求、上亿的同时请求，强到什么概念，凭什么可以这么强，是完全无法想象的…..就比如我先现在八块腹肌、单手五十个俯卧撑，外行感觉很强，但是只有我自己知道付出了多少努力(其实没有多少努力全看天赋[dog])…… 其实打算把这一部分划分到高并发区域，但是这都是一些基础的概念和微不足道的理解，还是划分到架构板块了。 11.21 新的理解：我的评价是高并发不及高可用！可高用是真的太难了[dog] 一些常见名词、概念： 峰值时间：每天80%的访问集中在20%的时间里，这20%时间即峰值时间 并发：一段时间访问的大量用户的请求（并发是最能体现你的程序和机器的性能。） 并行：同一时刻的大量用户的请求(并发和并行很像，但是维度不同) QPS（每秒查询率）：对应fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。 QPS = 并发量 / 平均响应时间 并发量 = QPS * 平均响应时间 峰值估算公式：( 总PV数 * 80% ) / ( 每天秒数 * 20% ) = 峰值时间每秒请求数(QPS) 机器：峰值时间每秒QPS / 单台机器的QPS = 需要的机器数量 阿姆达尔定律：不可并行部分的比率才是决定着是否能成倍增长效率的关键，CPU核心数的提升不一定能够提升QPS QPS：QPS（Queries-per-second）是每秒钟查询率，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准, 即每秒的响应请求数，也即是最大吞吐能力。 对应fetches/sec，即每秒的响应请求数/每秒查询率 QPS即每秒处理事务数，包括了用户请求服务器、服务器自己的内部处理、服务器返回给用户。这三个过程，每秒能够完成N个这三个过程，TPS也就是N； Qps基本类似于Tps，但是不同的是，对于一个页面的一次访问，形成一个Tps； 但一次页面请求，可能产生多次对服务器的请求，服务器对这些请求，就可计入“QPS”之中。理解上来看QPS可能颗粒度更细一点。 TPSTPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。 即每秒处理事务数，每个事务包括了如下3个过程： a.用户请求服务器 b.服务器自己的内部处理（包含应用服务器、数据库服务器等） c.服务器返回给用户 如果每秒能够完成N个这三个过程，tps就是N； 一般的，评价系统性能均以每秒钟完成的技术交易的数量来衡量。系统整体处理能力取决于处理能力最低模块的TPS值。这一点有一点深入的体会，金融交易所涉及的金融交易，一般使用的都是TPS来衡量。比如交易清仓高峰时间、订单峰值之类的可以联想一下。 RT响应时间即RT，处理一次请求所需要的平均处理时间。对于RT，客户端和服务端是大不相同的，因为请求从客户端到服务端，需要经过广域网，所以客户端RT往往远大于服务端RT，同时客户端的RT往往决定着用户的真实体验，服务端RT往往是评估我们系统好坏的一个关键因素。这一点说白了我们能够控制的只有服务器、后端的这一部分链路，涉及到物理、底层基础设施建设的我想很难把控，因为要考虑的因素太多，猜测涉及大部门之间的跨越。 最佳线程数的困扰在开发过程中，我们一定面临过很多的线程数量的配置问题，这种问题往往让人摸不到头脑，往往都是拍脑袋给出一个线程池的数量，但这可能恰恰是不靠谱的，过小的话会导致请求RT极具增加，过大也一样RT也会升高。所以对于最佳线程数的评估往往比较麻烦。 这一部分有一说一确实没有比较好的实践思路….后续可能会了解下，但有固定的思路，但go中又很特殊，是goroutine，猜想应用基准测试来尝试，慢慢试；或者计算，但是计算好像是线程层面的，比如Java中经常喜欢计算求得最佳线程数 QPS和RT的关系单线程场景：假设我们的服务端只有一个线程，那么所有的请求都是串行执行，我们可以很简单的算出系统的QPS，也就是：QPS = 1000ms/RT。假设一个RT过程中CPU计算的时间为49ms，CPU Wait Time 为200ms，那么QPS就为1000/（49+200） = 4.01。 多线程场景我们接下来把服务端的线程数提升到2，那么整个系统的QPS则为：2 *（1000/(49+200)）=8.02。可见QPS随着线程的增加而线性增长，那QPS上不去就加线程呗，听起来很有道理，公式也说得通，但是往往现实并非如此，后面会聊这个问题。 最佳线程数？从上面单线程场景来看，CPU Wait time为200ms,你可以理解为CPU这段时间什么都没做，是空闲的，显然我们没把CPU利用起来，这时候我们需要启多个线程去响应请求，把这部分利用起来，那么启动多少个线程呢？我们可以估算一下 空闲时间200ms，我们要把这部分时间转换为CPU Time,那么就是(200+49)/49 = 5.08个，不考虑上下文切换的话，约等于5个线程。同时还要考虑CPU的核心数和利用率问题，那么我们得到了最佳线程数计算的公式： (（CPU Time + CPU Wait Time）/ CPU Time) * coreSize * cupRatio 最大QPS？得到了最大的线程数和QPS的计算方式： QPS = Thread num * 单线程QPS = （CPU Time + CPU Wait Time）/CPU Time * coreSize * CupRatio * (1000ms/(CPU Time + CPU Wait Time)) = (1000ms/ CPU Time) * coreSize * cpuRatio 所以决定一个系统最大的QPS的因素是CPU Time、CoreSize和CPU利用率。看似增加CPU核数（或者说线程数）可以成倍的增加系统QPS，但实际上增加线程数的同时也增加了很大的系统负荷，更多的上下文切换，QPS和最大的QPS是有偏差的。 CPU Time &amp; CPU Wait Time &amp; CPU 利用率CPU Time：就是一次请求中，实际用到计算资源。CPU Time的消耗是全流程的，涉及到请求到应用服务器，再从应用服务器返回的全过程。实际上这取决于你的计算的复杂度。CPU Wait Time：是一次请求过程中对于IO的操作，CPU这段时间可以理解为空闲的，那么此时要尽量利用这些空闲时间，也就是增加线程数。CPU 利用率：是业务系统利用到CPU的比率，因为往往一个系统上会有一些其他的线程，这些线程会和CPU竞争计算资源，那么此时留给业务的计算资源比例就会下降，典型的像，GC线程的GC过程、锁的竞争过程都是消耗CPU的过程。甚至一些IO的瓶颈，也会导致CPU利用率下降(CPU都在Wait IO，利用率当然不高)。 增加CPU核数是否能对QPS得到提升？首先答案是不一定 从上面的公式我们可以看出，假设CPU Time和CPU 利用率不变，增加CPU的核数能使QPS呈线性增长。但是很遗憾，现实中不是这样的…首先先看一下阿姆达尔定律：阿姆达尔定律给出了任务在固定负载的情况下，随着系统资源的提升，执行速度的理论上限。以计算机科学家Gene Amdahl命名。 ​​​​​​ 其中Slatency: 整个任务的提速比。s: 部分任务得益于系统资源升级带来的提速比。p: 这部分任务执行时间占整个任务执行时间的百分比（系统资源提升前）。 从上可以得到： ​​​​ 以上公式说明了通过资源升级来给任务加速的加速比上限，而且和提速的幅度无关，理论加速比总是受限于不能加速的任务的比例。 阿姆达尔的定律常用于并行计算中，用来估计多处理器情况下的理论加速比。例如，如果有个程序在单核下需要执行20个小时，并且不能被并行处理的部分占1个小时的执行时间，剩余的19个小时(p=0,95)的任务可以并行化，那么不管有多少核心来并行处理这个程序，最小执行时间不可能小于一个小时。由此得到，理论加速比的上限是20倍（1/(1-p) = 20）。因此，并行计算只和少数的核心和极度可并行化的程序相关。 同样，对于1000ms/(CPU Time) * coreSize * cpuRatio我们不断的增加CoreSize或者说线程数的时候。我们的请求变多了，随之而来的就是大量的上下文切换（go中的协程不涉及上下文切换） 、大量的GC、大量的锁征用，这些会增加不可并行部分的总时间，也会大大的增加CPU Time。假设我们的串行部分不变的话，增大核数，CPU不能得到充分的利用，利用率也会降低。所以，对于阿姆达尔定律而言，不可并行部分的比率才是决定着是否能成倍增长效率的关键。也就是说最佳线程数也好，最大QPS也好，增加内核数量不一定能是系统指标有成倍的增长。更关键的是能改变自己的架构，减小串行的比率，让CPU更充分的利用，达到资源的最大利用率。 如何寻求最佳线程数和最大QPS通过上面一些例子，我们发现当线程数增加的时候，线程的上下文切换会增加，GC Time会增加。这也就导致CPU time 增加，QPS减小，RT也会随着增大。这显然不是我们希望的，我们希望的是在核数一定的情况下找到某个点，使系统的QPS最大，RT相对较小。所以我们需要不断的压测，调整线程池，找到这个QPS的峰值，并且使CPU的利用率达到100%,这样才是系统的最大QPS和最佳线程数。 补充上下文切换多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是: 当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。Linux 相比与其他操作系统(包括其他类 Unix 系统)有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。 如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的，CPU 根本没有得到充分利用。 如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率 CPU密集任务与I/O密集任务有一个简单并且适用面比较广的公式: CPU 密集型任务(N+1): 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N (CPU 核心数)+1.比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用CPU 的空闲时间。 I/O 密集型任务(2N): 这种任务应用起来，系统会用大部分的时间来处理 /0 交互，而线程在处理 /O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 如何判断是 CPU 密集任务还是Io 密集任务? CPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及网络读取，文件读取这类都是 I/O 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待I/O操作完成的时间来说很少，大部分时间都花在了等待 I/O 操作完成上. 线程数更严谨的计算的方法应该是: 最佳线程数 = (CPU 核心数)* (1+MT(线程等待时间)/ST (线程计算时间)) 公示也只是参考，具体还是要根据项目实际线上运行情况来动态调整。 PVPV（Page View）：页面访问量，即页面浏览量或点击量，用户每次刷新即被计算一次。可以统计服务一天的访问日志得到。 UVUV（Unique Visitor）：独立访客，统计1天内访问某站点的用户数。可以统计服务一天的访问日志并根据用户的唯一标识去重得到。响应时间（RT）：响应时间是指系统对请求作出响应的时间，一般取平均响应时间。可以通过Nginx、Apache之类的Web Server得到。 DAUDAU(Daily Active User)，日活跃用户数量。常用于反映网站、互联网应用或网络游戏的运营情况。DAU通常统计一日（统计日）之内，登录或使用了某个产品的用户数（去除重复登录的用户），与UV概念相似 MAUMAU(Month Active User)：月活跃用户数量，指网站、app等去重后的月活跃用户数量 系统吞吐量评估我们在做系统设计的时候就需要考虑CPU运算，IO，外部系统响应因素造成的影响以及对系统性能的初步预估。而通常情况下，我们面对需求，我们评估出来的出来QPS，并发数之外，还有另外一个维度：日pv。 通过观察系统的访问日志发现，在用户量很大的情况下，各个时间周期内的同一时间段的访问流量几乎一样。比如工作日的每天早上。只要能拿到日流量图和QPS我们就可以推算日流量。 通常的技术方法： 1、找出系统的最高TPS和日PV，这两个要素有相对比较稳定的关系（除了放假、季节性因素影响之外）。 2、通过压力测试或者经验预估，得出最高TPS，然后跟进1结果的关系，计算出系统最高的日吞吐量。B2B中文和淘宝面对的客户群不一样，这两个客户群的网络行为不应用，他们之间的TPS和PV关系比例也不一样。 软件性能测试的基本概念和计算公式软件做性能测试时需要关注哪些性能呢？ 首先，开发软件的目的是为了让用户使用，我们先站在用户的角度分析一下，用户需要关注哪些性能。 对于用户来说，当点击一个按钮、链接或发出一条指令开始，到系统把结果已用户感知的形式展现出来为止，这个过程所消耗的时间是用户对这个软件性能的直观印 象。也就是我们所说的响应时间，当相应时间较小时，用户体验是很好的，当然用户体验的响应时间包括个人主观因素和客观响应时间，在设计软件时，我们就需要考虑到如何更好地结合这两部分达到用户最佳的体验。如：用户在大数据量查询时，我们可以将先提取出来的数据展示给用户，在用户看的过程中继续进行数据检索，这时用户并不知道我们后台在做什么。 用户关注的是用户操作的相应时间。 其次，我们站在管理员的角度考虑需要关注的性能点。 1、 响应时间2、 服务器资源使用情况是否合理3、 应用服务器和数据库资源使用是否合理4、 系统能否实现扩展5、 系统最多支持多少用户访问、系统最大业务处理量是多少6、 系统性能可能存在的瓶颈在哪里7、 更换那些设备可以提高性能8、 系统能否支持7×24小时的业务访问 再次，站在开发（设计）人员角度去考虑。 1、 架构设计是否合理2、 数据库设计是否合理3、 代码是否存在性能方面的问题4、 系统中是否有不合理的内存使用方式5、 系统中是否存在不合理的线程同步方式6、 系统中是否存在不合理的资源竞争 ‍","link":"/post/server-performance-indicators-and-related-content-shallow-understanding-vlxgk.html"},{"title":"Reverse a String","text":"字符串反转翻转含有中文、数字、英文字母​的字符串 如：&quot;子asdf黑g白hjkl小&quot;​ 1234567891011121314151617181920212223242526272829303132package mainimport &quot;fmt&quot;/** @Title main @Description: 1.rune关键字，从golang源码中看出，它是int32的别名（-2^31 ~ 2^31-1），比起byte（-128～127），可表示更多的字符。 2.由于rune可表示的范围更大，所以能处理一切字符，当然也包括中文字符。在平时计算中文字符，可用rune。 3.因此将字符串转为rune的切片，再进行翻转，完美解决。 @Author luommy 2023-09-22 00:19:14**/func main() { src := &quot;子asdf黑g白hjkl小&quot; // int32 is the set of all signed 32-bit integers. Range: -2147483648 through 2147483647. str := reverse([]rune(src)) fmt.Printf(&quot;%v\\n&quot;, string(str))}/** @Title reverse @Description @Author luommy 2023-09-22 00:12:29 @Param s @Return []rune**/func reverse(s []rune) []rune { for i, j := 0, len(s)-1; i &lt; j; i, j = i+1, j-1 { s[i], s[j] = s[j], s[i] } return s} ‍ 运行结果： ​​","link":"/post/string-reverse-1jimhs.html"}],"tags":[{"name":"timeline","slug":"timeline","link":"/tags/timeline/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"领域算法","slug":"领域算法","link":"/tags/%E9%A2%86%E5%9F%9F%E7%AE%97%E6%B3%95/"},{"name":"知识体系","slug":"知识体系","link":"/tags/%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"并发编程","slug":"并发编程","link":"/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"goland技巧","slug":"goland技巧","link":"/tags/goland%E6%8A%80%E5%B7%A7/"},{"name":"高并发","slug":"高并发","link":"/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"Myblog","slug":"Myblog","link":"/tags/Myblog/"},{"name":"nacos","slug":"nacos","link":"/tags/nacos/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"}],"categories":[{"name":"京东到家","slug":"京东到家","link":"/categories/%E4%BA%AC%E4%B8%9C%E5%88%B0%E5%AE%B6/"},{"name":"数据结构与算法","slug":"数据结构与算法","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"算法加练","slug":"算法加练","link":"/categories/%E7%AE%97%E6%B3%95%E5%8A%A0%E7%BB%83/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"分布式、微服务、架构","slug":"分布式、微服务、架构","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E3%80%81%E6%9E%B6%E6%9E%84/"},{"name":"Golang","slug":"Golang","link":"/categories/Golang/"},{"name":"知识体系","slug":"知识体系","link":"/categories/%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"微服务","slug":"分布式、微服务、架构/微服务","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E3%80%81%E6%9E%B6%E6%9E%84/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"重学系列","slug":"Golang/重学系列","link":"/categories/Golang/%E9%87%8D%E5%AD%A6%E7%B3%BB%E5%88%97/"},{"name":"并发编程","slug":"Golang/并发编程","link":"/categories/Golang/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"架构","slug":"分布式、微服务、架构/架构","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E3%80%81%E6%9E%B6%E6%9E%84/%E6%9E%B6%E6%9E%84/"},{"name":"分布式","slug":"分布式、微服务、架构/分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E3%80%81%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"pages":[]}