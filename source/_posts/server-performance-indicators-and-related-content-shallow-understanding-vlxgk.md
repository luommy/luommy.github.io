---
title: 服务器性能指标相关内容
date: '2023-10-31 16:28:25'
updated: '2023-11-22 16:11:17'
excerpt: QPS、TPS、RT、阿姆达尔定律等相关的理解
categories:
  - 分布式、微服务、架构
  - 架构
permalink: >-
  /post/server-performance-indicators-and-related-content-shallow-understanding-vlxgk.html
comments: true
toc: true
---

# 服务器性能指标及相关内容浅浅理解

> 以前不了解这方面，常听说什么12306、双十一淘宝、天猫的并发支撑的某某性能数据都感觉很酷的样子.....实际情况就是：我知道确实很强，但这些数据性能反应怎么个水平，强到什么水平，完全没有概念，比如可以并发支持用户同时的千万级请求、上亿的同时请求，强到什么概念，凭什么可以这么强，是完全无法想象的.....就比如我先现在八块腹肌、单手五十个俯卧撑，外行感觉很强，但是只有我自己知道付出了多少努力(其实没有多少努力全看天赋[dog])......
>
> 其实打算把这一部分划分到高并发区域，但是这都是一些基础的概念和微不足道的理解，还是划分到架构板块了。

11.21 新的理解：我的评价是高并发不及高可用！可高用是真的太难了[dog]

一些常见名词、概念：

* **峰值时间：每天80%的访问集中在20%的时间里，这20%时间即峰值时间**
* **并发：一段时间访问的大量用户的请求（并发是最能体现你的程序和机器的性能。）**
* **并行：同一时刻的大量用户的请求(并发和并行很像，但是维度不同)**
* QPS（每秒查询率）：对应fetches/sec，即每秒的响应请求数，也即是最大吞吐能力。

  QPS = 并发量 / 平均响应时间

  并发量 = QPS * 平均响应时间
* 峰值估算公式：( 总PV数 * 80% ) / ( 每天秒数 * 20% ) = 峰值时间每秒请求数(QPS)
* 机器：峰值时间每秒QPS / 单台机器的QPS = 需要的机器数量
* **阿姆达尔定律**：不可并行部分的比率才是决定着是否能成倍增长效率的关键，CPU核心数的提升不一定能够提升QPS

# QP：

QPS（Queries-per-second）是每秒钟处理完请求的次数，即最大吞吐能力。这里的请求不是指一个查询或者数据库查询，是包括一个业务逻辑的整个流程，也就是说每秒钟响应的请求次数。

Queries-per-second对应fetches/sec，即每秒的响应请求数/每秒查询率 ，也即是最大吞吐能力。

QPS即每秒处理事务数，包括了用户请求服务器、服务器自己的内部处理、服务器返回给用户。这三个过程，每秒能够完成N个这三个过程，TPS也就是N；

Qps基本类似于Tps，但是不同的是，对于一个页面的一次访问，形成一个Tps；

但一次页面请求，可能产生多次对服务器的请求，服务器对这些请求，就可计入“QPS”之中。理解上来看QPS可能颗粒度更细一点。

# TPS

TPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。

TPS包括一条消息入和一条消息出，加上一次用户数据库访问。（业务TPS = CAPS【每秒建立呼叫数量】 × 每个呼叫平均TPS）

TPS是软件测试结果的测量单位。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。

一般的，评价系统性能均以每秒钟完成的技术交易的数量来衡量。系统整体处理能力取决于处理能力最低模块的TPS值。这一点有一点深入的体会，金融交易所涉及的金融交易，一般使用的都是TPS来衡量。比如交易清仓高峰时间、订单峰值之类的可以联想一下。

# RT

响应时间即RT，处理一次请求所需要的平均处理时间。对于RT，客户端和服务端是大不相同的，因为请求从客户端到服务端，需要经过*广域网*，所以**客户端RT往往远大于服务端RT**，同时客户端的RT往往决定着用户的真实体验，<u>服务端RT往往是评估我们系统好坏的一个关键因素</u>。这一点说白了我们能够控制的只有服务器、后端的这一部分链路，涉及到物理、底层基础设施建设的我想很难把控，因为要考虑的因素太多，猜测涉及大部门之间的跨越。

# 最佳线程数的困扰

在开发过程中，我们一定面临过很多的线程数量的配置问题，这种问题往往让人摸不到头脑，往往都是拍脑袋给出一个线程池的数量，但这可能恰恰是不靠谱的，过小的话会导致请求RT极具增加，过大也一样RT也会升高。所以对于最佳线程数的评估往往比较麻烦。

*这一部分有一说一确实没有比较好的实践思路....后续可能会了解下，但有固定的思路，但go中又很特殊，是goroutine，猜想应用基准测试来尝试，慢慢试；或者计算，但是计算好像是线程层面的，比如Java中经常喜欢计算求得最佳线程数*

# QPS和RT的关系

## 单线程场景：

假设我们的服务端只有一个线程，那么所有的请求都是串行执行，我们可以很简单的算出系统的QPS，也就是：**QPS = 1000ms/RT**。假设一个RT过程中CPU计算的时间为49ms，CPU Wait Time 为200ms，那么QPS就为1000/（49+200） = 4.01。

## 多线程场景

我们接下来把服务端的线程数提升到2，那么整个系统的QPS则为：2 *（1000/(49+200)）=8.02。可见QPS随着线程的增加而线性增长，那QPS上不去就加线程呗，听起来很有道理，公式也说得通，但是往往现实并非如此，后面会聊这个问题。

# 最佳线程数？

从上面单线程场景来看，CPU Wait time为200ms,你可以理解为CPU这段时间什么都没做，是空闲的，显然我们没把CPU利用起来，这时候我们需要启多个线程去响应请求，把这部分利用起来，那么启动多少个线程呢？我们可以估算一下 空闲时间200ms，我们要把这部分时间转换为CPU Time,那么就是(200+49)/49 = 5.08个，不考虑上下文切换的话，约等于5个线程。同时还要考虑CPU的核心数和利用率问题，那么我们得到了最佳线程数计算的公式：  
 **(（CPU Time + CPU Wait Time）/ CPU Time) * coreSize * cupRatio**

# 最大QPS？

得到了最大的线程数和QPS的计算方式：

**QPS = Thread num * 单线程QPS = （CPU Time + CPU Wait Time）/CPU Time * coreSize * CupRatio * (1000ms/(CPU Time + CPU Wait Time)) = (1000ms/ CPU Time) * coreSize * cpuRatio**

所以决定一个系统最大的QPS的因素是CPU Time、CoreSize和CPU利用率。看似增加CPU核数（或者说线程数）可以成倍的增加系统QPS，但实际上增加线程数的同时也增加了很大的系统负荷，更多的上下文切换，QPS和最大的QPS是有偏差的。

# CPU Time & CPU Wait Time & CPU 利用率

**CPU Time**：就是一次请求中，实际用到计算资源。CPU Time的消耗是全流程的，涉及到请求到应用服务器，再从应用服务器返回的全过程。实际上这取决于你的计算的复杂度。  
**CPU Wait Time**：是一次请求过程中对于IO的操作，CPU这段时间可以理解为空闲的，那么此时要尽量利用这些空闲时间，也就是增加线程数。  
**CPU 利用率**：是业务系统利用到CPU的比率，因为往往一个系统上会有一些其他的线程，这些线程会和CPU竞争计算资源，那么此时留给业务的计算资源比例就会下降，典型的像，GC线程的GC过程、锁的竞争过程都是消耗CPU的过程。甚至一些IO的瓶颈，也会导致CPU利用率下降(CPU都在Wait IO，利用率当然不高)。

# 增加CPU核数是否能对QPS得到提升？

**首先答案是不一定**

从上面的公式我们可以看出，假设CPU Time和CPU 利用率不变，增加CPU的核数能使QPS呈线性增长。但是很遗憾，现实中不是这样的…首先先看一下**阿姆达尔定律**：  
阿姆达尔定律给出了任务在固定负载的情况下，随着系统资源的提升，执行速度的理论上限。以计算机科学家Gene Amdahl命名。

​![在这里插入图片描述](https://img-blog.csdnimg.cn/2019030416542131.png)​

其中  
Slatency: 整个任务的提速比。  
s: 部分任务得益于系统资源升级带来的提速比。  
p: 这部分任务执行时间占整个任务执行时间的百分比（系统资源提升前）。

从上可以得到：

​![在这里插入图片描述](https://img-blog.csdnimg.cn/20190304165507871.png)​

以上公式说明了通过资源升级来给任务加速的加速比上限，而且和提速的幅度无关，理论加速比总是受限于不能加速的任务的比例。

阿姆达尔的定律常用于并行计算中，用来估计多处理器情况下的理论加速比。例如，如果有个程序在单核下需要执行20个小时，并且不能被并行处理的部分占1个小时的执行时间，剩余的19个小时(p=0,95)的任务可以并行化，那么不管有多少核心来并行处理这个程序，最小执行时间不可能小于一个小时。由此得到，理论加速比的上限是20倍（1/(1-p) = 20）。因此，并行计算只和少数的核心和极度可并行化的程序相关。

同样，对于1000ms/(CPU Time) * coreSize * cpuRatio我们不断的增加CoreSize或者说线程数的时候。我们的请求变多了，随之而来的就是大量的上下文切换（go中的协程不涉及上下文切换） **、大量的GC、大量的锁征用**，这些会增加不可并行部分的总时间，也会大大的增加CPU Time。假设我们的串行部分不变的话，增大核数，CPU不能得到充分的利用，利用率也会降低。所以，对于阿姆达尔定律而言，不可并行部分的比率才是决定着是否能成倍增长效率的关键。也就是说最佳线程数也好，最大QPS也好，增加内核数量不一定能是系统指标有成倍的增长。更关键的是能改变自己的架构，减小串行的比率，让CPU更充分的利用，达到资源的最大利用率。

# 如何寻求最佳线程数和最大QPS

通过上面一些例子，我们发现当线程数增加的时候，线程的上下文切换会增加，GC Time会增加。这也就导致CPU time 增加，QPS减小，RT也会随着增大。这显然不是我们希望的，我们希望的是在核数一定的情况下找到某个点，使系统的QPS最大，RT相对较小。所以我们需要不断的压测，调整线程池，找到这个QPS的峰值，并且使CPU的利用率达到100%,这样才是系统的最大QPS和最佳线程数。

# 补充

## 上下文切换

多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是: 当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。**Linux 相比与其他操作系统(包括其他类 Unix 系统)有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。**

类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。

如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的，CPU 根本没有得到充分利用。

如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率

## CPU密集任务与I/O密集任务

有一个简单并且适用面比较广的公式:

CPU 密集型任务(N+1): 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N (CPU 核心数)+1.比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用CPU 的空闲时间。

I/O 密集型任务(2N): 这种任务应用起来，系统会用大部分的时间来处理 /0 交互，而线程在处理 /O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。

如何判断是 CPU 密集任务还是Io 密集任务?

CPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及*网络读取，文件读取*这类都是 I/O 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待I/O操作完成的时间来说很少，大部分时间都花在了等待 I/O 操作完成上.

线程数更严谨的计算的方法应该是:

最佳线程数 =  (CPU 核心数)* (1+MT(线程等待时间)/ST (线程计算时间))

公示也只是参考，具体还是要根据项目实际线上运行情况来动态调整。

​​

​​
